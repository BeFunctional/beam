{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Beam is a highly-general library for accessing any kind of database with Haskell. It supports several backends. beam-postgres and beam-sqlite are included in the main beam repository. Others are hosted and maintained independently, such as beam-mysql and beam-firebird . The documentation here shows examples in all known backends. Beam is highly extensible and other backends can be shipped independently without requiring any changes in the core libraries.For information on creating additional SQL backends, see the manual section for more. Beam features Easy schema generation from existing databases A basic migration infrastructure for working with multiple versions of your database schema. Support for most SQL92, SQL99, and SQL2003 features across backends that support them, including aggregations, subqueries, and window functions. A straightforward Haskell-friendly query syntax . You can use Beam's Q monad much like you would interact with the [] monad. No Template Haskell Beam uses the GHC Haskell type system and nothing else. The types have been designed to be easily-inferrable by the compiler, and appropriate functions to refine types have been provided for the where the compiler may need more help. How to install Beam is available via Hackage and Stackage, and can be included in your stack project by adding beam-core and an appropriate beam backend to your .cabal file. Some projects may want to follow the latest master, for the newest features. If so, put the following in your stack.yaml to build and use beam in your project! packages : - . - location : git : https://github.com/haskell-beam/beam.git commit : <latest master commit> extra-dep : true subdirs : - beam-core - <backend> and add the following to your .cabal file, in the build-depends section: beam - core , < backend > You may also want to add the beam-migrate package if you want to manage your database schemas in Haskell as well. Available backends are: beam-postgres -- A feature-complete backend for the Postgres RDBMS. See the beam-postgres documentation for more information. beam-sqlite -- A feature-complete backend for the Sqlite library. Note that SQLite does not support all SQL92 features, so some of the examples may not work. Refer to the beam-sqlite documentation for more information on compatibility. beam-mysql -- A backend for MySQL or MariaDB. Maintained separately on GitHub . Quick Start Guide For those looking to get started with beam, we first recommend you go through the tutorial . The user guide contains much more detailed reference-like information. Finally, the documentation on hackage is always available (although the types may seem obtuse). If you're interested if beam supports your favorite database feature, refer to the documentation for your backend or take a look at the compatibility matrix . How to Contribute We always welcome contributions, especially to cover more database features or to add support for a new backend. Help is available on the beam-discussion Google Group . The following is a quick step-by-step guide of contributing a new feature: Fork the github repository at https://github.com/haskell-beam/beam and clone the fork to a local directory. Work on your feature on your own branch, or pick an issue . When you feel ready to contribute the feature back to beam-core , send a Pull Request on Github, with an explanation of what your patch does and whether it breaks the API. Respond to community comments and rework your patch. When the maintainer feels comfortable with your patch, he will commit it to the master branch and it will be included in the next minor version. API-breaking changes will not be included until the next major version. Tip Be sure to add your name to the CONTRIBUTORS file for eternal fame and glory! Questions, Feedback, Discussion For frequently asked questions, see the FAQ . For general questions, feedback on patches, support, or other concerns, please write to the mailing list For bugs or feature requests, please open an issue Why Beam? Beam is the most feature-complete, turnkey Haskell database solution out there. It supports the bulk of the SQL92, SQL99, SQL2003, SQL2006, SQL2008, SQL2011, and SQL2016 specifications, as well as the entire breadth of features of each of its backends. See the compatibility matrix . You will rarely be forced to write a SQL query 'by hand' when using Beam (but you can ). Additionally, Beam plays nice with the rest of the Haskell ecosystem, the standard Beam backends are all implemented in terms of pre-existing Haskell packages. Beam does not intend to make every query work across every database, and you are free to write queries in beam's DSL that only work on particular backends, using type classes to restrict which backends work. It is assumed that you have chosen you RDBMS with much care, and we want to support you in that. Beam's main purpose is to marshal data back and forth, to serve as the source of truth for the DB schema, and to generate properly formed SQL from Haskell expressions.","title":"Home"},{"location":"#how-to-install","text":"Beam is available via Hackage and Stackage, and can be included in your stack project by adding beam-core and an appropriate beam backend to your .cabal file. Some projects may want to follow the latest master, for the newest features. If so, put the following in your stack.yaml to build and use beam in your project! packages : - . - location : git : https://github.com/haskell-beam/beam.git commit : <latest master commit> extra-dep : true subdirs : - beam-core - <backend> and add the following to your .cabal file, in the build-depends section: beam - core , < backend > You may also want to add the beam-migrate package if you want to manage your database schemas in Haskell as well. Available backends are: beam-postgres -- A feature-complete backend for the Postgres RDBMS. See the beam-postgres documentation for more information. beam-sqlite -- A feature-complete backend for the Sqlite library. Note that SQLite does not support all SQL92 features, so some of the examples may not work. Refer to the beam-sqlite documentation for more information on compatibility. beam-mysql -- A backend for MySQL or MariaDB. Maintained separately on GitHub .","title":"How to install"},{"location":"#quick-start-guide","text":"For those looking to get started with beam, we first recommend you go through the tutorial . The user guide contains much more detailed reference-like information. Finally, the documentation on hackage is always available (although the types may seem obtuse). If you're interested if beam supports your favorite database feature, refer to the documentation for your backend or take a look at the compatibility matrix .","title":"Quick Start Guide"},{"location":"#how-to-contribute","text":"We always welcome contributions, especially to cover more database features or to add support for a new backend. Help is available on the beam-discussion Google Group . The following is a quick step-by-step guide of contributing a new feature: Fork the github repository at https://github.com/haskell-beam/beam and clone the fork to a local directory. Work on your feature on your own branch, or pick an issue . When you feel ready to contribute the feature back to beam-core , send a Pull Request on Github, with an explanation of what your patch does and whether it breaks the API. Respond to community comments and rework your patch. When the maintainer feels comfortable with your patch, he will commit it to the master branch and it will be included in the next minor version. API-breaking changes will not be included until the next major version. Tip Be sure to add your name to the CONTRIBUTORS file for eternal fame and glory!","title":"How to Contribute"},{"location":"#questions-feedback-discussion","text":"For frequently asked questions, see the FAQ . For general questions, feedback on patches, support, or other concerns, please write to the mailing list For bugs or feature requests, please open an issue","title":"Questions, Feedback, Discussion"},{"location":"#why-beam","text":"Beam is the most feature-complete, turnkey Haskell database solution out there. It supports the bulk of the SQL92, SQL99, SQL2003, SQL2006, SQL2008, SQL2011, and SQL2016 specifications, as well as the entire breadth of features of each of its backends. See the compatibility matrix . You will rarely be forced to write a SQL query 'by hand' when using Beam (but you can ). Additionally, Beam plays nice with the rest of the Haskell ecosystem, the standard Beam backends are all implemented in terms of pre-existing Haskell packages. Beam does not intend to make every query work across every database, and you are free to write queries in beam's DSL that only work on particular backends, using type classes to restrict which backends work. It is assumed that you have chosen you RDBMS with much care, and we want to support you in that. Beam's main purpose is to marshal data back and forth, to serve as the source of truth for the DB schema, and to generate properly formed SQL from Haskell expressions.","title":"Why Beam?"},{"location":"about/compatibility/","text":"Beam strives to cover the full breadth of the relevant SQL standards. In general, if there is something in a SQL standard that is not implemented in a generic manner in beam-core , feel free to file an issue requesting support. There are some features that beam purposefully omits because no major RDBMS implements them. For example, database-level assertions are not supported in any of the default beam backends, and thus are not supported by beam-core . If you have a need for these features, feel free to file an issue. Be sure to motivate your use case with examples and a testing strategy. The relevant SQL standards are SQL-92, SQL:1999, SQL:2003, SQL:2008, and SQL:2011. Because not all the standards are not publicly accessible, I've done my best to piece together features from various documents available online. I believe I've covered most of the common cases, but there may be pieces of functionality that are missing. File an issue if this is the case. The table below summarizes the features defined in each SQL standard and beam's support for them. FULL means beam supports everything in that feature. NONE means that there is no support for that feature, and none planned. N/A means that the feature only applies to RDBMSs, not the SQL language. WONTFIX means that the feature has been considered and willfully ignored. UNKNOWN means not enough investigation has gone into the feature to make a determination. TODO means the feature has not been implemented yet, but an implementation is planned. Tip The 'TODO' items are a great way to contribute to beam! Feature Status Notes B011 Embedded Ada NONE B012 Embedded C NONE B013 Embedded COBOL NONE B014 Embedded FORTRAN NONE B015 Embedded MUMPS NONE B016 Embedded Pascal NONE B017 Embedded PL/I NONE B021 Direct SQL NONE B031 Basic dynamic SQL NONE B032 Extended dynamic SQL NONE B033 Untyped SQL-invoked function arguments NONE B034 Dynamic specification of cursor attributes NONE B035 Non-extended descriptor names NONE B051 Enhanced execution rights NONE B111 Module language Ada NONE B112 Module language C NONE B113 Module language COBOL NONE B114 Module language Fortran NONE B115 Module language MUMPS NONE B116 Module language Pascal NONE B117 Module language PL/I NONE B121 Routine language Ada NONE B122 Routine language C NONE B123 Routine language COBOL NONE B124 Routine language Fortran NONE B125 Routine language MUMPS NONE B126 Routine language Pascal NONE B127 Routine language PL/I NONE B128 Routine language SQL NONE B211 Module language Ada: VARCHAR and NUMERIC support NONE B221 Routine language Ada: VARCHAR and NUMERIC support NONE E011 - Numeric data types E011-01 INTEGER and SMALLINT data types FULL Use Int32 for INTEGER , Int16 for SMALLINT E011-02 REAL, DOUBLE PRECISION, FLOAT FULL Use Double and Float E011-03 DECIMAL and NUMERIC data types FULL Use Scientific . You can provide the database precision using beam-migrate E011-04 Arithmetic operators FULL Use the Num instance for QGenExpr E011-05 Numeric comparison FULL Use the . suffixed operators (i.e., ==. , /=. , <. , etc) E011-06 Implicit casting among numeric data types WONTFIX Beam never implicitly casts. Use cast_ E021 Character string types E021-01 CHARACTER data type FULL Use Text . Use beam-migrate to specify width E021-02 CHARACTER VARYING data type FULL Use Text . Use beam-migrate to specify width. E021-03 Character literals FULL Use val_ E021-04 CHARACTER_LENGTH function FULL Use charLength_ E021-05 OCTET_LENGTH function FULL Use octetLength_ E021-06 SUBSTRING function TODO E021-07 Character concatenation FULL Use concat_ E021-08 UPPER and LOWER functions FULL Use upper_ and lower_ E021-09 TRIM function PARTIAL Use trim_ . Full support may be provided on backends that implement it E021-10 Implicit casting among string types WONTFIX Beam never implicitly casts. Use cast_ E021-11 POSITION function FULL Use position_ E021-12 Character comparison FULL Use comparison operators (See E011-05) E031 Identifiers E031-01 Delimited identifiers TODO Find out more E021-02 Lower case identifiers TODO E021-03 Trailing underscore N/A Beam will use whatever column names you specify E051 Basic query specification E051-01 SELECT DISTINCT FULL Use nub_ E051-02 GROUP BY clause FULL See aggregate_ or read the section on aggregates E051-04 GROUP BY can contain columns not in SELECT TODO Unsure how this applies to beam in particular E051-05 Select list items can be renamed N/A Beam uses this feature internally, the user never needs it E051-06 HAVING clause FULL guard_ and filter_ are appropriately converted to HAVING when allowed E051-07 Qualified * in select list N/A Beam handles projections instead E051-08 Correlation names in FROM TODO Unsure how this applies to beam E051-09 Rename columns in the FROM clause NONE Beam doesn't need this E061 Basic predicates and search conditions E061-01 Comparison predicate FULL Use the comparison operators (see E011-05) E061-02 BETWEEN predicate FULL Use between_ E061-03 IN predicate with list of values FULL Use in_ E061-04 LIKE predicate FULL Use like_ E061-05 LIKE predicate ESCAPE clause TODO Unsure how this would apply E061-06 NULL predicate FULL Use isNothing_ and isJust_ E061-07 Quantified comparison predicate FULL Use one of the quantified comparison operators ( ==*. , /=*. , <*. , >*. , <=*. , >=*. ) E051-08 EXISTS predicate FULL Use exists_ E061-09 Subqueries in comparison predicate FULL Use subquery_ as usual E061-11 Subqueries in IN predicate FULL E061-12 Subqueries in quantified comparison predicate FULL E061-13 Correlated subqueries FULL Use subquery_ E061-14 Search condition FULL Construct QGenExprs with type Bool E071 Basic query expressions E071-01 UNION DISTINCT table operator FULL Use union_ E071-02 UNION ALL table operator FULL Use unionAll_ E071-03 EXCEPT DISTINCT table operator FULL Use except_ E071-05 Columns combined via operators need not have same type WONTFIX Beam is strongly typed E071-06 Table operators in subqueries FULL Supported for backends that support it E081 Basic privileges NONE Database security is not beam's focus. beam-migrate may expose this in the future E091 Set functions E091-01 AVG FULL Use avg_ or avgOver_ E091-02 COUNT FULL Use countAll_ , countAllOver_ , count_ , or countOver_ E091-03 MAX FULL Use max_ or maxOver_ E091-04 MIN FULL Use min_ or minOver_ E091-05 SUM FULL Use sum_ or sumOver_ E091-06 ALL quantifier FULL Use the *Over_ functions with the allInGroupExplicitly_ quantifier E091-07 DISTINCT quantifier FULL Use the *Over_ functions with the distinctInGroup_ quantifier E101 Basic data manipulation E101-01 INSERT statement FULL Use insert and SqlInsert E101-03 Searched UPDATE FULL Use update and SqlUpdate E101-04 Searched DELETE FULL Use delete and SqlDelete E111 Single row SELECT statement FULL Use select as expected E121 Basic cursor support NONE Use the backends explicitly E131 Null value support PARTIAL Use Maybe column types, Nullable , and the just_ , nothing_ , and maybe_ functions E141 Basic integrity constraints Implemented in beam-migrate E141-01 NOT NULL constraints FULL Use notNull_ E141-02 UNIQUE constraints of NOT NULL columns TODO E141-03 PRIMARY KEY constraints FULL Instantiate Table with the correct PrimaryKey E141-04 Basic FOREIGN KEY constraints TODO You can embed the PrimaryKey of the relation directly. E141-06 CHECK constraints TODO E141-07 Column defaults FULL Use default_ from beam-migrate E141-08 NOT NULL inferred on PRIMARY KEY N/A E141-10 Names in a foreign key can be specified in any order N/A E151 Transaction support None Use the backend functions explicitly E152 SET TRANSACTION statement N/A E153 Updatable queries with subqueries TODO Not a common feature, but would be trivial to support E161 SQL comments with double minus N/A E171 SQLSTATE support N/A E182 Host language binding N/A F031 Basic schema manipulation F031-01 CREATE TABLE for persistent base tables FULL Use createTable_ in beam-migrate F031-02 CREATE VIEW statement TODO F031-03 GRANT statement TODO F031-04 ALTER TABLE statement: ADD COLUMN clause TODO F031-13 DROP TABLE statement: RESTRICT clause TODO F031-16 DROP VIEW statement: RESTRICT clause TODO F031-19 REVOKE statement: RESTRICT clause NONE See note for E081 F032 CASCADE drop behavior TODO Would be in beam-migrate F033 ALTER TABLE statement: DROP COLUMN clause TODO F034 Extended REVOKE statement NONE F041 Basic joined table F041-01 Inner join FULL Use the monadic join interface F041-02 INNER keyword N/A The INNER keyword is just syntactic sugar. The regular joins do what you want. F041-03 LEFT OUTER JOIN FULL Use leftJoin_ F041-04 RIGHT OUTER JOIN PARTIAL Supported in backend syntaxes, not exposed. Can always be written using LEFT OUTER JOIN F041-05 Outer joins can be nested FULL outerJoin_ can be nested arbitrarily F041-07 The inner table in outer join can be used in inner join TODO How does this apply to us? F041-08 All comparison operators in JOIN FULL Arbitrary QGenExpr s are supported. F051 Basic date and time F051-01 DATE data type FULL Use Day from Data.Time and val_ F051-02 TIME data type FULL Use TimeOfDay from Data.Time and val_ F051-03 TIMESTAMP datatype FULL Use LocalTime from Data.Time and val_ . Precision can be specified in beam-migrate F051-04 Comparison predicate on time types FULL Use comparison operatiors (See E011-05) F051-05 Explicit cast between date-time types and string TODO F051-06 CURRENT_DATE TODO F051-07 LOCALTIME TODO F051-08 LOCALTIMESTAMP TODO F081 UNION and EXCEPT in views FULL Views can use any query F111 Isolation levels other than SERIALIZABLE NONE Use backends F121 Basic diagnostics mangement NONE Use backends F122 Extended diagnostics management NONE Use backends F123 All diagnostics NONE Use backends F131 Grouped operations TODO Depends on grouped views F171 Multiple schemas per user N/A Depends on backend F191 Referential delete actions TODO F181 Multiple module support N/A F200 TRUNCATE TABLE statement TODO May be added in the future F201 CAST function FULL See cast_ F202 TRUNCATE TABLE: identity column restart option TODO Depends on F200 F221 Explicit defaults FULL Use default_ and insertExpressions when inserting F222 INSERT statement: DEFAULT VALUES clause TODO F251 Domain support PARTIAL Use DomainTypeEntity F261 CASE expression F261-01 Simple CASE TODO Use searched case (see F261-02) F261-02 Searched CASE FULL Use if_ , then_ , and else_ F261-03 NULLIF FULL Use nullIf_ F261-04 COALESCE FULL Use coalesce_ F262 Extended CASE expression WONTFIX Beam allows any expression in a WHEN condition F263 Comma-separater predicates in simple CASE expression WONTFIX Unnecessary F271 Compound character literals N/A This is syntactic sugar F281 LIKE enhancements FULL Supported in backends that support this F291 UNIQUE predicate FULL Use unique_ F301 CORRESPONDING in query expressions N/A Beam set functions work based off the query result type, not the column name F302 INTERSECT table operator FULL Use intersect_ F302-01 INTERSECT DISTINCT table operator FULL Use intersect_ F302-02 INTERSET ALL table operator FULL Use intersectAll_ F304 EXCEPT ALL table operator FULL Use exceptAll_ F311 Schema definition statement TODO Would be in beam-migrate F312 MERGE statement TODO F313 Enhanced MERGE statement TODO F314 MERGE statement with DELETE branch TODO F321 User authorization N/A F361 Subprogram support N/A F381 Extended schema manipulation TODO F382 Alter column data type TODO F384 Drop identity property clause TODO F385 Drop column generation expression clause TODO F386 Set identity column generation clause TODO F391 Long identifiers FULL Supported in backends that support it F392 Unicode escapes in identifiers TODO Unsure how this applies F393 Unicode escapes in literals TODO Unsure how this applies F394 Optional normal form specification N/A F401 Extended joined table FULL Full outer join using outerJoin_ . Natural join is not needed. A cross join is generated automatically when there are no join conditions. F402 Named column joins for LOBs, arrays, and multisets PARTIAL Supported in backends that support it F403 Partitioned join tables TODO F411 Time zone specification TODO F421 National character FULL Supported in beam-migrate as a data type for Text F431 Read-only scrollable cursors N/A Use the underlying backend F441 Extended set function support TODO F442 Mixed column references in set functions TODO Unsure how this would work with beam F451 Character set definition TODO Likely would go in beam-migrate F461 Named character sets TODO See F451 F491 Constraint management TODO F492 Optional table constraint enforcement TODO F521 Assertions TODO F531 Temporary tables TODO F481 Expanded NULL predicate FULL Supported in backends that support it F555 Enhanced seconds precision TODO F561 Full value expressions TODO F571 Truth value tests TODO F591 Derived tables TODO F611 Indicator data types TODO F641 Row and table constructors PARTIAL Use row_ (TODO) F651 Catalog name qualifiers TODO F661 Simple tables TODO F671 Subqueries in CHECK constraints TODO Planned with E141-06 F672 Retrospective CHECK constraints TODO Would require temporal DB support F690 Collation support PARTIAL beam-migrate supports some collation features F692 Enhanced collation support TODO F693 SQL-session and client module collations TODO F695 Translation support TODO F701 Referential update actions TODO F711 ALTER domain TODO F721 Deferrable constraints PARTIAL The syntax exists in beam-migrate F731 INSERT column privileges N/A F741 Referential MATCH type PARTIAL Exists in the syntax in beam-migrate , not exposed yet (TODO) F751 View CHECK enhancements TODO F761 Session management TODO F762 CURRENT_CATALOG TODO F763 CURRENT_SCHEMA TODO F812 Basic flagging N/A F841 LIKE_REGEX predicate TODO Easy F842 OCCURENCES_REGEX function TODO Easy F843 POSITION_REGEX function TODO Easy F844 SUBSTRING_REGEX function TODO Easy F845 TRANSLATE_REGEX function TODO Easy F846 Octet support in regular expression operators TODO F847 Nonconstant regular expression TODO Easy once regex support is added F850 Top-level in FULL Use orderBy_ as usual. Beam will do the right thing behind the scenes. F851 in subqueries FULL Works in backends that support it F852 Top-level in views FULL Views can use any query F855 Nested in UNKNOWN F856 Nested in N/A Beam automatically optimizes nested orderBy_ calls F857 Top-level in FULL limit_ and offset_ are correctly translated to dialect-specific pagination mechanisms F858 in subqueries FULL F859 Top-level in subqueries FULL * F860 dynamic in TODO * F861 Top-level in FULL See note for F587 F862 in subqueries FULL F863 Nested in FULL F864 Top-level in views FULL F865 dynamic in TODO F866 FETCH FIRST clause: PERCENT option TODO F867 FETCH FIRST clause: WITH TIES option TODO R010 Row pattern recognition: FROM clause TODO R020 Row pattern recognition: WINDOW clause TODO R030 Row pattern recognition: full aggregate support TODO S011 Distinct data types TODO S023 Basic structured types TODO S024 Enhanced structured types TODO S025 Final structured types TODO S026 Self-referencing structured types TODO S027 Create method by specific method name TODO S028 Permutable UDT options list TODO S041 Basic reference types TODO S043 Enhanced reference types TODO S051 Create table of type TODO S071 SQL paths in function and type name resolution N/A Beam qualifies everything anyway S081 Subtables PARTIAL You can use them right now, but there's no support for their creation or management in beam-migrate S091 Basic array support PARTIAL Supported in some backends ( beam-postgres for example) S092 Arrays of user-defined types TODO Depends on user-defined types S094 Arrays of reference types TODO S095 Array constructors by query PARTIAL S096 Optional array bounds PARTIAL Supported in beam-postgres S097 Array element assignment TODO Not yet, but should be easy enough in beam-postgres S098 ARRAY_AGG PARTIAL Supported in beam-postgres S111 ONLY in query expressions TODO S151 Type predicate TODO S161 Subtype treatment TODO S162 Subtype treatment for references TODO S201 SQL-invoked routines on arrays TODO Would be subsumed by sql-routines (T-321) S202 SQL-invoked routines on multisets TODO Would be subsumed by sql-routines (T-321) S211 User-defined cast functions TODO S231 Structured type locators TODO S232 Array locators TODO S233 Multiset locators TODO S241 Transform functions TODO S242 Alter transform statement TODO S251 User-defined orderings TODO S261 Specific type method TODO S271 Basic multiset support PARTIAL Supported in beam-postgres S272 Multisets of user-defined types TODO S274 Multisets reference types TODO S275 Advanced multiset support TODO S281 Nested collection types TODO S291 Unique constraint on entire row TODO S301 Enhanced UNNEST TODO S401 Distinct types based on array types TODO S402 Distinct types based on distinct types TODO S403 ARRAY_MAX_CARDINALITY TODO S404 TRIM_ARRAY TODO T021 BINARY and VARBINARY data types FULL T022 Advanced support for BINARY and VARBINARY data types TODO T023 Compound binary literals N/A Beam handles serialization T024 Spaces in binary literals N/A Beam handles serialization T031 Boolean data type FULL T041 Basic LOB data type support TODO T042 Extended LOB data type support TODO T043 Multiplier T TODO T044 Multiplier P TODO T051 Row types PARTIAL T061 UCS support TODO T071 BIGINT data type FULL T101 Enhanced nullability detection TODO T111 Updatable joins, unions, and columns TODO T121 WITH (excluding recursive) in query expression FULL Use selectWith , selecting , and reuse . See the section in the users guide T122 WITH (excluding recursive) in subquery TODO T131 Recursive query FULL Use selectWith and the MonadFix With instance. See the section for more details T132 Recursive query in subquery TODO T141 SIMILAR predicate FULL T151 DISTINCT predicate FULL T152 DISTINCT predicate with negation TODO T171 LIKE clause in table definition TODO T172 AS subquery clause in table definition TODO T173 Extended LIKE clause in table definition TODO T174 Identity columns TODO T175 Generated columns TODO T176 Sequence generator support TODO T177 Sequence generator support: simple restart option TODO T178 Identity columns: simple restart option TODO T180 System-versioned tables TODO T181 Application-time period tables TODO T191 Referential action RESTART TODO T201 Comparable data types for referential constraints TODO T211 Basic trigger capability TODO T212 Enhanced trigger capability TODO T213 INSTEAD OF triggers TODO T231 Sensitive cursors TODO T241 START TRANSACTION statement WONTFIX Use the backend library T251 SET TRANSACTION option: LOCAL option WONTFIX Use the backend library T261 Chained transactions N/A T271 Savepoints N/A T272 Enhanced savepoint management N/A T281 SELECT privilege with column granularity N/A T285 Enhanced derived column names N/A T301 Functional dependencies TODO T312 OVERLAY function TODO T321 Basic SQL-invoked routines TODO T323 Explicit security for external routines TODO T324 Explicit security for SQL routines TODO T325 Qualified SQL parameter references N/A Beam will likely use the qualified ones by default. Likely not exposed to user T326 Table functions TODO T331 Basic roles N/A T332 Extended roles N/A T341 Overleading of SQL-invoked functions and procodures WONTFIX Haskell doesn't allow overloading, and this seems complicated and unnecessary T351 Bracketed comments N/A T431 Extended grouping capabalities TODO T432 Nested and concatenated GROUPING SETs TODO T433 Multiargument GROUPING function TODO T434 GROUP BY DISTINCT TODO T441 ABS and MOD functions FULL T461 Symmetric BETWEEN predicate FULL Beam doesn't check this T471 Result sets return value TODO T472 DESCRIBE CURSOR N/A Use the backend library T491 LATERAL derived table TODO T495 Combined data change and retrieval TODO T501 Enhanced EXISTS predicate TODO T502 Period predicates TODO T511 Transaction counts TODO T521 Nested arguments in CALL statement TODO T522 Default values for IN parameters of SQL-invoked procs TODO T551 Optional key words for DEFAULT syntax TODO T561 Holdable locators TODO T571 Array-returning SQL-invoked functions TODO Will be supported once SQL-invoked functions are T572 Multiset-returning SQL-invoked functions TODO T581 Regular expression substring function TODO T591 UNIQUE constraints of possible NULL columns TODO T601 Local cursor references N/A T611 Elementary OLAP operations FULL See withWindow_ , window functions T612 Advanced OLAP operations PARTIAL No exclusions yet. See percentRank_ , cumeDist_ , and denseRank_ T613 Sampling TODO T614 NTILE function FULL ntile_ T615 LEAD and LAG function FULL lead1_ , lag1_ , lead_ , lag_ , leadWithDefault_ , lagWithDefault_ T616 Null treatment for LEAD and LAG functions TODO T617 FIRST_VALUE and LAST_VALUE function FULL lastValue_ and firstValue_ respectively T618 NTH_VALUE function FULL nthValue_ T619 Nested window function TODO T620 WINDOW clause: GROUPS option TODO T621 Enhanced numeric functions FULL All functions and aggregates in Database.Beam.Query.Extension T641 Multiple column assignment TODO T651 SQL-schema statements in SQL routines TODO T652 SQL-dynamic statements in SQL routines TODO T653 SQL-schema statements in external routines TODO T654 SQL-dynamic statements in external routines TODO T655 Cyclically dependent routines TODO","title":"Compatibility Matrix"},{"location":"about/faq/","text":"How does beam compare with <x> ? opaleye opaleye has similar aims as beam. Beam uses higher-kinded types to allow the use of 'normal' haskell data types, rather than a fully polymorphic type. For example, in opaleye you may have to write data Table column1 column2 column3 = Table { tblColumn1 :: column1 , tblColumn2 :: column2 , tblColumn3 :: column3 } This can get tiring when you have dozens of columns. In beam, types need only take one polymorphic parameter. data Table f = Table { tblColumn1 :: C f Column1Type , tblColumn2 :: C f Column2Type , tblColumn3 :: C f Column3Type } deriving ( Generic , Beamable ) Moreover, all beam instances and type synonyms are easily written by hand. There is no Template Haskell magic here. What you see is what you get. Beam is also fully polymorphic over the backend. That is to say that a beam query can be written once and used across multiple backends, so long as those backends support the features used in the query. Feature constraints are written as class constraints. For example, if you write a query that uses the SQL standard regr_slope function, you can make that query polymorphic over a choice in backend by using the IsSql2003EnhancedNumericFunctionsAggregationExpressionSyntax class. You can freely mix and match backends at any time (well, within the realms of possibility in terms of Haskell polymorphism). For example, the beam-migrate CLI tool loads backends at run-time and issues queries against them, without knowing the specifics of any particular backend. Finally, beam produces readable queries. Here is what opaleye produces on a left join: personBirthdayLeftJoin :: Query ((Column PGText, Column PGInt4, Column PGText), ColumnNullableBirthday) personBirthdayLeftJoin = leftJoin personQuery birthdayQuery eqName where eqName ((name, _, _), birthdayRow) = name .== bdName birthdayRow The generated SQL is ghci> printSql personBirthdayLeftJoin SELECT result1_0_3 as result1, result1_1_3 as result2, result1_2_3 as result3, result2_0_3 as result4, result2_1_3 as result5 FROM (SELECT * FROM (SELECT name0_1 as result1_0_3, age1_1 as result1_1_3, address2_1 as result1_2_3, name0_2 as result2_0_3, birthday1_2 as result2_1_3 FROM (SELECT * FROM (SELECT name as name0_1, age as age1_1, address as address2_1 FROM personTable as T1) as T1) as T1 LEFT OUTER JOIN (SELECT * FROM (SELECT name as name0_2, birthday as birthday1_2 FROM birthdayTable as T1) as T1) as T2 ON (name0_1) = (name0_2)) as T1) as T1 A similar query in beam: Haskell Postgres Sqlite do artist <- all_ ( artist chinookDb ) album <- leftJoin_ ( all_ ( album chinookDb )) ( \\ album -> albumArtist album ==. primaryKey artist ) pure ( artist , album ) SELECT \"t0\" . \"ArtistId\" AS \"res0\" , \"t0\" . \"Name\" AS \"res1\" , \"t1\" . \"AlbumId\" AS \"res2\" , \"t1\" . \"Title\" AS \"res3\" , \"t1\" . \"ArtistId\" AS \"res4\" FROM \"Artist\" AS \"t0\" LEFT JOIN \"Album\" AS \"t1\" ON ( \"t1\" . \"ArtistId\" ) = ( \"t0\" . \"ArtistId\" ) SELECT \"t0\" . \"ArtistId\" AS \"res0\" , \"t0\" . \"Name\" AS \"res1\" , \"t1\" . \"AlbumId\" AS \"res2\" , \"t1\" . \"Title\" AS \"res3\" , \"t1\" . \"ArtistId\" AS \"res4\" FROM \"Artist\" AS \"t0\" LEFT JOIN \"Album\" AS \"t1\" ON ( \"t1\" . \"ArtistId\" ) = ( \"t0\" . \"ArtistId\" ); -- With values: [] persistent / esqueleto Persistent is a simple relational mapper for Haskell. Like beam, it also supports multiple backends. However, unlike beam , it does not offer a DSL for expressing joins or complex queries. Many people use the esqueleto library to add these features to persistent. However, esqueleto allows a number of constructs to compile that lead to run-time errors. In particular, join ON conditions need to match the order specified in the FROM clause, but this is not checked at compile time. In contrast, beam handles this for you. If the query compiles, it will generate proper code. Moreover, beam's approach is more flexible. Esqueleto relies on pre-defined algebraic data types. Beam uses a finally tagless encoding so that external packages can provide completely new functionality. For example, beam-postgres is packaged independently of beam-core and offers several advanced features, such as row-level locking, JSON support, etc, without requiring any changes to core modules. An OpenGIS package is also in the works, and beam's approach means this will be packaged separately from core. groundhog Groundhog is a fork of persistent and it shares many of the same goals as well as restrictions. It, also, does not offer a DSL for expressing joins or complex queries. Moreover it currently lacks a companion library similar to esqueleto . Thus, groundhog can be useful for making some basic queries safe, while more complex things must be handled through a raw query escape hatch or directly through a backend library such as postgresql-simple . hasql hasql is a library offering compile-time checking of queries using a quasiquoter. It's really great if you want to write your own SQL query and embed it in your Haskell source. However, it cannot check that the shape of the data returned by the query matches what your code expects. For example, if you issue a command SELECT a, b, c, d, e , but then unpack a 6-tuple in your Haskell code, this will lead to a run-time error. Beam is much more heavy weight but guarantees that the shape of data matches. Also, beam allows you to write and compose queries in a very straightforward Haskell style, that is more expressive than vanilla SQL. selda selda has similar aims as beam. However, beam encourages the use of Haskell record types, whereas selda has its own type-level combinators for constructing table types. This makes it more straightforward to use beam types in pre-existing applications. Beam also has more robust support for migrations. Beam backends typically map more features than selda ones. squeal Help! The type checker keeps complaining about Syntax types Suppose you had the following code to run a query over an arbitrary backend that supported the SQL92 syntax. listEmployees :: ( IsSql92Syntax cmd , MonadBeam cmd be m ) => m [ Employee ] listEmployees = runSelectReturningList $ select ( all_ ( employees employeeDb )) You may get an error message like the following MyQueries.hs:1:1: error: * Could not deduce: Sql92ProjectionExpressionSyntax (Sql92SelectTableProjectionSyntax (Sql92SelectSelectTableSyntax (Sql92SelectSyntax cmd))) ~ Sql92SelectTableExpressionSyntax (Sql92SelectSelectTableSyntax (Sql92SelectSyntax cmd)) arising from a use of 'select' Beam uses a finally-tagless encoding for syntaxes. This means we never deal with concrete syntax types internally, just types that fulfill certain constraints (in this case, being a valid description of a SQL92 syntax). This works really nicely for extensibility, but makes the types slightly confusing. Here, the type checker is complaining that it cannot prove that the type of expressions used in projections is the same as the type of expressions used in WHERE and HAVING clauses. Of course, any sane SQL92 syntax would indeed meet this criteria, but this criteria is difficult to enforce at the type class level (it leads to cycles in superclasses, which requires the scary-looking UndecidableSuperclasses extension in GHC). Nevertheless, we can avoid all this hullabaloo by using the Sql92SanityCheck constraint synonym. This takes a command syntax and asserts all the type equalities that a sane SQL92 syntax would support. Thus the code above becomes. listEmployees :: ( IsSql92Syntax cmd , Sql92SanityCheck cmd , MonadBeam cmd be m ) => m [ Employee ] listEmployees = runSelectReturningList $ select ( all_ ( employees employeeDb )) Other database mappers simulate features on databases that lack support, why not beam? Beam assumes that the developer has picked their RDBMS for a reason. Beam does not try to take on features of the RDBMS, because often there is no reasonable and equally performant substitution that can be made. Beam tries to follow the principle of least surprise -- the SQL queries beam generates should be easily guessable from the Haskell query DSL (modulo aliasing). Generating complicated emulation code which can result in unpredictable performance would violate this principle.","title":"Frequently Asked Questions"},{"location":"about/faq/#how-does-beam-compare-with-x","text":"","title":"How does beam compare with &lt;x&gt;?"},{"location":"about/faq/#opaleye","text":"opaleye has similar aims as beam. Beam uses higher-kinded types to allow the use of 'normal' haskell data types, rather than a fully polymorphic type. For example, in opaleye you may have to write data Table column1 column2 column3 = Table { tblColumn1 :: column1 , tblColumn2 :: column2 , tblColumn3 :: column3 } This can get tiring when you have dozens of columns. In beam, types need only take one polymorphic parameter. data Table f = Table { tblColumn1 :: C f Column1Type , tblColumn2 :: C f Column2Type , tblColumn3 :: C f Column3Type } deriving ( Generic , Beamable ) Moreover, all beam instances and type synonyms are easily written by hand. There is no Template Haskell magic here. What you see is what you get. Beam is also fully polymorphic over the backend. That is to say that a beam query can be written once and used across multiple backends, so long as those backends support the features used in the query. Feature constraints are written as class constraints. For example, if you write a query that uses the SQL standard regr_slope function, you can make that query polymorphic over a choice in backend by using the IsSql2003EnhancedNumericFunctionsAggregationExpressionSyntax class. You can freely mix and match backends at any time (well, within the realms of possibility in terms of Haskell polymorphism). For example, the beam-migrate CLI tool loads backends at run-time and issues queries against them, without knowing the specifics of any particular backend. Finally, beam produces readable queries. Here is what opaleye produces on a left join: personBirthdayLeftJoin :: Query ((Column PGText, Column PGInt4, Column PGText), ColumnNullableBirthday) personBirthdayLeftJoin = leftJoin personQuery birthdayQuery eqName where eqName ((name, _, _), birthdayRow) = name .== bdName birthdayRow The generated SQL is ghci> printSql personBirthdayLeftJoin SELECT result1_0_3 as result1, result1_1_3 as result2, result1_2_3 as result3, result2_0_3 as result4, result2_1_3 as result5 FROM (SELECT * FROM (SELECT name0_1 as result1_0_3, age1_1 as result1_1_3, address2_1 as result1_2_3, name0_2 as result2_0_3, birthday1_2 as result2_1_3 FROM (SELECT * FROM (SELECT name as name0_1, age as age1_1, address as address2_1 FROM personTable as T1) as T1) as T1 LEFT OUTER JOIN (SELECT * FROM (SELECT name as name0_2, birthday as birthday1_2 FROM birthdayTable as T1) as T1) as T2 ON (name0_1) = (name0_2)) as T1) as T1 A similar query in beam: Haskell Postgres Sqlite do artist <- all_ ( artist chinookDb ) album <- leftJoin_ ( all_ ( album chinookDb )) ( \\ album -> albumArtist album ==. primaryKey artist ) pure ( artist , album ) SELECT \"t0\" . \"ArtistId\" AS \"res0\" , \"t0\" . \"Name\" AS \"res1\" , \"t1\" . \"AlbumId\" AS \"res2\" , \"t1\" . \"Title\" AS \"res3\" , \"t1\" . \"ArtistId\" AS \"res4\" FROM \"Artist\" AS \"t0\" LEFT JOIN \"Album\" AS \"t1\" ON ( \"t1\" . \"ArtistId\" ) = ( \"t0\" . \"ArtistId\" ) SELECT \"t0\" . \"ArtistId\" AS \"res0\" , \"t0\" . \"Name\" AS \"res1\" , \"t1\" . \"AlbumId\" AS \"res2\" , \"t1\" . \"Title\" AS \"res3\" , \"t1\" . \"ArtistId\" AS \"res4\" FROM \"Artist\" AS \"t0\" LEFT JOIN \"Album\" AS \"t1\" ON ( \"t1\" . \"ArtistId\" ) = ( \"t0\" . \"ArtistId\" ); -- With values: []","title":"opaleye"},{"location":"about/faq/#persistentesqueleto","text":"Persistent is a simple relational mapper for Haskell. Like beam, it also supports multiple backends. However, unlike beam , it does not offer a DSL for expressing joins or complex queries. Many people use the esqueleto library to add these features to persistent. However, esqueleto allows a number of constructs to compile that lead to run-time errors. In particular, join ON conditions need to match the order specified in the FROM clause, but this is not checked at compile time. In contrast, beam handles this for you. If the query compiles, it will generate proper code. Moreover, beam's approach is more flexible. Esqueleto relies on pre-defined algebraic data types. Beam uses a finally tagless encoding so that external packages can provide completely new functionality. For example, beam-postgres is packaged independently of beam-core and offers several advanced features, such as row-level locking, JSON support, etc, without requiring any changes to core modules. An OpenGIS package is also in the works, and beam's approach means this will be packaged separately from core.","title":"persistent/esqueleto"},{"location":"about/faq/#groundhog","text":"Groundhog is a fork of persistent and it shares many of the same goals as well as restrictions. It, also, does not offer a DSL for expressing joins or complex queries. Moreover it currently lacks a companion library similar to esqueleto . Thus, groundhog can be useful for making some basic queries safe, while more complex things must be handled through a raw query escape hatch or directly through a backend library such as postgresql-simple .","title":"groundhog"},{"location":"about/faq/#hasql","text":"hasql is a library offering compile-time checking of queries using a quasiquoter. It's really great if you want to write your own SQL query and embed it in your Haskell source. However, it cannot check that the shape of the data returned by the query matches what your code expects. For example, if you issue a command SELECT a, b, c, d, e , but then unpack a 6-tuple in your Haskell code, this will lead to a run-time error. Beam is much more heavy weight but guarantees that the shape of data matches. Also, beam allows you to write and compose queries in a very straightforward Haskell style, that is more expressive than vanilla SQL.","title":"hasql"},{"location":"about/faq/#selda","text":"selda has similar aims as beam. However, beam encourages the use of Haskell record types, whereas selda has its own type-level combinators for constructing table types. This makes it more straightforward to use beam types in pre-existing applications. Beam also has more robust support for migrations. Beam backends typically map more features than selda ones.","title":"selda"},{"location":"about/faq/#squeal","text":"","title":"squeal"},{"location":"about/faq/#help-the-type-checker-keeps-complaining-about-syntax-types","text":"Suppose you had the following code to run a query over an arbitrary backend that supported the SQL92 syntax. listEmployees :: ( IsSql92Syntax cmd , MonadBeam cmd be m ) => m [ Employee ] listEmployees = runSelectReturningList $ select ( all_ ( employees employeeDb )) You may get an error message like the following MyQueries.hs:1:1: error: * Could not deduce: Sql92ProjectionExpressionSyntax (Sql92SelectTableProjectionSyntax (Sql92SelectSelectTableSyntax (Sql92SelectSyntax cmd))) ~ Sql92SelectTableExpressionSyntax (Sql92SelectSelectTableSyntax (Sql92SelectSyntax cmd)) arising from a use of 'select' Beam uses a finally-tagless encoding for syntaxes. This means we never deal with concrete syntax types internally, just types that fulfill certain constraints (in this case, being a valid description of a SQL92 syntax). This works really nicely for extensibility, but makes the types slightly confusing. Here, the type checker is complaining that it cannot prove that the type of expressions used in projections is the same as the type of expressions used in WHERE and HAVING clauses. Of course, any sane SQL92 syntax would indeed meet this criteria, but this criteria is difficult to enforce at the type class level (it leads to cycles in superclasses, which requires the scary-looking UndecidableSuperclasses extension in GHC). Nevertheless, we can avoid all this hullabaloo by using the Sql92SanityCheck constraint synonym. This takes a command syntax and asserts all the type equalities that a sane SQL92 syntax would support. Thus the code above becomes. listEmployees :: ( IsSql92Syntax cmd , Sql92SanityCheck cmd , MonadBeam cmd be m ) => m [ Employee ] listEmployees = runSelectReturningList $ select ( all_ ( employees employeeDb ))","title":"Help! The type checker keeps complaining about Syntax types"},{"location":"about/faq/#other-database-mappers-simulate-features-on-databases-that-lack-support-why-not-beam","text":"Beam assumes that the developer has picked their RDBMS for a reason. Beam does not try to take on features of the RDBMS, because often there is no reasonable and equally performant substitution that can be made. Beam tries to follow the principle of least surprise -- the SQL queries beam generates should be easily guessable from the Haskell query DSL (modulo aliasing). Generating complicated emulation code which can result in unpredictable performance would violate this principle.","title":"Other database mappers simulate features on databases that lack support, why not beam?"},{"location":"about/license/","text":"The MIT License (MIT) Copyright \u00a9 2015-2020 Travis Athougies and the Beam authors Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \u201cSoftware\u201d), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \u201cAS IS\u201d, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.","title":"License"},{"location":"about/license/#the-mit-license-mit","text":"Copyright \u00a9 2015-2020 Travis Athougies and the Beam authors Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \u201cSoftware\u201d), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \u201cAS IS\u201d, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.","title":"The MIT License (MIT)"},{"location":"about/release-notes/","text":"Beam Release Notes 0.7.0.0 Correct boolean handling Previous versions of beam used the SQL = operator to compare potentially NULL values. This is incorrect, as NULL = NULL => UNKNOWN in ANSI-compliant implementations. Beam has changed its emitted SQL to produce proper comparisons, but this can dramatically affect performance in some backends. Particularly, proper JOIN index usage in Postgres requires an exact match on an equality constructor, which may not be what you get when using the proper boolean handling. If you are okay using SQL null handling, you can use the new ==?. and /=?. operators which produce an expression with type SqlBool instead. SqlBool is a type that can represent the SQL BOOL type in all its gritty glory. Note however, that these operators do not compare for haskell equality, only SQL equality, so please understand what that means before using them. Correspondingly, many functions that took Bool expressions now have corresponding versions that take SqlBool . For example, to use guard_ with a SqlBool expression use guard_' (note the prime). (Note: I don't really like that we have to do this, but this is the only way unless we introspect user expressions. Beam's philosophy is to be as direct as possible. The ==. operator corresponds to haskell == , and so produces the boolean we would expect as Haskell programmers. The ==?. operator is a new operator that users must explicitly opt in to. Both produce the most direct code possible on each backend.) Aggregations return Maybe types In previous versions of beam, aggregations such as avg_ , sum_ , etc returned the an expression of the same type as its inputs. However, this does not match standard SQL behavior, where these aggregates can return NULL if no rows are selected for the aggregation. This breaks older code, but is more correct. To restore the older behavior, use the fromMaybe_ function to supply a default value. Miscellaneous name changes The Database.Beam.Query.lookup function was renamed to lookup_ to avoid overlap with the Prelude function of the same name. Reintroduce explicit backends to Database class Some database entites only work on particular backends. For example, beam-postgres extension support only works in beam-postgres. The lack of a backend parameter on the Database type class essentially mandated that every database entity worked on every backend. By introducing a backend parameter to Database , we allow the user to restrict which backends a database can work on. The old behavior is still easily recovered. Whereas before you'd write instance Database MyDatabase Now write instance Database be MyDatabase Require backends to explicitly declare types that can be compared for equality Beam previously allowed any two types to be compared for SQL equality. This is no longer the case. Rather, only types that are instances of HasSqlEqualityCheck for the given expression syntax can be checked for equality. Correspondingly, only types that are instances of HasSqlQuantifiedEqualityCheck can be checked for quantified equality. This change is somewhat invasive, as the relationship and join operators depend on the ability to check primary keys for equality. You may have to add appropriate class constraints to your queries. In order to assert that a table can be compared for equality, you can use the HasTableEquality constraint synonym. For Backends Backend implementors should establish instances of HasSqlEqualityCheck and HasSqlQuantifiedEqualityCheck for every type that can be compared in their syntax. You may choose to implement a custom equality and inequality operator. Alternatively, you can leave the instances empty to use the defaults, which match the old behavior. Properly deal with NULL values in equality Previous versions of Beam would use SQL = and <> operators to compare potentially NULL values. However, NULL = NULL is always false, according to the SQL standard, so this behavior is incorrect. Now, Beam will generate a CASE .. WHEN .. statement to explicitly handle mismatching NULL s. This is the 'expected' behavior from the Haskell perspective, but does not match what one may expect in SQL. Note that it is always better to explicitly handle NULL s using maybe_ , and beam recommends this approach in robust code. Remove Auto for fields with default values Auto was a convenience type for dealing with tables where some columns have been given a default value. Auto worked well enough but it was a very leaky abstraction. Moreover, it was unnecessary. Everything you can do with Auto can be done more safely with default_ . For example, instead of using insertValues [ Table1 ( Auto Nothing ) \"Field Value\" \"Another Field Value\" ] use insertExpressions [ Table1 default_ ( val_ \"Field Value\" ) ( val_ \"Another Field Value\" ) ] 0.6.0.0 Mostly complete SQL92, SQL99 support Piecemeal support for SQL2003 and SQL2008 features Completely modular backends Various bug improvements and fixes 0.5.0.0 Move to using finally tagless style for SQL generation Split out backends from beam-core Allow non-table entities to be stored in databases Basic migrations support","title":"Release Notes"},{"location":"about/release-notes/#beam-release-notes","text":"","title":"Beam Release Notes"},{"location":"about/release-notes/#0700","text":"","title":"0.7.0.0"},{"location":"about/release-notes/#correct-boolean-handling","text":"Previous versions of beam used the SQL = operator to compare potentially NULL values. This is incorrect, as NULL = NULL => UNKNOWN in ANSI-compliant implementations. Beam has changed its emitted SQL to produce proper comparisons, but this can dramatically affect performance in some backends. Particularly, proper JOIN index usage in Postgres requires an exact match on an equality constructor, which may not be what you get when using the proper boolean handling. If you are okay using SQL null handling, you can use the new ==?. and /=?. operators which produce an expression with type SqlBool instead. SqlBool is a type that can represent the SQL BOOL type in all its gritty glory. Note however, that these operators do not compare for haskell equality, only SQL equality, so please understand what that means before using them. Correspondingly, many functions that took Bool expressions now have corresponding versions that take SqlBool . For example, to use guard_ with a SqlBool expression use guard_' (note the prime). (Note: I don't really like that we have to do this, but this is the only way unless we introspect user expressions. Beam's philosophy is to be as direct as possible. The ==. operator corresponds to haskell == , and so produces the boolean we would expect as Haskell programmers. The ==?. operator is a new operator that users must explicitly opt in to. Both produce the most direct code possible on each backend.)","title":"Correct boolean handling"},{"location":"about/release-notes/#aggregations-return-maybe-types","text":"In previous versions of beam, aggregations such as avg_ , sum_ , etc returned the an expression of the same type as its inputs. However, this does not match standard SQL behavior, where these aggregates can return NULL if no rows are selected for the aggregation. This breaks older code, but is more correct. To restore the older behavior, use the fromMaybe_ function to supply a default value.","title":"Aggregations return Maybe types"},{"location":"about/release-notes/#miscellaneous-name-changes","text":"The Database.Beam.Query.lookup function was renamed to lookup_ to avoid overlap with the Prelude function of the same name.","title":"Miscellaneous name changes"},{"location":"about/release-notes/#reintroduce-explicit-backends-to-database-class","text":"Some database entites only work on particular backends. For example, beam-postgres extension support only works in beam-postgres. The lack of a backend parameter on the Database type class essentially mandated that every database entity worked on every backend. By introducing a backend parameter to Database , we allow the user to restrict which backends a database can work on. The old behavior is still easily recovered. Whereas before you'd write instance Database MyDatabase Now write instance Database be MyDatabase","title":"Reintroduce explicit backends to Database class"},{"location":"about/release-notes/#require-backends-to-explicitly-declare-types-that-can-be-compared-for-equality","text":"Beam previously allowed any two types to be compared for SQL equality. This is no longer the case. Rather, only types that are instances of HasSqlEqualityCheck for the given expression syntax can be checked for equality. Correspondingly, only types that are instances of HasSqlQuantifiedEqualityCheck can be checked for quantified equality. This change is somewhat invasive, as the relationship and join operators depend on the ability to check primary keys for equality. You may have to add appropriate class constraints to your queries. In order to assert that a table can be compared for equality, you can use the HasTableEquality constraint synonym.","title":"Require backends to explicitly declare types that can be compared for equality"},{"location":"about/release-notes/#for-backends","text":"Backend implementors should establish instances of HasSqlEqualityCheck and HasSqlQuantifiedEqualityCheck for every type that can be compared in their syntax. You may choose to implement a custom equality and inequality operator. Alternatively, you can leave the instances empty to use the defaults, which match the old behavior.","title":"For Backends"},{"location":"about/release-notes/#properly-deal-with-null-values-in-equality","text":"Previous versions of Beam would use SQL = and <> operators to compare potentially NULL values. However, NULL = NULL is always false, according to the SQL standard, so this behavior is incorrect. Now, Beam will generate a CASE .. WHEN .. statement to explicitly handle mismatching NULL s. This is the 'expected' behavior from the Haskell perspective, but does not match what one may expect in SQL. Note that it is always better to explicitly handle NULL s using maybe_ , and beam recommends this approach in robust code.","title":"Properly deal with NULL values in equality"},{"location":"about/release-notes/#remove-auto-for-fields-with-default-values","text":"Auto was a convenience type for dealing with tables where some columns have been given a default value. Auto worked well enough but it was a very leaky abstraction. Moreover, it was unnecessary. Everything you can do with Auto can be done more safely with default_ . For example, instead of using insertValues [ Table1 ( Auto Nothing ) \"Field Value\" \"Another Field Value\" ] use insertExpressions [ Table1 default_ ( val_ \"Field Value\" ) ( val_ \"Another Field Value\" ) ]","title":"Remove Auto for fields with default values"},{"location":"about/release-notes/#0600","text":"Mostly complete SQL92, SQL99 support Piecemeal support for SQL2003 and SQL2008 features Completely modular backends Various bug improvements and fixes","title":"0.6.0.0"},{"location":"about/release-notes/#0500","text":"Move to using finally tagless style for SQL generation Split out backends from beam-core Allow non-table entities to be stored in databases Basic migrations support","title":"0.5.0.0"},{"location":"schema-guide/alter-table/","text":"ALTer table support","title":"Alter table"},{"location":"schema-guide/create/","text":"Create TABLE support","title":"Create"},{"location":"schema-guide/delete/","text":"drop table support","title":"Delete"},{"location":"schema-guide/library/","text":"supported","title":"The beam-migrate library"},{"location":"schema-guide/migrations/","text":"In the User Guide we saw how to declare a schema for an already created database and use it to perform queries. Beam can also manage a database schema based on Haskell datatypes you feed it. The migrations framework is meant to be a robust and modular way of managing schema changes. It is an optional part of beam provided in the beam-migrate package. Install the migrations framework by running. $ cabal install beam-migrate # or $ stack install beam-migrate Basic concepts In the user guide, we saw how we can use defaultDbSettings to generate default metadata that can be used to access the database. This default metadata is enough to query, but not enough for beam-migrate . beam-migrate offers the defaultMigratableDbSettings function, which annotates the database schema with additional information. Whereas defaultDbSettings yields a value of type DatabaseSettings be db , defaultMigratableDbSettings yields a value of type CheckedDatabaseSettings be db . You can recover a DatabaseSettings be db from a CheckedDatabaseSettings be db value by applying the unCheckDatabase function. The CheckedDatabaseSettings value contains the original DatabaseSettings along with a series of predicates . Each predicate describes one aspect of the database. As far as beam-migrate is concerned, each database schema is fully specified by the set of predicates that apply to it. beam-migrate calls this the checked type of the database. For example, a database schema that consists of one table named table with no fields is represented uniquely by the checked type of [TableExistsPredicate \"table\"] . If you add a field field1 of type INT to the table, then the checked type becomes [TableExistsPredicate \"table\", TableHasColumn \"table\" \"field1\" intType] . Note The types are a bit more complicated than what they appear. In particular, a predicate can be of any type that satisfies the DatabasePredicate type class. The predicates can be stored in a ( heterogenous ) list because they are wrapped in the SomeDatabasePredicate GADT that holds the type class instance as well. Usage modes beam-migrate can be used as a library or a command-line tool in managed or unmanaged mode. The beam-migrate library The beam-migrate library provides syntax definitions for common SQL DDL tasks. It also provides types for expressing migrations as transformations of one or more schemas to another. beam-migrate offers a built-in way to apply these migrations to a production database, running only those migrations that are necessary. You can also directly interpret the beam-migrate DSL to hook your Haskell migrations into your own system. beam-migrate is described in more detail in the beam-migrate migrations guide The beam-migrate tool There is an optional beam-migrate command line tool, available in the beam-migrate-cli package. The beam-migrate tool can generate a beam schema from a pre-existing database, manage migrations for several production databases, automatically generate migrations between two schemas, and much more. It is rather opinionated, and is described in more detail in the beam-migrate CLI guide Automatic migration generation Given two CheckedDatabaseSettings values, beam-migrate can generate a set of SQL steps that will transform one schema to another. The generation of such steps is an exceedingly difficult problem in general. beam-migrate can automatically handle most common cases, but it will not always succeed. In this case, it can present to you a list of steps it thinks are best as well as what remains to be solved. The migration generation is implemented as a proof search in linear logic 1 . In particular, beam-migrate views a migration as a linear logic proof of the form a \u22b8 b , where a is the set of predicates of the original schema and b is the set of predicates in the target schema. beam-migrate ships with a set of default proof steps. Backends can add to these steps for backend-specific predicates. Note At this time, Haskell does not allow the expression of linear programs (this will change with the introduction of linear types). Thus, migrations written in Haskell are not checked by GHC for linear-ness, but beam-migrate will validate such migrations at run-time to the best of its ability. The migration prover may not be able to find a migration in a sufficiently short period of time. beam-migrate 's algorithm is designed to terminate, but this may take a while. Additionally, the prover will not automatically generate steps for some migrations. For example, beam-migrate will never rename a table without explicit instructions. Advantages of checked migrations Unlike other database migration frameworks, the checking process allows beam-migrate to be sure that the migration you specify will result in the database type you want. Also, checked migrations allow the programmer to verify that the database they are accessing indeed matches what their schema expects. Linear logic is a type of logic first described by Jean-Yves Gerard. In particular, it constrains the weakening and strengthening rules of classical logic. Intuitively, you can think of it as forcing the rule that each assumption is used exactly once to produce the result. Read more on Wikipedia . \u21a9","title":"The Migrations Framework"},{"location":"schema-guide/migrations/#basic-concepts","text":"In the user guide, we saw how we can use defaultDbSettings to generate default metadata that can be used to access the database. This default metadata is enough to query, but not enough for beam-migrate . beam-migrate offers the defaultMigratableDbSettings function, which annotates the database schema with additional information. Whereas defaultDbSettings yields a value of type DatabaseSettings be db , defaultMigratableDbSettings yields a value of type CheckedDatabaseSettings be db . You can recover a DatabaseSettings be db from a CheckedDatabaseSettings be db value by applying the unCheckDatabase function. The CheckedDatabaseSettings value contains the original DatabaseSettings along with a series of predicates . Each predicate describes one aspect of the database. As far as beam-migrate is concerned, each database schema is fully specified by the set of predicates that apply to it. beam-migrate calls this the checked type of the database. For example, a database schema that consists of one table named table with no fields is represented uniquely by the checked type of [TableExistsPredicate \"table\"] . If you add a field field1 of type INT to the table, then the checked type becomes [TableExistsPredicate \"table\", TableHasColumn \"table\" \"field1\" intType] . Note The types are a bit more complicated than what they appear. In particular, a predicate can be of any type that satisfies the DatabasePredicate type class. The predicates can be stored in a ( heterogenous ) list because they are wrapped in the SomeDatabasePredicate GADT that holds the type class instance as well.","title":"Basic concepts"},{"location":"schema-guide/migrations/#usage-modes","text":"beam-migrate can be used as a library or a command-line tool in managed or unmanaged mode.","title":"Usage modes"},{"location":"schema-guide/migrations/#the-beam-migrate-library","text":"The beam-migrate library provides syntax definitions for common SQL DDL tasks. It also provides types for expressing migrations as transformations of one or more schemas to another. beam-migrate offers a built-in way to apply these migrations to a production database, running only those migrations that are necessary. You can also directly interpret the beam-migrate DSL to hook your Haskell migrations into your own system. beam-migrate is described in more detail in the beam-migrate migrations guide","title":"The beam-migrate library"},{"location":"schema-guide/migrations/#the-beam-migrate-tool","text":"There is an optional beam-migrate command line tool, available in the beam-migrate-cli package. The beam-migrate tool can generate a beam schema from a pre-existing database, manage migrations for several production databases, automatically generate migrations between two schemas, and much more. It is rather opinionated, and is described in more detail in the beam-migrate CLI guide","title":"The beam-migrate tool"},{"location":"schema-guide/migrations/#automatic-migration-generation","text":"Given two CheckedDatabaseSettings values, beam-migrate can generate a set of SQL steps that will transform one schema to another. The generation of such steps is an exceedingly difficult problem in general. beam-migrate can automatically handle most common cases, but it will not always succeed. In this case, it can present to you a list of steps it thinks are best as well as what remains to be solved. The migration generation is implemented as a proof search in linear logic 1 . In particular, beam-migrate views a migration as a linear logic proof of the form a \u22b8 b , where a is the set of predicates of the original schema and b is the set of predicates in the target schema. beam-migrate ships with a set of default proof steps. Backends can add to these steps for backend-specific predicates. Note At this time, Haskell does not allow the expression of linear programs (this will change with the introduction of linear types). Thus, migrations written in Haskell are not checked by GHC for linear-ness, but beam-migrate will validate such migrations at run-time to the best of its ability. The migration prover may not be able to find a migration in a sufficiently short period of time. beam-migrate 's algorithm is designed to terminate, but this may take a while. Additionally, the prover will not automatically generate steps for some migrations. For example, beam-migrate will never rename a table without explicit instructions.","title":"Automatic migration generation"},{"location":"schema-guide/migrations/#advantages-of-checked-migrations","text":"Unlike other database migration frameworks, the checking process allows beam-migrate to be sure that the migration you specify will result in the database type you want. Also, checked migrations allow the programmer to verify that the database they are accessing indeed matches what their schema expects. Linear logic is a type of logic first described by Jean-Yves Gerard. In particular, it constrains the weakening and strengthening rules of classical logic. Intuitively, you can think of it as forcing the rule that each assumption is used exactly once to produce the result. Read more on Wikipedia . \u21a9","title":"Advantages of checked migrations"},{"location":"schema-guide/tool/","text":"Tool","title":"The beam-migrate tool"},{"location":"tutorials/tutorial1/","text":"In this tutorial sequence, we'll walk through creating a schema for a simple shopping cart database. We'll start by defining a user table. Then, we'll show how beam makes it easy to manipulate data in our database. Finally, we'll demonstrate how beam lets us declare type-safe and composable queries. Beam Module Structure Beam makes extensive use of GHC's Generics mechanism. This extension means beam does not need to rely on template haskell. To start defining beam schemas and queries, you only need to import the Database.Beam module. To interface with an actual database, you'll need to import one of the database backends. We'll see how to use the Sqlite backend here (found in the beam-sqlite package). Now, open up a GHCi prompt for us to use. Make sure to get the beam-core , beam-sqlite and text packages. $ stack repl --package beam-core --package beam-sqlite --package sqlite-simple --package beam-migrate --package text This will put you into a GHCi prompt with the beam-core and beam-sqlite packages available. We also include the sqlite-simple package. Beam mainly manages querying and data marshalling. Connections to the backends are done via backend specific packages. In this case, beam-sqlite uses the sqlite-simple backend. Before starting, we'll need to enable some extensions. > : set - XDeriveGeneric - XGADTs - XOverloadedStrings - XFlexibleContexts - XFlexibleInstances - XTypeFamilies - XTypeApplications - XDeriveAnyClass And import some modules... import Database.Beam import Database.Beam.Sqlite import Data.Text ( Text ) Defining our first table Beam tables are regular Haskell data types with a bit of scaffolding. Thankfully, the magic of the modern Haskell type system allows us to remove the overhead and the syntactic fuzz of the scaffolding in most situations. We start by declaring a data structure named UserT . As a matter of convention, Beam table types are suffixed with 'T'. Table types have only one constructor. Again, as a matter of convention, the constructor has the same name as the table, but without the 'T' suffix. We'll soon see the reason for this convention. In this tutorial, I'll prefix all record selectors with an underscore. This is a matter of personal preference. One reason for the prefix is that it plays nicely with the lens library. Beam does not necessitate the use of lens (in fact Beam includes its own mechanism to generically derive van Laarhoven lenses), but I recognize that some programmers use lens quite a lot. data UserT f = User { _userEmail :: Columnar f Text , _userFirstName :: Columnar f Text , _userLastName :: Columnar f Text , _userPassword :: Columnar f Text } deriving Generic This data type might look very complicated, so I'd like to show you that it's not that scary. Let's see if we can use GHCi to help us. Prelude Database . Beam . Sqlite Database . Beam Data . Text > : t User User :: Columnar f Text -> Columnar f Text -> Columnar f Text -> Columnar f Text -> UserT f Hmm... That did not help much. Let's see what happens if we bind f to something concrete, like Identity . Using the TypeApplications extension: Prelude Database . Beam Database . Beam . Sqlite Data . Text > : t ( User @Identity ) ( User @Identity ) :: Text -> Text -> Text -> Text -> UserT Identity Woah! That looks a lot like what we'd expect if we had declared the type in the \"regular\" Haskell way: data User = User { _userEmail :: Text , _userFirstName :: Text , _userLastName :: Text , _userPassword :: Text } This functionality is due to the fact that Columnar is a type family defined such that for any x , Columnar Identity x = x . This strategy is known as defunctionalization 1 or higher-kinded data types 2 . Knowing this, let's define a type synonym to make our life easier. type User = UserT Identity type UserId = PrimaryKey UserT Identity Now you can see why we named the type of the table UserT and its constructor User . This allows us to use the \"regular\" User constructor to construct values of type User . We can use the StandaloneDeriving and TypeSynonymInstances extensions to derive instances of Show and Eq for the 'regular' datatype. > :set -XStandaloneDeriving -XTypeSynonymInstances -XMultiParamTypeClasses Now we can derive Show and Eq instances. deriving instance Show User deriving instance Eq User Note that this does require us to use an explicit type signature where we otherwise wouldn't. For example, Prelude Database . Beam . Sqlite Database . Beam Data . Text > User \"john@example.com\" \"John\" \"Smith\" \"password!\" < interactive >: 46 : 2 : error : * No instance for ( Show ( UserT f0 )) arising from a use of \u2018 print \u2019 * In a stmt of an interactive GHCi command : print it Here, GHC is complaining that it cannot infer the type of the f parameter based on the values we've supplied. This is because the Columnar type family is non-injective. However, an explicit type annotation fixes it all up. Prelude Database . Beam . Sqlite Database . Beam Data . Text > User \"john@example.com\" \"John\" \"Smith\" \"password!\" :: User User { _userEmail = \"john@example.com\" , _userFirstName = \"John\" , _userLastName = \"Smith\" , _userPassword = \"password!\" } You can also use type applications, if you like that style better: Prelude Database . Beam Database . Beam . Sqlite Data . Text > User @Identity \"john@example.com\" \"John\" \"Smith\" \"password!\" User { _userEmail = \"john@example.com\" , _userFirstName = \"John\" , _userLastName = \"Smith\" , _userPassword = \"password!\" } Usually, you won't need to deal with this, as you'll explicitly annotate your top-level functions to use the User type. Teaching Beam about our table We've defined a type that can represent the data in our table. Now, let's inform beam that we'd like to use UserT as a table. All beam tables need to implement the Beamable type class. Due to GHC's DeriveGeneric and DefaultSignatures extensions, all these methods can be written for us by the compiler at compile-time! instance Beamable UserT Tip If you turn on the DeriveAnyClass feature, you can simply derive the Beamable type class. For example, the type data UserT f = User { _userEmail :: Columnar f Text , _userFirstName :: Columnar f Text , _userLastName :: Columnar f Text , _userPassword :: Columnar f Text } deriving Generic could be written data UserT f = User { _userEmail :: Columnar f Text , _userFirstName :: Columnar f Text , _userLastName :: Columnar f Text , _userPassword :: Columnar f Text } deriving (Generic, Beamable) Additionally, all beam tables must implement the Table type class, which we can use to declare a primary key. The only thing we need to provide is the type of the primary keys for users, and a function that can extract the primary key from any UserT f object. To do this, add the following lines to the instance declaration. instance Table UserT where data PrimaryKey UserT f = UserId (Columnar f Text) deriving (Generic, Beamable) primaryKey = UserId . _userEmail The data declaration is similar to a toplevel data definition, construct a key for UserT with the UserId constructor like a regular table. userKey = UserId \"john@doe.org\" Defining our database Now that we have our table, we're going to define a type to hold information about our database. Defining our database is going to follow the same pattern as defining a table. We'll define a higher-kinded datatype and then declare an instance of Database , and let the compiler figure most of it out. Tables are a collection of Columnar values. Databases are a collection of entities, such as tables. Many database systems can also hold other entities (such as views, domain types, etc). Beam allows you to declare these as well 3 . Our database consists of only one table. data ShoppingCartDb f = ShoppingCartDb { _shoppingCartUsers :: f ( TableEntity UserT ) } deriving ( Generic , Database be ) Note By deriving Database be we actually allowed our database to be used with any Beam backend that supports it. We could also have explicitly listed the database backends we liked. For example, specifying deriving (Generic, Database Sqlite, Database Postgres) would derive instances only for SQLite or Postgres. The next step is to create a description of the particular database we'd like to create. This involves giving each of the tables in our database a name. If you've named all your database selectors using camel case, beam can automatically figure out what all the table names should be. If you haven't, or you have multiple tables holding the same type in your database, you might have to manually name your tables. For now, we'll let beam do the hard work 4 . shoppingCartDb :: DatabaseSettings be ShoppingCartDb shoppingCartDb = defaultDbSettings Adding users to our database Let's add some users to our database. As we said above, beam is backend-agnostic. However, backend integration libraries are maintained in the official beam repository. The beam-sqlite package offers straightforwards integration with the sqlite-simple library. First, let's create a sqlite3 database with the right schema. Open up terminal, and do $ sqlite3 shoppingcart1.db SQLite version 3.14.0 2016-07-26 15:17:14 Enter \".help\" for usage hints. sqlite> CREATE TABLE cart_users (email VARCHAR NOT NULL, first_name VARCHAR NOT NULL, last_name VARCHAR NOT NULL, password VARCHAR NOT NULL, PRIMARY KEY( email )); sqlite> Now, let's open the database in Haskell. import Database.SQLite.Simple conn <- open \"shoppingcart1.db\" Now let's add a few users. We'll give each user an MD5 encoded password too. We'll use the runBeamSqliteDebug function (supplied by beam-sqlite ) to output the statements that beam would normally run. In production, you'd use the runBeamSqlite function, or use the backend integration packages to directly use the underlying backend library. runBeamSqliteDebug putStrLn {- for debug output -} conn $ runInsert $ insert ( _shoppingCartUsers shoppingCartDb ) $ insertValues [ User \"james@example.com\" \"James\" \"Smith\" \"b4cc344d25a2efe540adbf2678e2304c\" {- james -} , User \"betty@example.com\" \"Betty\" \"Jones\" \"82b054bd83ffad9b6cf8bdb98ce3cc2f\" {- betty -} , User \"sam@example.com\" \"Sam\" \"Taylor\" \"332532dcfaa1cbf61e2a266bd723612c\" {- sam -} ] The runInsert function runs an insert statement, which we construct using the insert function. Since we're inserting concrete values, we use the insertValues function to supply the values. We can also use the insertExpressions function to insert arbitrary SQL expressions, or the insertFrom to insert the results of an arbitrary select (the INSERT INTO .. SELECT .. syntax). Because we're in debug mode, we'll see the SQL that beam is running: INSERT INTO \"cart_users\" ( \"email\" , \"first_name\" , \"last_name\" , \"password\" ) VALUES ( ? , ? , ? , ? ), ( ? , ? , ? , ? ), ( ? , ? , ? , ? ) -- With values: [SQLText \"james@example.com\",SQLText \"James\",SQLText \"Smith\",SQLText \"b4cc344d25a2efe540adbf2678e2304c\",SQLText \"betty@example.com\",SQLText \"Betty\",SQLText \"Jones\",SQLText \"82b054bd83ffad9b6cf8bdb98ce3cc2f\",SQLText \"sam@example.com\",SQLText \"Sam\",SQLText \"Taylor\",SQLText \"332532dcfaa1cbf61e2a266bd723612c\"] The ? represent the values passed to the database (beam uses the backend's value interpolation to avoid SQL injection attacks). Querying the database Now let's write some queries for the database. Let's get all the users we just added. Click between the tabs to see the SQL and console output generated Haskell Sql Output let allUsers = all_ ( _shoppingCartUsers shoppingCartDb ) runBeamSqliteDebug putStrLn conn $ do users <- runSelectReturningList $ select allUsers mapM_ ( liftIO . putStrLn . show ) users SELECT \"t0\" . \"email\" AS \"res0\" , \"t0\" . \"first_name\" AS \"res1\" , \"t0\" . \"last_name\" AS \"res2\" , \"t0\" . \"password\" AS \"res3\" FROM \"cart_users\" AS \"t0\" ; -- With values: [] User { _userEmail = \"james@example.com\" , _userFirstName = \"James\" , _userLastName = \"Smith\" , _userPassword = \"b4cc344d25a2efe540adbf2678e2304c\" } User { _userEmail = \"betty@example.com\" , _userFirstName = \"Betty\" , _userLastName = \"Jones\" , _userPassword = \"82b054bd83ffad9b6cf8bdb98ce3cc2f\" } User { _userEmail = \"sam@example.com\" , _userFirstName = \"Sam\" , _userLastName = \"Taylor\" , _userPassword = \"332532dcfaa1cbf61e2a266bd723612c\" } Note The -- at the ends of the console output lines are an artifact of the documentation build process. They won't appear in your console. Next let's suppose you wanted to sort the users into order by their first name, and then descending by their last name. We can use the orderBy_ function to order the query results. This is similar to the sortBy function for lists. Haskell Sql Output let sortUsersByFirstName = orderBy_ ( \\ u -> ( asc_ ( _userFirstName u ), desc_ ( _userLastName u ))) ( all_ ( _shoppingCartUsers shoppingCartDb )) runBeamSqliteDebug putStrLn conn $ do users <- runSelectReturningList $ select sortUsersByFirstName mapM_ ( liftIO . putStrLn . show ) users SELECT \"t0\" . \"email\" AS \"res0\" , \"t0\" . \"first_name\" AS \"res1\" , \"t0\" . \"last_name\" AS \"res2\" , \"t0\" . \"password\" AS \"res3\" FROM \"cart_users\" AS \"t0\" ORDER BY \"t0\" . \"first_name\" ASC , \"t0\" . \"last_name\" DESC ; -- With values: [] User { _userEmail = \"betty@example.com\" , _userFirstName = \"Betty\" , _userLastName = \"Jones\" , _userPassword = \"82b054bd83ffad9b6cf8bdb98ce3cc2f\" } User { _userEmail = \"james@example.com\" , _userFirstName = \"James\" , _userLastName = \"Smith\" , _userPassword = \"b4cc344d25a2efe540adbf2678e2304c\" } User { _userEmail = \"sam@example.com\" , _userFirstName = \"Sam\" , _userLastName = \"Taylor\" , _userPassword = \"332532dcfaa1cbf61e2a266bd723612c\" } We can use limit_ and offset_ in a similar manner to take and drop respectively. Haskell Sql Output let boundedQuery = limit_ 1 $ offset_ 1 $ orderBy_ ( asc_ . _userFirstName ) $ all_ ( _shoppingCartUsers shoppingCartDb ) runBeamSqliteDebug putStrLn conn $ do users <- runSelectReturningList ( select boundedQuery ) mapM_ ( liftIO . putStrLn . show ) users SELECT \"t0\" . \"email\" AS \"res0\" , \"t0\" . \"first_name\" AS \"res1\" , \"t0\" . \"last_name\" AS \"res2\" , \"t0\" . \"password\" AS \"res3\" FROM \"cart_users\" AS \"t0\" ORDER BY \"t0\" . \"first_name\" ASC LIMIT 1 OFFSET 1 ; -- With values: [] User { _userEmail = \"james@example.com\" , _userFirstName = \"James\" , _userLastName = \"Smith\" , _userPassword = \"b4cc344d25a2efe540adbf2678e2304c\" } Aggregations Sometimes we also want to group our data together and perform calculations over the groups of data. SQL calls these aggregations. The simplest aggregation is counting. We use the aggregate_ function to create aggregations. For example, to count all users, we can use the countAll_ aggregation. We also use the runSelectReturningOne function to get at most one record from the database. Haskell Sql Output let userCount = aggregate_ ( \\ u -> as_ @ Int32 countAll_ ) ( all_ ( _shoppingCartUsers shoppingCartDb )) runBeamSqliteDebug putStrLn conn $ do Just c <- runSelectReturningOne $ select userCount liftIO $ putStrLn ( \"We have \" ++ show c ++ \" users in the database\" ) SELECT COUNT ( * ) AS \"res0\" FROM \"cart_users\" AS \"t0\" ; -- With values: [] We have 3 users in the database Note countAll_ is happy to unmarshal into any Integral type, so we use as_ to constrain the type to Int32 . Maybe we'd like something a little more interesting, such as the number of users for each unique first name. We can also express these aggregations using the aggregate_ function. In order to get interesting results, we'll need to add more users to our database. Haskell Sql runBeamSqliteDebug putStrLn conn $ runInsert $ insert ( _shoppingCartUsers shoppingCartDb ) $ insertValues [ User \"james@pallo.com\" \"James\" \"Pallo\" \"b4cc344d25a2efe540adbf2678e2304c\" {- james -} , User \"betty@sims.com\" \"Betty\" \"Sims\" \"82b054bd83ffad9b6cf8bdb98ce3cc2f\" {- betty -} , User \"james@oreily.com\" \"James\" \"O'Reily\" \"b4cc344d25a2efe540adbf2678e2304c\" {- james -} , User \"sam@sophitz.com\" \"Sam\" \"Sophitz\" \"332532dcfaa1cbf61e2a266bd723612c\" {- sam -} , User \"sam@jely.com\" \"Sam\" \"Jely\" \"332532dcfaa1cbf61e2a266bd723612c\" {- sam -} ] INSERT INTO \"cart_users\" ( \"email\" , \"first_name\" , \"last_name\" , \"password\" ) VALUES ( ? , ? , ? , ? ), ( ? , ? , ? , ? ), ( ? , ? , ? , ? ), ( ? , ? , ? , ? ), ( ? , ? , ? , ? ); -- With values: [SQLText \"james@pallo.com\",SQLText \"James\",SQLText \"Pallo\",SQLText \"b4cc344d25a2efe540adbf2678e2304c\",SQLText \"betty@sims.com\",SQLText \"Betty\",SQLText \"Sims\",SQLText \"82b054bd83ffad9b6cf8bdb98ce3cc2f\",SQLText \"james@oreily.com\",SQLText \"James\",SQLText \"O'Reily\",SQLText \"b4cc344d25a2efe540adbf2678e2304c\",SQLText \"sam@sophitz.com\",SQLText \"Sam\",SQLText \"Sophitz\",SQLText \"332532dcfaa1cbf61e2a266bd723612c\",SQLText \"sam@jely.com\",SQLText \"Sam\",SQLText \"Jely\",SQLText \"332532dcfaa1cbf61e2a266bd723612c\"] Now we can use aggregate_ to both group by a user's first name, and then count the number of users. Haskell Sql Output let numberOfUsersByName = aggregate_ ( \\ u -> ( group_ ( _userFirstName u ), as_ @ Int32 countAll_ )) $ all_ ( _shoppingCartUsers shoppingCartDb ) runBeamSqliteDebug putStrLn conn $ do countedByName <- runSelectReturningList $ select numberOfUsersByName mapM_ ( liftIO . putStrLn . show ) countedByName SELECT \"t0\" . \"first_name\" AS \"res0\" , COUNT ( * ) AS \"res1\" FROM \"cart_users\" AS \"t0\" GROUP BY \"t0\" . \"first_name\" ; -- With values: [] (\"Betty\",2) (\"James\",3) (\"Sam\",3) Conclusion In this tutorial, we've covered creating a database schema, opening up a beam database, inserting values into the database, and querying values from them. We used the knowledge we learned to create a partial shopping cart database that contains information about users. In the next tutorial, we'll delve deeper into the some of the query types and show how we can create relations between tables. We'll also use the monadic query interface to create SQL joins. Until next time! If you have any questions about beam, feel free to send them to travis@athougies.net . Pull requests and bug reports are welcome on GitHub . Thanks to various bloggers for pointing this out. You can read more about this technique here . \u21a9 https://reasonablypolymorphic.com/blog/higher-kinded-data/ \u21a9 Adding entities other than tables is covered in more depth in the user guide . \u21a9 More on the default naming conventions can be found in the models section of the user guide. We'll talk about how to override defaults in the next sections. \u21a9","title":"Part 1"},{"location":"tutorials/tutorial1/#beam-module-structure","text":"Beam makes extensive use of GHC's Generics mechanism. This extension means beam does not need to rely on template haskell. To start defining beam schemas and queries, you only need to import the Database.Beam module. To interface with an actual database, you'll need to import one of the database backends. We'll see how to use the Sqlite backend here (found in the beam-sqlite package). Now, open up a GHCi prompt for us to use. Make sure to get the beam-core , beam-sqlite and text packages. $ stack repl --package beam-core --package beam-sqlite --package sqlite-simple --package beam-migrate --package text This will put you into a GHCi prompt with the beam-core and beam-sqlite packages available. We also include the sqlite-simple package. Beam mainly manages querying and data marshalling. Connections to the backends are done via backend specific packages. In this case, beam-sqlite uses the sqlite-simple backend. Before starting, we'll need to enable some extensions. > : set - XDeriveGeneric - XGADTs - XOverloadedStrings - XFlexibleContexts - XFlexibleInstances - XTypeFamilies - XTypeApplications - XDeriveAnyClass And import some modules... import Database.Beam import Database.Beam.Sqlite import Data.Text ( Text )","title":"Beam Module Structure"},{"location":"tutorials/tutorial1/#defining-our-first-table","text":"Beam tables are regular Haskell data types with a bit of scaffolding. Thankfully, the magic of the modern Haskell type system allows us to remove the overhead and the syntactic fuzz of the scaffolding in most situations. We start by declaring a data structure named UserT . As a matter of convention, Beam table types are suffixed with 'T'. Table types have only one constructor. Again, as a matter of convention, the constructor has the same name as the table, but without the 'T' suffix. We'll soon see the reason for this convention. In this tutorial, I'll prefix all record selectors with an underscore. This is a matter of personal preference. One reason for the prefix is that it plays nicely with the lens library. Beam does not necessitate the use of lens (in fact Beam includes its own mechanism to generically derive van Laarhoven lenses), but I recognize that some programmers use lens quite a lot. data UserT f = User { _userEmail :: Columnar f Text , _userFirstName :: Columnar f Text , _userLastName :: Columnar f Text , _userPassword :: Columnar f Text } deriving Generic This data type might look very complicated, so I'd like to show you that it's not that scary. Let's see if we can use GHCi to help us. Prelude Database . Beam . Sqlite Database . Beam Data . Text > : t User User :: Columnar f Text -> Columnar f Text -> Columnar f Text -> Columnar f Text -> UserT f Hmm... That did not help much. Let's see what happens if we bind f to something concrete, like Identity . Using the TypeApplications extension: Prelude Database . Beam Database . Beam . Sqlite Data . Text > : t ( User @Identity ) ( User @Identity ) :: Text -> Text -> Text -> Text -> UserT Identity Woah! That looks a lot like what we'd expect if we had declared the type in the \"regular\" Haskell way: data User = User { _userEmail :: Text , _userFirstName :: Text , _userLastName :: Text , _userPassword :: Text } This functionality is due to the fact that Columnar is a type family defined such that for any x , Columnar Identity x = x . This strategy is known as defunctionalization 1 or higher-kinded data types 2 . Knowing this, let's define a type synonym to make our life easier. type User = UserT Identity type UserId = PrimaryKey UserT Identity Now you can see why we named the type of the table UserT and its constructor User . This allows us to use the \"regular\" User constructor to construct values of type User . We can use the StandaloneDeriving and TypeSynonymInstances extensions to derive instances of Show and Eq for the 'regular' datatype. > :set -XStandaloneDeriving -XTypeSynonymInstances -XMultiParamTypeClasses Now we can derive Show and Eq instances. deriving instance Show User deriving instance Eq User Note that this does require us to use an explicit type signature where we otherwise wouldn't. For example, Prelude Database . Beam . Sqlite Database . Beam Data . Text > User \"john@example.com\" \"John\" \"Smith\" \"password!\" < interactive >: 46 : 2 : error : * No instance for ( Show ( UserT f0 )) arising from a use of \u2018 print \u2019 * In a stmt of an interactive GHCi command : print it Here, GHC is complaining that it cannot infer the type of the f parameter based on the values we've supplied. This is because the Columnar type family is non-injective. However, an explicit type annotation fixes it all up. Prelude Database . Beam . Sqlite Database . Beam Data . Text > User \"john@example.com\" \"John\" \"Smith\" \"password!\" :: User User { _userEmail = \"john@example.com\" , _userFirstName = \"John\" , _userLastName = \"Smith\" , _userPassword = \"password!\" } You can also use type applications, if you like that style better: Prelude Database . Beam Database . Beam . Sqlite Data . Text > User @Identity \"john@example.com\" \"John\" \"Smith\" \"password!\" User { _userEmail = \"john@example.com\" , _userFirstName = \"John\" , _userLastName = \"Smith\" , _userPassword = \"password!\" } Usually, you won't need to deal with this, as you'll explicitly annotate your top-level functions to use the User type.","title":"Defining our first table"},{"location":"tutorials/tutorial1/#teaching-beam-about-our-table","text":"We've defined a type that can represent the data in our table. Now, let's inform beam that we'd like to use UserT as a table. All beam tables need to implement the Beamable type class. Due to GHC's DeriveGeneric and DefaultSignatures extensions, all these methods can be written for us by the compiler at compile-time! instance Beamable UserT Tip If you turn on the DeriveAnyClass feature, you can simply derive the Beamable type class. For example, the type data UserT f = User { _userEmail :: Columnar f Text , _userFirstName :: Columnar f Text , _userLastName :: Columnar f Text , _userPassword :: Columnar f Text } deriving Generic could be written data UserT f = User { _userEmail :: Columnar f Text , _userFirstName :: Columnar f Text , _userLastName :: Columnar f Text , _userPassword :: Columnar f Text } deriving (Generic, Beamable) Additionally, all beam tables must implement the Table type class, which we can use to declare a primary key. The only thing we need to provide is the type of the primary keys for users, and a function that can extract the primary key from any UserT f object. To do this, add the following lines to the instance declaration. instance Table UserT where data PrimaryKey UserT f = UserId (Columnar f Text) deriving (Generic, Beamable) primaryKey = UserId . _userEmail The data declaration is similar to a toplevel data definition, construct a key for UserT with the UserId constructor like a regular table. userKey = UserId \"john@doe.org\"","title":"Teaching Beam about our table"},{"location":"tutorials/tutorial1/#defining-our-database","text":"Now that we have our table, we're going to define a type to hold information about our database. Defining our database is going to follow the same pattern as defining a table. We'll define a higher-kinded datatype and then declare an instance of Database , and let the compiler figure most of it out. Tables are a collection of Columnar values. Databases are a collection of entities, such as tables. Many database systems can also hold other entities (such as views, domain types, etc). Beam allows you to declare these as well 3 . Our database consists of only one table. data ShoppingCartDb f = ShoppingCartDb { _shoppingCartUsers :: f ( TableEntity UserT ) } deriving ( Generic , Database be ) Note By deriving Database be we actually allowed our database to be used with any Beam backend that supports it. We could also have explicitly listed the database backends we liked. For example, specifying deriving (Generic, Database Sqlite, Database Postgres) would derive instances only for SQLite or Postgres. The next step is to create a description of the particular database we'd like to create. This involves giving each of the tables in our database a name. If you've named all your database selectors using camel case, beam can automatically figure out what all the table names should be. If you haven't, or you have multiple tables holding the same type in your database, you might have to manually name your tables. For now, we'll let beam do the hard work 4 . shoppingCartDb :: DatabaseSettings be ShoppingCartDb shoppingCartDb = defaultDbSettings","title":"Defining our database"},{"location":"tutorials/tutorial1/#adding-users-to-our-database","text":"Let's add some users to our database. As we said above, beam is backend-agnostic. However, backend integration libraries are maintained in the official beam repository. The beam-sqlite package offers straightforwards integration with the sqlite-simple library. First, let's create a sqlite3 database with the right schema. Open up terminal, and do $ sqlite3 shoppingcart1.db SQLite version 3.14.0 2016-07-26 15:17:14 Enter \".help\" for usage hints. sqlite> CREATE TABLE cart_users (email VARCHAR NOT NULL, first_name VARCHAR NOT NULL, last_name VARCHAR NOT NULL, password VARCHAR NOT NULL, PRIMARY KEY( email )); sqlite> Now, let's open the database in Haskell. import Database.SQLite.Simple conn <- open \"shoppingcart1.db\" Now let's add a few users. We'll give each user an MD5 encoded password too. We'll use the runBeamSqliteDebug function (supplied by beam-sqlite ) to output the statements that beam would normally run. In production, you'd use the runBeamSqlite function, or use the backend integration packages to directly use the underlying backend library. runBeamSqliteDebug putStrLn {- for debug output -} conn $ runInsert $ insert ( _shoppingCartUsers shoppingCartDb ) $ insertValues [ User \"james@example.com\" \"James\" \"Smith\" \"b4cc344d25a2efe540adbf2678e2304c\" {- james -} , User \"betty@example.com\" \"Betty\" \"Jones\" \"82b054bd83ffad9b6cf8bdb98ce3cc2f\" {- betty -} , User \"sam@example.com\" \"Sam\" \"Taylor\" \"332532dcfaa1cbf61e2a266bd723612c\" {- sam -} ] The runInsert function runs an insert statement, which we construct using the insert function. Since we're inserting concrete values, we use the insertValues function to supply the values. We can also use the insertExpressions function to insert arbitrary SQL expressions, or the insertFrom to insert the results of an arbitrary select (the INSERT INTO .. SELECT .. syntax). Because we're in debug mode, we'll see the SQL that beam is running: INSERT INTO \"cart_users\" ( \"email\" , \"first_name\" , \"last_name\" , \"password\" ) VALUES ( ? , ? , ? , ? ), ( ? , ? , ? , ? ), ( ? , ? , ? , ? ) -- With values: [SQLText \"james@example.com\",SQLText \"James\",SQLText \"Smith\",SQLText \"b4cc344d25a2efe540adbf2678e2304c\",SQLText \"betty@example.com\",SQLText \"Betty\",SQLText \"Jones\",SQLText \"82b054bd83ffad9b6cf8bdb98ce3cc2f\",SQLText \"sam@example.com\",SQLText \"Sam\",SQLText \"Taylor\",SQLText \"332532dcfaa1cbf61e2a266bd723612c\"] The ? represent the values passed to the database (beam uses the backend's value interpolation to avoid SQL injection attacks).","title":"Adding users to our database"},{"location":"tutorials/tutorial1/#querying-the-database","text":"Now let's write some queries for the database. Let's get all the users we just added. Click between the tabs to see the SQL and console output generated Haskell Sql Output let allUsers = all_ ( _shoppingCartUsers shoppingCartDb ) runBeamSqliteDebug putStrLn conn $ do users <- runSelectReturningList $ select allUsers mapM_ ( liftIO . putStrLn . show ) users SELECT \"t0\" . \"email\" AS \"res0\" , \"t0\" . \"first_name\" AS \"res1\" , \"t0\" . \"last_name\" AS \"res2\" , \"t0\" . \"password\" AS \"res3\" FROM \"cart_users\" AS \"t0\" ; -- With values: [] User { _userEmail = \"james@example.com\" , _userFirstName = \"James\" , _userLastName = \"Smith\" , _userPassword = \"b4cc344d25a2efe540adbf2678e2304c\" } User { _userEmail = \"betty@example.com\" , _userFirstName = \"Betty\" , _userLastName = \"Jones\" , _userPassword = \"82b054bd83ffad9b6cf8bdb98ce3cc2f\" } User { _userEmail = \"sam@example.com\" , _userFirstName = \"Sam\" , _userLastName = \"Taylor\" , _userPassword = \"332532dcfaa1cbf61e2a266bd723612c\" } Note The -- at the ends of the console output lines are an artifact of the documentation build process. They won't appear in your console. Next let's suppose you wanted to sort the users into order by their first name, and then descending by their last name. We can use the orderBy_ function to order the query results. This is similar to the sortBy function for lists. Haskell Sql Output let sortUsersByFirstName = orderBy_ ( \\ u -> ( asc_ ( _userFirstName u ), desc_ ( _userLastName u ))) ( all_ ( _shoppingCartUsers shoppingCartDb )) runBeamSqliteDebug putStrLn conn $ do users <- runSelectReturningList $ select sortUsersByFirstName mapM_ ( liftIO . putStrLn . show ) users SELECT \"t0\" . \"email\" AS \"res0\" , \"t0\" . \"first_name\" AS \"res1\" , \"t0\" . \"last_name\" AS \"res2\" , \"t0\" . \"password\" AS \"res3\" FROM \"cart_users\" AS \"t0\" ORDER BY \"t0\" . \"first_name\" ASC , \"t0\" . \"last_name\" DESC ; -- With values: [] User { _userEmail = \"betty@example.com\" , _userFirstName = \"Betty\" , _userLastName = \"Jones\" , _userPassword = \"82b054bd83ffad9b6cf8bdb98ce3cc2f\" } User { _userEmail = \"james@example.com\" , _userFirstName = \"James\" , _userLastName = \"Smith\" , _userPassword = \"b4cc344d25a2efe540adbf2678e2304c\" } User { _userEmail = \"sam@example.com\" , _userFirstName = \"Sam\" , _userLastName = \"Taylor\" , _userPassword = \"332532dcfaa1cbf61e2a266bd723612c\" } We can use limit_ and offset_ in a similar manner to take and drop respectively. Haskell Sql Output let boundedQuery = limit_ 1 $ offset_ 1 $ orderBy_ ( asc_ . _userFirstName ) $ all_ ( _shoppingCartUsers shoppingCartDb ) runBeamSqliteDebug putStrLn conn $ do users <- runSelectReturningList ( select boundedQuery ) mapM_ ( liftIO . putStrLn . show ) users SELECT \"t0\" . \"email\" AS \"res0\" , \"t0\" . \"first_name\" AS \"res1\" , \"t0\" . \"last_name\" AS \"res2\" , \"t0\" . \"password\" AS \"res3\" FROM \"cart_users\" AS \"t0\" ORDER BY \"t0\" . \"first_name\" ASC LIMIT 1 OFFSET 1 ; -- With values: [] User { _userEmail = \"james@example.com\" , _userFirstName = \"James\" , _userLastName = \"Smith\" , _userPassword = \"b4cc344d25a2efe540adbf2678e2304c\" }","title":"Querying the database"},{"location":"tutorials/tutorial1/#aggregations","text":"Sometimes we also want to group our data together and perform calculations over the groups of data. SQL calls these aggregations. The simplest aggregation is counting. We use the aggregate_ function to create aggregations. For example, to count all users, we can use the countAll_ aggregation. We also use the runSelectReturningOne function to get at most one record from the database. Haskell Sql Output let userCount = aggregate_ ( \\ u -> as_ @ Int32 countAll_ ) ( all_ ( _shoppingCartUsers shoppingCartDb )) runBeamSqliteDebug putStrLn conn $ do Just c <- runSelectReturningOne $ select userCount liftIO $ putStrLn ( \"We have \" ++ show c ++ \" users in the database\" ) SELECT COUNT ( * ) AS \"res0\" FROM \"cart_users\" AS \"t0\" ; -- With values: [] We have 3 users in the database Note countAll_ is happy to unmarshal into any Integral type, so we use as_ to constrain the type to Int32 . Maybe we'd like something a little more interesting, such as the number of users for each unique first name. We can also express these aggregations using the aggregate_ function. In order to get interesting results, we'll need to add more users to our database. Haskell Sql runBeamSqliteDebug putStrLn conn $ runInsert $ insert ( _shoppingCartUsers shoppingCartDb ) $ insertValues [ User \"james@pallo.com\" \"James\" \"Pallo\" \"b4cc344d25a2efe540adbf2678e2304c\" {- james -} , User \"betty@sims.com\" \"Betty\" \"Sims\" \"82b054bd83ffad9b6cf8bdb98ce3cc2f\" {- betty -} , User \"james@oreily.com\" \"James\" \"O'Reily\" \"b4cc344d25a2efe540adbf2678e2304c\" {- james -} , User \"sam@sophitz.com\" \"Sam\" \"Sophitz\" \"332532dcfaa1cbf61e2a266bd723612c\" {- sam -} , User \"sam@jely.com\" \"Sam\" \"Jely\" \"332532dcfaa1cbf61e2a266bd723612c\" {- sam -} ] INSERT INTO \"cart_users\" ( \"email\" , \"first_name\" , \"last_name\" , \"password\" ) VALUES ( ? , ? , ? , ? ), ( ? , ? , ? , ? ), ( ? , ? , ? , ? ), ( ? , ? , ? , ? ), ( ? , ? , ? , ? ); -- With values: [SQLText \"james@pallo.com\",SQLText \"James\",SQLText \"Pallo\",SQLText \"b4cc344d25a2efe540adbf2678e2304c\",SQLText \"betty@sims.com\",SQLText \"Betty\",SQLText \"Sims\",SQLText \"82b054bd83ffad9b6cf8bdb98ce3cc2f\",SQLText \"james@oreily.com\",SQLText \"James\",SQLText \"O'Reily\",SQLText \"b4cc344d25a2efe540adbf2678e2304c\",SQLText \"sam@sophitz.com\",SQLText \"Sam\",SQLText \"Sophitz\",SQLText \"332532dcfaa1cbf61e2a266bd723612c\",SQLText \"sam@jely.com\",SQLText \"Sam\",SQLText \"Jely\",SQLText \"332532dcfaa1cbf61e2a266bd723612c\"] Now we can use aggregate_ to both group by a user's first name, and then count the number of users. Haskell Sql Output let numberOfUsersByName = aggregate_ ( \\ u -> ( group_ ( _userFirstName u ), as_ @ Int32 countAll_ )) $ all_ ( _shoppingCartUsers shoppingCartDb ) runBeamSqliteDebug putStrLn conn $ do countedByName <- runSelectReturningList $ select numberOfUsersByName mapM_ ( liftIO . putStrLn . show ) countedByName SELECT \"t0\" . \"first_name\" AS \"res0\" , COUNT ( * ) AS \"res1\" FROM \"cart_users\" AS \"t0\" GROUP BY \"t0\" . \"first_name\" ; -- With values: [] (\"Betty\",2) (\"James\",3) (\"Sam\",3)","title":"Aggregations"},{"location":"tutorials/tutorial1/#conclusion","text":"In this tutorial, we've covered creating a database schema, opening up a beam database, inserting values into the database, and querying values from them. We used the knowledge we learned to create a partial shopping cart database that contains information about users. In the next tutorial, we'll delve deeper into the some of the query types and show how we can create relations between tables. We'll also use the monadic query interface to create SQL joins. Until next time! If you have any questions about beam, feel free to send them to travis@athougies.net . Pull requests and bug reports are welcome on GitHub . Thanks to various bloggers for pointing this out. You can read more about this technique here . \u21a9 https://reasonablypolymorphic.com/blog/higher-kinded-data/ \u21a9 Adding entities other than tables is covered in more depth in the user guide . \u21a9 More on the default naming conventions can be found in the models section of the user guide. We'll talk about how to override defaults in the next sections. \u21a9","title":"Conclusion"},{"location":"tutorials/tutorial2/","text":"Introduction In the last part, we created a simple database with one table. We then used the beam interface to add entities into that table and query them. In this tutorial, we'll see how to update and delete rows and how to establish and query relations between tables. We'll then delve deeper into queries to see how to create queries that return multiple tables. Adding a related table The users in our simple e-commerce application would like to ship orders to their homes. Let's build an addresses model to allow users to add home addresses to their profile. Our table will store United States addresses for now. An address in the United States consists of an auto-incrementing primary key one required house number and street line an optional apartment/suite number line a required city a required 2-letter state/territory code one 5-digit ZIP code Let's build the AddressT table. AddressT will follow a similar formula to UserT , but it will contain a reference to a UserT table. ] data AddressT f = Address { _addressId :: C f Int32 , _addressLine1 :: C f Text , _addressLine2 :: C f ( Maybe Text ) , _addressCity :: C f Text , _addressState :: C f Text , _addressZip :: C f Text , _addressForUser :: PrimaryKey UserT f } deriving ( Generic , Beamable ) type Address = AddressT Identity deriving instance Show ( PrimaryKey UserT Identity ) deriving instance Show Address instance Table AddressT where data PrimaryKey AddressT f = AddressId ( Columnar f Int32 ) deriving ( Generic , Beamable ) primaryKey = AddressId . _addressId type AddressId = PrimaryKey AddressT Identity -- For convenience Tip Above, we used the C constructor instead of Columnar for each column. C is a type synonym for Columnar , and some find it reduces the syntactic overhead of model declaration. Notice that _addressForUser is declared as a PrimaryKey UserT f . This pulls in all the columns necessary for referencing a UserT 1 . Later, we'll also see how beam can use the field to automatically create JOINs. Notice also that _addressId corresponds to our auto-increminting primary key field. In general, beam doesn't care if the underlying field is assigned automatically, only about the type of final values of that field. We have all the tables we need now, so let's go ahead and redefine our newest database type. data ShoppingCartDb f = ShoppingCartDb { _shoppingCartUsers :: f ( TableEntity UserT ) , _shoppingCartUserAddresses :: f ( TableEntity AddressT ) } deriving ( Generic , Database be ) Modifying the default naming choices In the last part of the tutorial, we let beam decide our field names for us. This is great for simple cases. However, sometimes you want more control over the naming options. Note Previous versions of this tutorial had instructions on changing the schema type of particular tables. This functionality has been moved from beam-core into the beam-migrate package. See the migrations guide for more information. The defaultDbSettings function generates names using the Haskell record selector names 2 . This function returns the DatabaseType parameterized over DatabaseEntity , which is a type that contains metadata about entity names. We can modify this description after it is created by using the withDbModification function. You can think of withDbModification as applying a transformation function to each name in our database. Most of the time withDbModification needs a full description of the database names. However, most of the time we only want to rename certain columns or tables. We can use the dbModification value to construct a modification that doesn't change any names. We can then use the Haskell record update syntax to update field and column names. This is best illustrated by an example. Recall our Haskell data types above. data UserT f = User { _userEmail :: Columnar f Text , _userFirstName :: Columnar f Text , _userLastName :: Columnar f Text , _userPassword :: Columnar f Text } deriving Generic data AddressT f = Address { _addressId :: C f Int32 , _addressLine1 :: C f Text , _addressLine2 :: C f ( Maybe Text ) , _addressCity :: C f Text , _addressState :: C f Text , _addressZip :: C f Text , _addressForUser :: PrimaryKey UserT f } deriving Generic data ShoppingCartDb f = ShoppingCartDb { _shoppingCartUsers :: f ( TableEntity UserT ) , _shoppingCartUserAddresses :: f ( TableEntity AddressT ) } deriving Generic Now, let's say we want beam to use the name addresses to access the _shoppingCartUserAddresses table, and the names address1 and address2 to access _addressLine1 and _addressLine2 respectively. shoppingCartDb :: DatabaseSettings be ShoppingCartDb shoppingCartDb = defaultDbSettings ` withDbModification ` dbModification { _shoppingCartUserAddresses = setEntityName \"addresses\" <> modifyTableFields tableModification { _addressLine1 = fieldNamed \"address1\" , _addressLine2 = fieldNamed \"address2\" } } Above, we use dbModification to produce a default modification, then we override the _shoppingCartUserAddresses modification to change the addresses table. We modify the table in two ways. First, we use the setEntityName function to change the name of the table. Then, we use modifyTableFields to change the names of each field. The modifications can be combined with the semigroup operator (<>) . We only override the _addressLine1 and _addressLine2 modifications with fieldNamed \"address1\" and fieldNamed \"address2\" . Because tableModification produces a default modification, the other columns are kept at their default value. Tip The OverloadedStrings extension lets us avoid typing fieldNamed . For example, instead of _addressLine1 = fieldNamed \"address1\" we could have written _addressLine1 = \"address1\" When renaming a field referring to a foreign key (for example the _addressForUser field), remember to wrap the field name with the table's PrimaryKey constructor: _addressForUser = UserId \"user\" Given that in part one we defined UserId as the PrimaryKey constructor for UserT : instance Table UserT where data PrimaryKey UserT f = UserId ( Columnar f Text ) deriving ( Generic , Beamable ) primaryKey = UserId . _userEmail If you didn't need to modify any of the field names, you can omit modifyTableFields . For example, to simply produce a database with the first table named users and the second named user_addresses , you can do shoppingCartDb1 :: DatabaseSettings be ShoppingCartDb shoppingCartDb1 = defaultDbSettings ` withDbModification ` dbModification { _shoppingCartUsers = setEntityName \"users\" , _shoppingCartUserAddresses = setEntityName \"user_addresses\" } For the purposes of this tutorial, we'll stick with shoppingCartDb . Easier queries with lenses In the previous part, we accessed table columns by using regular Haskell record syntax. Sometimes, we would like to use the more convenient lens syntax to access columns. Of course, all of beam's definitions are compatible with the lens library -- that is to say, makeLenses will work just fine. However, beam's motivation is, in part, the avoidance of Template Haskell, and it would hardly be worth it if you had to include a Template Haskell splice just to have lenses for the models you declared TH free. In reality, the lens library isn't required to construct valid lenses. Lenses are a plain old Haskell type. We can use beam's Columnar mechanism to automatically derive lenses. The tableLenses function produces a table value where each column is given a type LensFor , which is a newtype wrapper over a correctly constructed, polymorphic Van Laarhoven lens. We can bring these lenses into scope globally via a global pattern match against tableLenses . For example, to get lenses for each column of the AddressT and UserT table. -- Add the following to the top of the file, for GHC >8.2 {-# LANGUAGE ImpredicativeTypes #-} Address ( LensFor addressId ) ( LensFor addressLine1 ) ( LensFor addressLine2 ) ( LensFor addressCity ) ( LensFor addressState ) ( LensFor addressZip ) ( UserId ( LensFor addressForUserId )) = tableLenses User ( LensFor userEmail ) ( LensFor userFirstName ) ( LensFor userLastName ) ( LensFor userPassword ) = tableLenses Note The ImpredicativeTypes language extension is necessary for newer GHC to allow the polymorphically typed lenses to be introduced at the top-level. Older GHCs were more lenient. As in tables, we can generate lenses for databases via the dbLenses function. ShoppingCartDb ( TableLens shoppingCartUsers ) ( TableLens shoppingCartUserAddresses ) = dbLenses We can ask GHCi for the type of a column lens. Prelude Database . Beam Database . Beam . Sqlite Data . Text Database . SQLite . Simple > : t addressId addressId :: Functor f2 => ( Columnar f1 Int32 -> f2 ( Columnar f1 Int32 )) -> AddressT f1 -> f2 ( AddressT f1 ) This lens is compatible with those of the lens library. And a table lens, for good measure Prelude Database . Beam Database . Beam . Sqlite Data . Text Database . SQLite . Simple > : t shoppingCartUsers shoppingCartUsers :: Functor f1 => ( f2 ( TableEntity UserT ) -> f1 ( f2 ( TableEntity UserT ))) -> ShoppingCartDb f2 -> f1 ( ShoppingCartDb f2 ) Warning These lens generating functions are awesome but if you use them in a compiled Haskell module (rather than GHC), GHC may give you odd compile errors about ambiguous types. These occur due to what's known as the monomorphism restriction. You can turn it off using the NoMonomorphismRestriction extension. The monomorphism restriction is part of the Haskell standard, but there has been talk about removing it in future language versions. Basically, it requires GHC to not automatically infer polymorphic types for global definitions. In this case though, polymorphic global definitions is exactly what we want. Working with relations Now, let's see how we can add related addresses to our database. We begin by opening up a connection for us to use in the rest of the tutorial. First, let's open a new database and create the schema. $ sqlite3 shoppingcart2.db SQLite version 3.14.0 2016-07-26 15:17:14 Enter \".help\" for usage hints. sqlite> CREATE TABLE cart_users (email VARCHAR NOT NULL, first_name VARCHAR NOT NULL, last_name VARCHAR NOT NULL, password VARCHAR NOT NULL, PRIMARY KEY( email )); sqlite> CREATE TABLE addresses ( id INTEGER PRIMARY KEY, address1 VARCHAR NOT NULL, address2 VARCHAR, city VARCHAR NOT NULL, state VARCHAR NOT NULL, zip VARCHAR NOT NULL, for_user__email VARCHAR NOT NULL ); Now, in GHCi, we can use sqlite-simple to get a handle to this database. conn <- open \"shoppingcart2.db\" Before we add addresses, we need to add some users that we can reference. let james = User \"james@example.com\" \"James\" \"Smith\" \"b4cc344d25a2efe540adbf2678e2304c\" betty = User \"betty@example.com\" \"Betty\" \"Jones\" \"82b054bd83ffad9b6cf8bdb98ce3cc2f\" sam = User \"sam@example.com\" \"Sam\" \"Taylor\" \"332532dcfaa1cbf61e2a266bd723612c\" runBeamSqliteDebug putStrLn conn $ runInsert $ insert ( _shoppingCartUsers shoppingCartDb ) $ insertValues [ james , betty , sam ] Now that we have some User objects, we can create associated addresses. Notice that above, we used insertValues to insert concrete User rows. This worked because we could determine every field of User before insertion. Address es however have a pesky auto-incrementing primary key field. We can get around this by inserting expressions instead of values . We can use default_ to stand for a value that the database needs to fill in. We can use val_ to lift a literal value into an expression. With that in mind, let's give James one address, Betty two addresses, and Sam none. let addresses = [ Address default_ ( val_ \"123 Little Street\" ) ( val_ Nothing ) ( val_ \"Boston\" ) ( val_ \"MA\" ) ( val_ \"12345\" ) ( pk james ) , Address default_ ( val_ \"222 Main Street\" ) ( val_ ( Just \"Ste 1\" )) ( val_ \"Houston\" ) ( val_ \"TX\" ) ( val_ \"8888\" ) ( pk betty ) , Address default_ ( val_ \"9999 Residence Ave\" ) ( val_ Nothing ) ( val_ \"Sugarland\" ) ( val_ \"TX\" ) ( val_ \"8989\" ) ( pk betty ) ] runBeamSqliteDebug putStrLn conn $ runInsert $ insert ( _shoppingCartUserAddresses shoppingCartDb ) $ insertExpressions addresses Notice that we used the pk function to assign the reference to the UserT table. pk is a synonym of the primaryKey function from the Table type class. It should be clear what's going on, but if it's not, let's ask GHCi. *NextSteps> pk (james :: User)p UserId \"james@example.com\" If we query for all the addresses, we'll see that SQLite has assigned them an appropriate id. First, let's use the new lenses we made. Make sure to import Lens.Micro or Control.Lens or whichever (van Laarhoven) lens module you prefer. Haskell Sql Output -- import Lens.Micro -- import Control.Lens addresses <- runBeamSqliteDebug putStrLn conn $ runSelectReturningList $ select ( all_ ( shoppingCartDb ^. shoppingCartUserAddresses )) mapM_ print addresses SELECT \"t0\" . \"id\" AS \"res0\" , \"t0\" . \"address1\" AS \"res1\" , \"t0\" . \"address2\" AS \"res2\" , \"t0\" . \"city\" AS \"res3\" , \"t0\" . \"state\" AS \"res4\" , \"t0\" . \"zip\" AS \"res5\" , \"t0\" . \"for_user__email\" AS \"res6\" FROM \"addresses\" AS \"t0\" ; -- With values: [] Address { _addressId = 1 , _addressLine1 = \"123 Little Street\" , _addressLine2 = Nothing , _addressCity = \"Boston\" , _addressState = \"MA\" , _addressZip = \"12345\" , _addressForUser = UserId \"james@example.com\" } Address { _addressId = 2 , _addressLine1 = \"222 Main Street\" , _addressLine2 = Just \"Ste 1\" , _addressCity = \"Houston\" , _addressState = \"TX\" , _addressZip = \"8888\" , _addressForUser = UserId \"betty@example.com\" } Address { _addressId = 3 , _addressLine1 = \"9999 Residence Ave\" , _addressLine2 = Nothing , _addressCity = \"Sugarland\" , _addressState = \"TX\" , _addressZip = \"8989\" , _addressForUser = UserId \"betty@example.com\" } A note about queries In the last tutorial, we saw how queries and list supported similar interfaces. Namely we saw how limit_ is like take , offset_ like drop , orderBy like an enhanced sortBy , and aggregate like an enhanced groupBy . These corresponded to the LIMIT , OFFSET , ORDER BY , and GROUP BY SQL constructs. The missing SQL operation in this list is the JOIN , which computes the cartesian product of two tables. In other words, a join between table A and table B results in a query of pairs (x, y) for every x in A and every y in B . SQL joins can result in two-way, three-way, four-way, etc. cartesian products. Those familiar with lists in Haskell will note that there is an easy abstraction for taking n -ary cartesian products over lists: monads. The list monad We can use GHCi to see what we mean. * NextSteps > do { x <- [ 1 , 2 , 3 ]; y <- [ 4 , 5 , 6 ]; return ( x , y ); } [( 1 , 4 ),( 1 , 5 ),( 1 , 6 ),( 2 , 4 ),( 2 , 5 ),( 2 , 6 ),( 3 , 4 ),( 3 , 5 ),( 3 , 6 )] We get the two-way cartesian product of [1,2,3] and [4,5,6] . We can make the product arbitrarily long. * NextSteps > do { w <- [ 10 , 20 , 30 ]; x <- [ 1 , 2 , 3 ]; y <- [ 4 , 5 , 6 ]; z <- [ 100 , 200 , 1 ]; return ( x , y , z , w ); } [( 1 , 4 , 100 , 10 ),( 1 , 4 , 200 , 10 ),( 1 , 4 , 1 , 10 ),( 1 , 5 , 100 , 10 ),( 1 , 5 , 200 , 10 ),( 1 , 5 , 1 , 10 ), ... ] We can also use guard from Control.Monad to limit the combinations that the list monad puts together. For example, if we had the lists let usersList = [( 1 , \"james\" ), ( 2 , \"betty\" ), ( 3 , \"tom\" )] addressesList = [( 1 , \"address1\" ), ( 1 , \"address2\" ), ( 3 , \"address3\" )] We can use guard to return all pairs of elements from usersList and addressesList that matched on their first element. For example, * NextSteps > do { user <- usersList ; address <- addressesList ; guard ( fst user == fst address ); return ( user , address ) } [(( 1 , \"james\" ),( 1 , \"address1\" )),(( 1 , \"james\" ),( 1 , \"address2\" )),(( 3 , \"tom\" ),( 3 , \"address3\" ))] The query monad As I claimed in the first tutorial, queries support many of the same interfaces and operations lists do. It follows that queries also expose a monadic interface. For example, to retrieve every pair of user and address, we can write the following query: Haskell Sql Output allPairs <- runBeamSqliteDebug putStrLn conn $ runSelectReturningList $ select $ do user <- all_ ( shoppingCartDb ^. shoppingCartUsers ) address <- all_ ( shoppingCartDb ^. shoppingCartUserAddresses ) return ( user , address ) mapM_ print allPairs SELECT \"t0\" . \"email\" AS \"res0\" , \"t0\" . \"first_name\" AS \"res1\" , \"t0\" . \"last_name\" AS \"res2\" , \"t0\" . \"password\" AS \"res3\" , \"t1\" . \"id\" AS \"res4\" , \"t1\" . \"address1\" AS \"res5\" , \"t1\" . \"address2\" AS \"res6\" , \"t1\" . \"city\" AS \"res7\" , \"t1\" . \"state\" AS \"res8\" , \"t1\" . \"zip\" AS \"res9\" , \"t1\" . \"for_user__email\" AS \"res10\" FROM \"cart_users\" AS \"t0\" INNER JOIN \"addresses\" AS \"t1\" ; -- With values: [] ( User { _userEmail = \"james@example.com\" , _userFirstName = \"James\" , _userLastName = \"Smith\" , _userPassword = \"b4cc344d25a2efe540adbf2678e2304c\" } , Address { _addressId = 1 , _addressLine1 = \"123 Little Street\" , _addressLine2 = Nothing , _addressCity = \"Boston\" , _addressState = \"MA\" , _addressZip = \"12345\" , _addressForUser = UserId \"james@example.com\" } ) ( User { _userEmail = \"james@example.com\" , _userFirstName = \"James\" , _userLastName = \"Smith\" , _userPassword = \"b4cc344d25a2efe540adbf2678e2304c\" } , Address { _addressId = 2 , _addressLine1 = \"222 Main Street\" , _addressLine2 = Just \"Ste 1\" , _addressCity = \"Houston\" , _addressState = \"TX\" , _addressZip = \"8888\" , _addressForUser = UserId \"betty@example.com\" } ) ( User { _userEmail = \"james@example.com\" , _userFirstName = \"James\" , _userLastName = \"Smith\" , _userPassword = \"b4cc344d25a2efe540adbf2678e2304c\" } , Address { _addressId = 3 , _addressLine1 = \"9999 Residence Ave\" , _addressLine2 = Nothing , _addressCity = \"Sugarland\" , _addressState = \"TX\" , _addressZip = \"8989\" , _addressForUser = UserId \"betty@example.com\" } ) ( User { _userEmail = \"betty@example.com\" , _userFirstName = \"Betty\" , _userLastName = \"Jones\" , _userPassword = \"82b054bd83ffad9b6cf8bdb98ce3cc2f\" } , Address { _addressId = 1 , _addressLine1 = \"123 Little Street\" , _addressLine2 = Nothing , _addressCity = \"Boston\" , _addressState = \"MA\" , _addressZip = \"12345\" , _addressForUser = UserId \"james@example.com\" } ) ( User { _userEmail = \"betty@example.com\" , _userFirstName = \"Betty\" , _userLastName = \"Jones\" , _userPassword = \"82b054bd83ffad9b6cf8bdb98ce3cc2f\" } , Address { _addressId = 2 , _addressLine1 = \"222 Main Street\" , _addressLine2 = Just \"Ste 1\" , _addressCity = \"Houston\" , _addressState = \"TX\" , _addressZip = \"8888\" , _addressForUser = UserId \"betty@example.com\" } ) ( User { _userEmail = \"betty@example.com\" , _userFirstName = \"Betty\" , _userLastName = \"Jones\" , _userPassword = \"82b054bd83ffad9b6cf8bdb98ce3cc2f\" } , Address { _addressId = 3 , _addressLine1 = \"9999 Residence Ave\" , _addressLine2 = Nothing , _addressCity = \"Sugarland\" , _addressState = \"TX\" , _addressZip = \"8989\" , _addressForUser = UserId \"betty@example.com\" } ) ( User { _userEmail = \"sam@example.com\" , _userFirstName = \"Sam\" , _userLastName = \"Taylor\" , _userPassword = \"332532dcfaa1cbf61e2a266bd723612c\" } , Address { _addressId = 1 , _addressLine1 = \"123 Little Street\" , _addressLine2 = Nothing , _addressCity = \"Boston\" , _addressState = \"MA\" , _addressZip = \"12345\" , _addressForUser = UserId \"james@example.com\" } ) ( User { _userEmail = \"sam@example.com\" , _userFirstName = \"Sam\" , _userLastName = \"Taylor\" , _userPassword = \"332532dcfaa1cbf61e2a266bd723612c\" } , Address { _addressId = 2 , _addressLine1 = \"222 Main Street\" , _addressLine2 = Just \"Ste 1\" , _addressCity = \"Houston\" , _addressState = \"TX\" , _addressZip = \"8888\" , _addressForUser = UserId \"betty@example.com\" } ) ( User { _userEmail = \"sam@example.com\" , _userFirstName = \"Sam\" , _userLastName = \"Taylor\" , _userPassword = \"332532dcfaa1cbf61e2a266bd723612c\" } , Address { _addressId = 3 , _addressLine1 = \"9999 Residence Ave\" , _addressLine2 = Nothing , _addressCity = \"Sugarland\" , _addressState = \"TX\" , _addressZip = \"8989\" , _addressForUser = UserId \"betty@example.com\" } ) Just like with lists we can also use a construct similar to guard to ensure that we only retrieve users and addresses that are related. The guard_ function takes in expression of type QExpr s Bool which represents a SQL expression that returns a boolean. QExpr s Bool s support all the common operators we have on regular Bool , except they're suffixed with a . . For example, where you'd use (&&) on two Haskell-level Bool s, we'd use (&&.) on QExpr -level bools. Haskell Sql Output usersAndRelatedAddresses <- runBeamSqliteDebug putStrLn conn $ runSelectReturningList $ select $ do user <- all_ ( shoppingCartDb ^. shoppingCartUsers ) address <- all_ ( shoppingCartDb ^. shoppingCartUserAddresses ) guard_ ( address ^. addressForUserId ==. user ^. userEmail ) pure ( user , address ) mapM_ print usersAndRelatedAddresses SELECT \"t0\" . \"email\" AS \"res0\" , \"t0\" . \"first_name\" AS \"res1\" , \"t0\" . \"last_name\" AS \"res2\" , \"t0\" . \"password\" AS \"res3\" , \"t1\" . \"id\" AS \"res4\" , \"t1\" . \"address1\" AS \"res5\" , \"t1\" . \"address2\" AS \"res6\" , \"t1\" . \"city\" AS \"res7\" , \"t1\" . \"state\" AS \"res8\" , \"t1\" . \"zip\" AS \"res9\" , \"t1\" . \"for_user__email\" AS \"res10\" FROM \"cart_users\" AS \"t0\" INNER JOIN \"addresses\" AS \"t1\" WHERE ( \"t1\" . \"for_user__email\" ) = ( \"t0\" . \"email\" ); -- With values: [] ( User { _userEmail = \"james@example.com\" , _userFirstName = \"James\" , _userLastName = \"Smith\" , _userPassword = \"b4cc344d25a2efe540adbf2678e2304c\" } , Address { _addressId = 1 , _addressLine1 = \"123 Little Street\" , _addressLine2 = Nothing , _addressCity = \"Boston\" , _addressState = \"MA\" , _addressZip = \"12345\" , _addressForUser = UserId \"james@example.com\" } ) ( User { _userEmail = \"betty@example.com\" , _userFirstName = \"Betty\" , _userLastName = \"Jones\" , _userPassword = \"82b054bd83ffad9b6cf8bdb98ce3cc2f\" } , Address { _addressId = 2 , _addressLine1 = \"222 Main Street\" , _addressLine2 = Just \"Ste 1\" , _addressCity = \"Houston\" , _addressState = \"TX\" , _addressZip = \"8888\" , _addressForUser = UserId \"betty@example.com\" } ) ( User { _userEmail = \"betty@example.com\" , _userFirstName = \"Betty\" , _userLastName = \"Jones\" , _userPassword = \"82b054bd83ffad9b6cf8bdb98ce3cc2f\" } , Address { _addressId = 3 , _addressLine1 = \"9999 Residence Ave\" , _addressLine2 = Nothing , _addressCity = \"Sugarland\" , _addressState = \"TX\" , _addressZip = \"8989\" , _addressForUser = UserId \"betty@example.com\" } ) Of course this is kind of messy because it involves manually matching the primary key of User with the reference in Address . Alternatively, we can use the references_ predicate to have beam automatically generate a QExpr expression that can match primary keys together. Haskell Sql Output usersAndRelatedAddressesUsingReferences <- runBeamSqliteDebug putStrLn conn $ runSelectReturningList $ select $ do user <- all_ ( shoppingCartDb ^. shoppingCartUsers ) address <- all_ ( shoppingCartDb ^. shoppingCartUserAddresses ) guard_ ( _addressForUser address ` references_ ` user ) pure ( user , address ) mapM_ print usersAndRelatedAddressesUsingReferences SELECT \"t0\" . \"email\" AS \"res0\" , \"t0\" . \"first_name\" AS \"res1\" , \"t0\" . \"last_name\" AS \"res2\" , \"t0\" . \"password\" AS \"res3\" , \"t1\" . \"id\" AS \"res4\" , \"t1\" . \"address1\" AS \"res5\" , \"t1\" . \"address2\" AS \"res6\" , \"t1\" . \"city\" AS \"res7\" , \"t1\" . \"state\" AS \"res8\" , \"t1\" . \"zip\" AS \"res9\" , \"t1\" . \"for_user__email\" AS \"res10\" FROM \"cart_users\" AS \"t0\" INNER JOIN \"addresses\" AS \"t1\" WHERE ( \"t1\" . \"for_user__email\" ) = ( \"t0\" . \"email\" ); -- With values: [] ( User { _userEmail = \"james@example.com\" , _userFirstName = \"James\" , _userLastName = \"Smith\" , _userPassword = \"b4cc344d25a2efe540adbf2678e2304c\" } , Address { _addressId = 1 , _addressLine1 = \"123 Little Street\" , _addressLine2 = Nothing , _addressCity = \"Boston\" , _addressState = \"MA\" , _addressZip = \"12345\" , _addressForUser = UserId \"james@example.com\" } ) ( User { _userEmail = \"betty@example.com\" , _userFirstName = \"Betty\" , _userLastName = \"Jones\" , _userPassword = \"82b054bd83ffad9b6cf8bdb98ce3cc2f\" } , Address { _addressId = 2 , _addressLine1 = \"222 Main Street\" , _addressLine2 = Just \"Ste 1\" , _addressCity = \"Houston\" , _addressState = \"TX\" , _addressZip = \"8888\" , _addressForUser = UserId \"betty@example.com\" } ) ( User { _userEmail = \"betty@example.com\" , _userFirstName = \"Betty\" , _userLastName = \"Jones\" , _userPassword = \"82b054bd83ffad9b6cf8bdb98ce3cc2f\" } , Address { _addressId = 3 , _addressLine1 = \"9999 Residence Ave\" , _addressLine2 = Nothing , _addressCity = \"Sugarland\" , _addressState = \"TX\" , _addressZip = \"8989\" , _addressForUser = UserId \"betty@example.com\" } ) You may have noticed that the joins up until now did not include a SQL ON clause. Instead we joined the tables together, and then used the WHERE clause to filter out results we don't want. If you'd like to use the ON clause to make the SQL clearer or save a line in your code, beam offers the related_ combinator to pull related tables directly into the query monad. Haskell Sql Output usersAndRelatedAddressesUsingRelated <- runBeamSqliteDebug putStrLn conn $ runSelectReturningList $ select $ do address <- all_ ( shoppingCartDb ^. shoppingCartUserAddresses ) user <- related_ ( shoppingCartDb ^. shoppingCartUsers ) ( _addressForUser address ) pure ( user , address ) mapM_ print usersAndRelatedAddressesUsingRelated SELECT \"t1\" . \"email\" AS \"res0\" , \"t1\" . \"first_name\" AS \"res1\" , \"t1\" . \"last_name\" AS \"res2\" , \"t1\" . \"password\" AS \"res3\" , \"t0\" . \"id\" AS \"res4\" , \"t0\" . \"address1\" AS \"res5\" , \"t0\" . \"address2\" AS \"res6\" , \"t0\" . \"city\" AS \"res7\" , \"t0\" . \"state\" AS \"res8\" , \"t0\" . \"zip\" AS \"res9\" , \"t0\" . \"for_user__email\" AS \"res10\" FROM \"addresses\" AS \"t0\" INNER JOIN \"cart_users\" AS \"t1\" ON ( \"t0\" . \"for_user__email\" ) = ( \"t1\" . \"email\" ); -- With values: [] ( User { _userEmail = \"james@example.com\" , _userFirstName = \"James\" , _userLastName = \"Smith\" , _userPassword = \"b4cc344d25a2efe540adbf2678e2304c\" } , Address { _addressId = 1 , _addressLine1 = \"123 Little Street\" , _addressLine2 = Nothing , _addressCity = \"Boston\" , _addressState = \"MA\" , _addressZip = \"12345\" , _addressForUser = UserId \"james@example.com\" } ) ( User { _userEmail = \"betty@example.com\" , _userFirstName = \"Betty\" , _userLastName = \"Jones\" , _userPassword = \"82b054bd83ffad9b6cf8bdb98ce3cc2f\" } , Address { _addressId = 2 , _addressLine1 = \"222 Main Street\" , _addressLine2 = Just \"Ste 1\" , _addressCity = \"Houston\" , _addressState = \"TX\" , _addressZip = \"8888\" , _addressForUser = UserId \"betty@example.com\" } ) ( User { _userEmail = \"betty@example.com\" , _userFirstName = \"Betty\" , _userLastName = \"Jones\" , _userPassword = \"82b054bd83ffad9b6cf8bdb98ce3cc2f\" } , Address { _addressId = 3 , _addressLine1 = \"9999 Residence Ave\" , _addressLine2 = Nothing , _addressCity = \"Sugarland\" , _addressState = \"TX\" , _addressZip = \"8989\" , _addressForUser = UserId \"betty@example.com\" } ) We can also query the addresses for a particular user given a UserId . Haskell Sql Output -- This is a contrived example to show how we can use an arbitrary UserId to fetch a particular user. -- We don't always have access to the full 'User' lying around. For example we may be in a function that -- only accepts 'UserId's. let bettyId = UserId \"betty@example.com\" :: UserId bettysAddresses <- runBeamSqliteDebug putStrLn conn $ runSelectReturningList $ select $ do address <- all_ ( shoppingCartDb ^. shoppingCartUserAddresses ) guard_ ( _addressForUser address ==. val_ bettyId ) pure address mapM_ print bettysAddresses SELECT \"t0\" . \"id\" AS \"res0\" , \"t0\" . \"address1\" AS \"res1\" , \"t0\" . \"address2\" AS \"res2\" , \"t0\" . \"city\" AS \"res3\" , \"t0\" . \"state\" AS \"res4\" , \"t0\" . \"zip\" AS \"res5\" , \"t0\" . \"for_user__email\" AS \"res6\" FROM \"addresses\" AS \"t0\" WHERE ( \"t0\" . \"for_user__email\" ) = ( ? ); -- With values: [SQLText \"betty@example.com\"] Address { _addressId = 2 , _addressLine1 = \"222 Main Street\" , _addressLine2 = Just \"Ste 1\" , _addressCity = \"Houston\" , _addressState = \"TX\" , _addressZip = \"8888\" , _addressForUser = UserId \"betty@example.com\" } Address { _addressId = 3 , _addressLine1 = \"9999 Residence Ave\" , _addressLine2 = Nothing , _addressCity = \"Sugarland\" , _addressState = \"TX\" , _addressZip = \"8989\" , _addressForUser = UserId \"betty@example.com\" } Tip More complicated joins are also supported. See the section on relationships Updates and deletions So far we've only seen how to insert data and query it. There are two other SQL operations that we have not covered: updates and deletions. Beam has full support for these manipulations as well. Updates Like INSERT and SELECT , to run an UPDATE command, we use the runUpdate function, with a value of SqlUpdate . The save function constructs a value of SqlUpdate given a full record. It will generate an UPDATE that will set every field (except for the primary key fields) for the row that completely matches the primary key. Let's first look at updating passwords given a User . For this we can use the saveTo function. Suppose James wants to change his password to the md5 hash of \"supersecure\", which is 52a516ca6df436828d9c0d26e31ef704 . We have a User object representing James so we can simply call saveTo on the update value to update the corresponding record in the database. Haskell Sql Output [ james ] <- runBeamSqliteDebug putStrLn conn $ do runUpdate $ save ( shoppingCartDb ^. shoppingCartUsers ) ( james { _userPassword = \"52a516ca6df436828d9c0d26e31ef704\" }) runSelectReturningList $ lookup_ ( shoppingCartDb ^. shoppingCartUsers ) ( UserId \"james@example.com\" ) putStrLn ( \"James's new password is \" ++ show ( james ^. userPassword )) UPDATE \"cart_users\" SET \"first_name\" =? , \"last_name\" =? , \"password\" =? WHERE ( ? ) = ( \"email\" ); -- With values: [SQLText \"James\",SQLText \"Smith\",SQLText \"52a516ca6df436828d9c0d26e31ef704\",SQLText \"james@example.com\"] SELECT \"t0\" . \"email\" AS \"res0\" , \"t0\" . \"first_name\" AS \"res1\" , \"t0\" . \"last_name\" AS \"res2\" , \"t0\" . \"password\" AS \"res3\" FROM \"cart_users\" AS \"t0\" WHERE ( \"t0\" . \"email\" ) = ( ? ); -- With values: [SQLText \"james@example.com\"] James's new password is \"52a516ca6df436828d9c0d26e31ef704\" Tip lookup_ (defined in Database.Beam.Query ) can be used to easily lookup a single entity given a table entity in a database and a primary key. This works great, but save requires that we have the whole User object at our disposal. Additionally, you'll notice that it causes every field to be set in the UPDATE query. Typically, this doesn't matter, but sometimes we'd like to update fewer fields, multiple rows, or use criteria other than a primary key match. The update function offers finer-grained control over the command submitted to the database. To illustrate use of this function, let's suppose the city of \"Sugarland, TX\" was renamed \"Sugarville, TX\" and had its ZIP code changed to be \"12345\" citywide. The following beam command will update all addresses in the old city to use the new name and ZIP code. Haskell Sql Output addresses <- runBeamSqliteDebug putStrLn conn $ do runUpdate $ update ( shoppingCartDb ^. shoppingCartUserAddresses ) ( \\ address -> mconcat [ address ^. addressCity <-. val_ \"Sugarville\" , address ^. addressZip <-. val_ \"12345\" ]) ( \\ address -> address ^. addressCity ==. val_ \"Sugarland\" &&. address ^. addressState ==. val_ \"TX\" ) runSelectReturningList $ select $ all_ ( shoppingCartDb ^. shoppingCartUserAddresses ) mapM_ print addresses UPDATE \"addresses\" SET \"city\" =? , \"zip\" =? WHERE (( \"city\" ) = ( ? )) AND (( \"state\" ) = ( ? )); -- With values: [SQLText \"Sugarville\",SQLText \"12345\",SQLText \"Sugarland\",SQLText \"TX\"] SELECT \"t0\" . \"id\" AS \"res0\" , \"t0\" . \"address1\" AS \"res1\" , \"t0\" . \"address2\" AS \"res2\" , \"t0\" . \"city\" AS \"res3\" , \"t0\" . \"state\" AS \"res4\" , \"t0\" . \"zip\" AS \"res5\" , \"t0\" . \"for_user__email\" AS \"res6\" FROM \"addresses\" AS \"t0\" ; -- With values: [] Address { _addressId = 1 , _addressLine1 = \"123 Little Street\" , _addressLine2 = Nothing , _addressCity = \"Boston\" , _addressState = \"MA\" , _addressZip = \"12345\" , _addressForUser = UserId \"james@example.com\" } Address { _addressId = 2 , _addressLine1 = \"222 Main Street\" , _addressLine2 = Just \"Ste 1\" , _addressCity = \"Houston\" , _addressState = \"TX\" , _addressZip = \"8888\" , _addressForUser = UserId \"betty@example.com\" } Address { _addressId = 3 , _addressLine1 = \"9999 Residence Ave\" , _addressLine2 = Nothing , _addressCity = \"Sugarville\" , _addressState = \"TX\" , _addressZip = \"12345\" , _addressForUser = UserId \"betty@example.com\" } Deletions Now suppose that Betty has decided to give up her place in Houston. We can use runDelete to run a DELETE command. Haskell Sql runBeamSqliteDebug putStrLn conn $ runDelete $ delete ( shoppingCartDb ^. shoppingCartUserAddresses ) ( \\ address -> address ^. addressCity ==. \"Houston\" &&. _addressForUser address ` references_ ` betty ) DELETE FROM \"addresses\" WHERE (( \"city\" ) = ( ? )) AND (( \"for_user__email\" ) = ( ? )); -- With values: [SQLText \"Houston\",SQLText \"betty@example.com\"] Conclusion In this tutorial we created our first beam relationship. We saw how to use the modifications system to override the default names given to database entities. We saw how to use tableLenses to generate lenses that can be used with any lens library. We used the monadic query interface to write queries that used SQL joins, and we saw how beam makes it easy to automatically pull related tables into our queries. Finally we introduced the runUpdate and runDelete functions and demonstrated several ways to construct UPDATEs and DELETEs. At this point, we've covered enough of the beam interface to start writing interesting programs. Take some time to explore beam and create your own databases. Afterwards, read on for the last part of the tutorial. Actually, any Beamable type can be wholly embedded in another. See the section on models in the user guide for more information. \u21a9 The models guide explains the exact mechanisms used \u21a9","title":"Part 2"},{"location":"tutorials/tutorial2/#introduction","text":"In the last part, we created a simple database with one table. We then used the beam interface to add entities into that table and query them. In this tutorial, we'll see how to update and delete rows and how to establish and query relations between tables. We'll then delve deeper into queries to see how to create queries that return multiple tables.","title":"Introduction"},{"location":"tutorials/tutorial2/#adding-a-related-table","text":"The users in our simple e-commerce application would like to ship orders to their homes. Let's build an addresses model to allow users to add home addresses to their profile. Our table will store United States addresses for now. An address in the United States consists of an auto-incrementing primary key one required house number and street line an optional apartment/suite number line a required city a required 2-letter state/territory code one 5-digit ZIP code Let's build the AddressT table. AddressT will follow a similar formula to UserT , but it will contain a reference to a UserT table. ] data AddressT f = Address { _addressId :: C f Int32 , _addressLine1 :: C f Text , _addressLine2 :: C f ( Maybe Text ) , _addressCity :: C f Text , _addressState :: C f Text , _addressZip :: C f Text , _addressForUser :: PrimaryKey UserT f } deriving ( Generic , Beamable ) type Address = AddressT Identity deriving instance Show ( PrimaryKey UserT Identity ) deriving instance Show Address instance Table AddressT where data PrimaryKey AddressT f = AddressId ( Columnar f Int32 ) deriving ( Generic , Beamable ) primaryKey = AddressId . _addressId type AddressId = PrimaryKey AddressT Identity -- For convenience Tip Above, we used the C constructor instead of Columnar for each column. C is a type synonym for Columnar , and some find it reduces the syntactic overhead of model declaration. Notice that _addressForUser is declared as a PrimaryKey UserT f . This pulls in all the columns necessary for referencing a UserT 1 . Later, we'll also see how beam can use the field to automatically create JOINs. Notice also that _addressId corresponds to our auto-increminting primary key field. In general, beam doesn't care if the underlying field is assigned automatically, only about the type of final values of that field. We have all the tables we need now, so let's go ahead and redefine our newest database type. data ShoppingCartDb f = ShoppingCartDb { _shoppingCartUsers :: f ( TableEntity UserT ) , _shoppingCartUserAddresses :: f ( TableEntity AddressT ) } deriving ( Generic , Database be )","title":"Adding a related table"},{"location":"tutorials/tutorial2/#modifying-the-default-naming-choices","text":"In the last part of the tutorial, we let beam decide our field names for us. This is great for simple cases. However, sometimes you want more control over the naming options. Note Previous versions of this tutorial had instructions on changing the schema type of particular tables. This functionality has been moved from beam-core into the beam-migrate package. See the migrations guide for more information. The defaultDbSettings function generates names using the Haskell record selector names 2 . This function returns the DatabaseType parameterized over DatabaseEntity , which is a type that contains metadata about entity names. We can modify this description after it is created by using the withDbModification function. You can think of withDbModification as applying a transformation function to each name in our database. Most of the time withDbModification needs a full description of the database names. However, most of the time we only want to rename certain columns or tables. We can use the dbModification value to construct a modification that doesn't change any names. We can then use the Haskell record update syntax to update field and column names. This is best illustrated by an example. Recall our Haskell data types above. data UserT f = User { _userEmail :: Columnar f Text , _userFirstName :: Columnar f Text , _userLastName :: Columnar f Text , _userPassword :: Columnar f Text } deriving Generic data AddressT f = Address { _addressId :: C f Int32 , _addressLine1 :: C f Text , _addressLine2 :: C f ( Maybe Text ) , _addressCity :: C f Text , _addressState :: C f Text , _addressZip :: C f Text , _addressForUser :: PrimaryKey UserT f } deriving Generic data ShoppingCartDb f = ShoppingCartDb { _shoppingCartUsers :: f ( TableEntity UserT ) , _shoppingCartUserAddresses :: f ( TableEntity AddressT ) } deriving Generic Now, let's say we want beam to use the name addresses to access the _shoppingCartUserAddresses table, and the names address1 and address2 to access _addressLine1 and _addressLine2 respectively. shoppingCartDb :: DatabaseSettings be ShoppingCartDb shoppingCartDb = defaultDbSettings ` withDbModification ` dbModification { _shoppingCartUserAddresses = setEntityName \"addresses\" <> modifyTableFields tableModification { _addressLine1 = fieldNamed \"address1\" , _addressLine2 = fieldNamed \"address2\" } } Above, we use dbModification to produce a default modification, then we override the _shoppingCartUserAddresses modification to change the addresses table. We modify the table in two ways. First, we use the setEntityName function to change the name of the table. Then, we use modifyTableFields to change the names of each field. The modifications can be combined with the semigroup operator (<>) . We only override the _addressLine1 and _addressLine2 modifications with fieldNamed \"address1\" and fieldNamed \"address2\" . Because tableModification produces a default modification, the other columns are kept at their default value. Tip The OverloadedStrings extension lets us avoid typing fieldNamed . For example, instead of _addressLine1 = fieldNamed \"address1\" we could have written _addressLine1 = \"address1\" When renaming a field referring to a foreign key (for example the _addressForUser field), remember to wrap the field name with the table's PrimaryKey constructor: _addressForUser = UserId \"user\" Given that in part one we defined UserId as the PrimaryKey constructor for UserT : instance Table UserT where data PrimaryKey UserT f = UserId ( Columnar f Text ) deriving ( Generic , Beamable ) primaryKey = UserId . _userEmail If you didn't need to modify any of the field names, you can omit modifyTableFields . For example, to simply produce a database with the first table named users and the second named user_addresses , you can do shoppingCartDb1 :: DatabaseSettings be ShoppingCartDb shoppingCartDb1 = defaultDbSettings ` withDbModification ` dbModification { _shoppingCartUsers = setEntityName \"users\" , _shoppingCartUserAddresses = setEntityName \"user_addresses\" } For the purposes of this tutorial, we'll stick with shoppingCartDb .","title":"Modifying the default naming choices"},{"location":"tutorials/tutorial2/#easier-queries-with-lenses","text":"In the previous part, we accessed table columns by using regular Haskell record syntax. Sometimes, we would like to use the more convenient lens syntax to access columns. Of course, all of beam's definitions are compatible with the lens library -- that is to say, makeLenses will work just fine. However, beam's motivation is, in part, the avoidance of Template Haskell, and it would hardly be worth it if you had to include a Template Haskell splice just to have lenses for the models you declared TH free. In reality, the lens library isn't required to construct valid lenses. Lenses are a plain old Haskell type. We can use beam's Columnar mechanism to automatically derive lenses. The tableLenses function produces a table value where each column is given a type LensFor , which is a newtype wrapper over a correctly constructed, polymorphic Van Laarhoven lens. We can bring these lenses into scope globally via a global pattern match against tableLenses . For example, to get lenses for each column of the AddressT and UserT table. -- Add the following to the top of the file, for GHC >8.2 {-# LANGUAGE ImpredicativeTypes #-} Address ( LensFor addressId ) ( LensFor addressLine1 ) ( LensFor addressLine2 ) ( LensFor addressCity ) ( LensFor addressState ) ( LensFor addressZip ) ( UserId ( LensFor addressForUserId )) = tableLenses User ( LensFor userEmail ) ( LensFor userFirstName ) ( LensFor userLastName ) ( LensFor userPassword ) = tableLenses Note The ImpredicativeTypes language extension is necessary for newer GHC to allow the polymorphically typed lenses to be introduced at the top-level. Older GHCs were more lenient. As in tables, we can generate lenses for databases via the dbLenses function. ShoppingCartDb ( TableLens shoppingCartUsers ) ( TableLens shoppingCartUserAddresses ) = dbLenses We can ask GHCi for the type of a column lens. Prelude Database . Beam Database . Beam . Sqlite Data . Text Database . SQLite . Simple > : t addressId addressId :: Functor f2 => ( Columnar f1 Int32 -> f2 ( Columnar f1 Int32 )) -> AddressT f1 -> f2 ( AddressT f1 ) This lens is compatible with those of the lens library. And a table lens, for good measure Prelude Database . Beam Database . Beam . Sqlite Data . Text Database . SQLite . Simple > : t shoppingCartUsers shoppingCartUsers :: Functor f1 => ( f2 ( TableEntity UserT ) -> f1 ( f2 ( TableEntity UserT ))) -> ShoppingCartDb f2 -> f1 ( ShoppingCartDb f2 ) Warning These lens generating functions are awesome but if you use them in a compiled Haskell module (rather than GHC), GHC may give you odd compile errors about ambiguous types. These occur due to what's known as the monomorphism restriction. You can turn it off using the NoMonomorphismRestriction extension. The monomorphism restriction is part of the Haskell standard, but there has been talk about removing it in future language versions. Basically, it requires GHC to not automatically infer polymorphic types for global definitions. In this case though, polymorphic global definitions is exactly what we want.","title":"Easier queries with lenses"},{"location":"tutorials/tutorial2/#working-with-relations","text":"Now, let's see how we can add related addresses to our database. We begin by opening up a connection for us to use in the rest of the tutorial. First, let's open a new database and create the schema. $ sqlite3 shoppingcart2.db SQLite version 3.14.0 2016-07-26 15:17:14 Enter \".help\" for usage hints. sqlite> CREATE TABLE cart_users (email VARCHAR NOT NULL, first_name VARCHAR NOT NULL, last_name VARCHAR NOT NULL, password VARCHAR NOT NULL, PRIMARY KEY( email )); sqlite> CREATE TABLE addresses ( id INTEGER PRIMARY KEY, address1 VARCHAR NOT NULL, address2 VARCHAR, city VARCHAR NOT NULL, state VARCHAR NOT NULL, zip VARCHAR NOT NULL, for_user__email VARCHAR NOT NULL ); Now, in GHCi, we can use sqlite-simple to get a handle to this database. conn <- open \"shoppingcart2.db\" Before we add addresses, we need to add some users that we can reference. let james = User \"james@example.com\" \"James\" \"Smith\" \"b4cc344d25a2efe540adbf2678e2304c\" betty = User \"betty@example.com\" \"Betty\" \"Jones\" \"82b054bd83ffad9b6cf8bdb98ce3cc2f\" sam = User \"sam@example.com\" \"Sam\" \"Taylor\" \"332532dcfaa1cbf61e2a266bd723612c\" runBeamSqliteDebug putStrLn conn $ runInsert $ insert ( _shoppingCartUsers shoppingCartDb ) $ insertValues [ james , betty , sam ] Now that we have some User objects, we can create associated addresses. Notice that above, we used insertValues to insert concrete User rows. This worked because we could determine every field of User before insertion. Address es however have a pesky auto-incrementing primary key field. We can get around this by inserting expressions instead of values . We can use default_ to stand for a value that the database needs to fill in. We can use val_ to lift a literal value into an expression. With that in mind, let's give James one address, Betty two addresses, and Sam none. let addresses = [ Address default_ ( val_ \"123 Little Street\" ) ( val_ Nothing ) ( val_ \"Boston\" ) ( val_ \"MA\" ) ( val_ \"12345\" ) ( pk james ) , Address default_ ( val_ \"222 Main Street\" ) ( val_ ( Just \"Ste 1\" )) ( val_ \"Houston\" ) ( val_ \"TX\" ) ( val_ \"8888\" ) ( pk betty ) , Address default_ ( val_ \"9999 Residence Ave\" ) ( val_ Nothing ) ( val_ \"Sugarland\" ) ( val_ \"TX\" ) ( val_ \"8989\" ) ( pk betty ) ] runBeamSqliteDebug putStrLn conn $ runInsert $ insert ( _shoppingCartUserAddresses shoppingCartDb ) $ insertExpressions addresses Notice that we used the pk function to assign the reference to the UserT table. pk is a synonym of the primaryKey function from the Table type class. It should be clear what's going on, but if it's not, let's ask GHCi. *NextSteps> pk (james :: User)p UserId \"james@example.com\" If we query for all the addresses, we'll see that SQLite has assigned them an appropriate id. First, let's use the new lenses we made. Make sure to import Lens.Micro or Control.Lens or whichever (van Laarhoven) lens module you prefer. Haskell Sql Output -- import Lens.Micro -- import Control.Lens addresses <- runBeamSqliteDebug putStrLn conn $ runSelectReturningList $ select ( all_ ( shoppingCartDb ^. shoppingCartUserAddresses )) mapM_ print addresses SELECT \"t0\" . \"id\" AS \"res0\" , \"t0\" . \"address1\" AS \"res1\" , \"t0\" . \"address2\" AS \"res2\" , \"t0\" . \"city\" AS \"res3\" , \"t0\" . \"state\" AS \"res4\" , \"t0\" . \"zip\" AS \"res5\" , \"t0\" . \"for_user__email\" AS \"res6\" FROM \"addresses\" AS \"t0\" ; -- With values: [] Address { _addressId = 1 , _addressLine1 = \"123 Little Street\" , _addressLine2 = Nothing , _addressCity = \"Boston\" , _addressState = \"MA\" , _addressZip = \"12345\" , _addressForUser = UserId \"james@example.com\" } Address { _addressId = 2 , _addressLine1 = \"222 Main Street\" , _addressLine2 = Just \"Ste 1\" , _addressCity = \"Houston\" , _addressState = \"TX\" , _addressZip = \"8888\" , _addressForUser = UserId \"betty@example.com\" } Address { _addressId = 3 , _addressLine1 = \"9999 Residence Ave\" , _addressLine2 = Nothing , _addressCity = \"Sugarland\" , _addressState = \"TX\" , _addressZip = \"8989\" , _addressForUser = UserId \"betty@example.com\" }","title":"Working with relations"},{"location":"tutorials/tutorial2/#a-note-about-queries","text":"In the last tutorial, we saw how queries and list supported similar interfaces. Namely we saw how limit_ is like take , offset_ like drop , orderBy like an enhanced sortBy , and aggregate like an enhanced groupBy . These corresponded to the LIMIT , OFFSET , ORDER BY , and GROUP BY SQL constructs. The missing SQL operation in this list is the JOIN , which computes the cartesian product of two tables. In other words, a join between table A and table B results in a query of pairs (x, y) for every x in A and every y in B . SQL joins can result in two-way, three-way, four-way, etc. cartesian products. Those familiar with lists in Haskell will note that there is an easy abstraction for taking n -ary cartesian products over lists: monads.","title":"A note about queries"},{"location":"tutorials/tutorial2/#the-list-monad","text":"We can use GHCi to see what we mean. * NextSteps > do { x <- [ 1 , 2 , 3 ]; y <- [ 4 , 5 , 6 ]; return ( x , y ); } [( 1 , 4 ),( 1 , 5 ),( 1 , 6 ),( 2 , 4 ),( 2 , 5 ),( 2 , 6 ),( 3 , 4 ),( 3 , 5 ),( 3 , 6 )] We get the two-way cartesian product of [1,2,3] and [4,5,6] . We can make the product arbitrarily long. * NextSteps > do { w <- [ 10 , 20 , 30 ]; x <- [ 1 , 2 , 3 ]; y <- [ 4 , 5 , 6 ]; z <- [ 100 , 200 , 1 ]; return ( x , y , z , w ); } [( 1 , 4 , 100 , 10 ),( 1 , 4 , 200 , 10 ),( 1 , 4 , 1 , 10 ),( 1 , 5 , 100 , 10 ),( 1 , 5 , 200 , 10 ),( 1 , 5 , 1 , 10 ), ... ] We can also use guard from Control.Monad to limit the combinations that the list monad puts together. For example, if we had the lists let usersList = [( 1 , \"james\" ), ( 2 , \"betty\" ), ( 3 , \"tom\" )] addressesList = [( 1 , \"address1\" ), ( 1 , \"address2\" ), ( 3 , \"address3\" )] We can use guard to return all pairs of elements from usersList and addressesList that matched on their first element. For example, * NextSteps > do { user <- usersList ; address <- addressesList ; guard ( fst user == fst address ); return ( user , address ) } [(( 1 , \"james\" ),( 1 , \"address1\" )),(( 1 , \"james\" ),( 1 , \"address2\" )),(( 3 , \"tom\" ),( 3 , \"address3\" ))]","title":"The list monad"},{"location":"tutorials/tutorial2/#the-query-monad","text":"As I claimed in the first tutorial, queries support many of the same interfaces and operations lists do. It follows that queries also expose a monadic interface. For example, to retrieve every pair of user and address, we can write the following query: Haskell Sql Output allPairs <- runBeamSqliteDebug putStrLn conn $ runSelectReturningList $ select $ do user <- all_ ( shoppingCartDb ^. shoppingCartUsers ) address <- all_ ( shoppingCartDb ^. shoppingCartUserAddresses ) return ( user , address ) mapM_ print allPairs SELECT \"t0\" . \"email\" AS \"res0\" , \"t0\" . \"first_name\" AS \"res1\" , \"t0\" . \"last_name\" AS \"res2\" , \"t0\" . \"password\" AS \"res3\" , \"t1\" . \"id\" AS \"res4\" , \"t1\" . \"address1\" AS \"res5\" , \"t1\" . \"address2\" AS \"res6\" , \"t1\" . \"city\" AS \"res7\" , \"t1\" . \"state\" AS \"res8\" , \"t1\" . \"zip\" AS \"res9\" , \"t1\" . \"for_user__email\" AS \"res10\" FROM \"cart_users\" AS \"t0\" INNER JOIN \"addresses\" AS \"t1\" ; -- With values: [] ( User { _userEmail = \"james@example.com\" , _userFirstName = \"James\" , _userLastName = \"Smith\" , _userPassword = \"b4cc344d25a2efe540adbf2678e2304c\" } , Address { _addressId = 1 , _addressLine1 = \"123 Little Street\" , _addressLine2 = Nothing , _addressCity = \"Boston\" , _addressState = \"MA\" , _addressZip = \"12345\" , _addressForUser = UserId \"james@example.com\" } ) ( User { _userEmail = \"james@example.com\" , _userFirstName = \"James\" , _userLastName = \"Smith\" , _userPassword = \"b4cc344d25a2efe540adbf2678e2304c\" } , Address { _addressId = 2 , _addressLine1 = \"222 Main Street\" , _addressLine2 = Just \"Ste 1\" , _addressCity = \"Houston\" , _addressState = \"TX\" , _addressZip = \"8888\" , _addressForUser = UserId \"betty@example.com\" } ) ( User { _userEmail = \"james@example.com\" , _userFirstName = \"James\" , _userLastName = \"Smith\" , _userPassword = \"b4cc344d25a2efe540adbf2678e2304c\" } , Address { _addressId = 3 , _addressLine1 = \"9999 Residence Ave\" , _addressLine2 = Nothing , _addressCity = \"Sugarland\" , _addressState = \"TX\" , _addressZip = \"8989\" , _addressForUser = UserId \"betty@example.com\" } ) ( User { _userEmail = \"betty@example.com\" , _userFirstName = \"Betty\" , _userLastName = \"Jones\" , _userPassword = \"82b054bd83ffad9b6cf8bdb98ce3cc2f\" } , Address { _addressId = 1 , _addressLine1 = \"123 Little Street\" , _addressLine2 = Nothing , _addressCity = \"Boston\" , _addressState = \"MA\" , _addressZip = \"12345\" , _addressForUser = UserId \"james@example.com\" } ) ( User { _userEmail = \"betty@example.com\" , _userFirstName = \"Betty\" , _userLastName = \"Jones\" , _userPassword = \"82b054bd83ffad9b6cf8bdb98ce3cc2f\" } , Address { _addressId = 2 , _addressLine1 = \"222 Main Street\" , _addressLine2 = Just \"Ste 1\" , _addressCity = \"Houston\" , _addressState = \"TX\" , _addressZip = \"8888\" , _addressForUser = UserId \"betty@example.com\" } ) ( User { _userEmail = \"betty@example.com\" , _userFirstName = \"Betty\" , _userLastName = \"Jones\" , _userPassword = \"82b054bd83ffad9b6cf8bdb98ce3cc2f\" } , Address { _addressId = 3 , _addressLine1 = \"9999 Residence Ave\" , _addressLine2 = Nothing , _addressCity = \"Sugarland\" , _addressState = \"TX\" , _addressZip = \"8989\" , _addressForUser = UserId \"betty@example.com\" } ) ( User { _userEmail = \"sam@example.com\" , _userFirstName = \"Sam\" , _userLastName = \"Taylor\" , _userPassword = \"332532dcfaa1cbf61e2a266bd723612c\" } , Address { _addressId = 1 , _addressLine1 = \"123 Little Street\" , _addressLine2 = Nothing , _addressCity = \"Boston\" , _addressState = \"MA\" , _addressZip = \"12345\" , _addressForUser = UserId \"james@example.com\" } ) ( User { _userEmail = \"sam@example.com\" , _userFirstName = \"Sam\" , _userLastName = \"Taylor\" , _userPassword = \"332532dcfaa1cbf61e2a266bd723612c\" } , Address { _addressId = 2 , _addressLine1 = \"222 Main Street\" , _addressLine2 = Just \"Ste 1\" , _addressCity = \"Houston\" , _addressState = \"TX\" , _addressZip = \"8888\" , _addressForUser = UserId \"betty@example.com\" } ) ( User { _userEmail = \"sam@example.com\" , _userFirstName = \"Sam\" , _userLastName = \"Taylor\" , _userPassword = \"332532dcfaa1cbf61e2a266bd723612c\" } , Address { _addressId = 3 , _addressLine1 = \"9999 Residence Ave\" , _addressLine2 = Nothing , _addressCity = \"Sugarland\" , _addressState = \"TX\" , _addressZip = \"8989\" , _addressForUser = UserId \"betty@example.com\" } ) Just like with lists we can also use a construct similar to guard to ensure that we only retrieve users and addresses that are related. The guard_ function takes in expression of type QExpr s Bool which represents a SQL expression that returns a boolean. QExpr s Bool s support all the common operators we have on regular Bool , except they're suffixed with a . . For example, where you'd use (&&) on two Haskell-level Bool s, we'd use (&&.) on QExpr -level bools. Haskell Sql Output usersAndRelatedAddresses <- runBeamSqliteDebug putStrLn conn $ runSelectReturningList $ select $ do user <- all_ ( shoppingCartDb ^. shoppingCartUsers ) address <- all_ ( shoppingCartDb ^. shoppingCartUserAddresses ) guard_ ( address ^. addressForUserId ==. user ^. userEmail ) pure ( user , address ) mapM_ print usersAndRelatedAddresses SELECT \"t0\" . \"email\" AS \"res0\" , \"t0\" . \"first_name\" AS \"res1\" , \"t0\" . \"last_name\" AS \"res2\" , \"t0\" . \"password\" AS \"res3\" , \"t1\" . \"id\" AS \"res4\" , \"t1\" . \"address1\" AS \"res5\" , \"t1\" . \"address2\" AS \"res6\" , \"t1\" . \"city\" AS \"res7\" , \"t1\" . \"state\" AS \"res8\" , \"t1\" . \"zip\" AS \"res9\" , \"t1\" . \"for_user__email\" AS \"res10\" FROM \"cart_users\" AS \"t0\" INNER JOIN \"addresses\" AS \"t1\" WHERE ( \"t1\" . \"for_user__email\" ) = ( \"t0\" . \"email\" ); -- With values: [] ( User { _userEmail = \"james@example.com\" , _userFirstName = \"James\" , _userLastName = \"Smith\" , _userPassword = \"b4cc344d25a2efe540adbf2678e2304c\" } , Address { _addressId = 1 , _addressLine1 = \"123 Little Street\" , _addressLine2 = Nothing , _addressCity = \"Boston\" , _addressState = \"MA\" , _addressZip = \"12345\" , _addressForUser = UserId \"james@example.com\" } ) ( User { _userEmail = \"betty@example.com\" , _userFirstName = \"Betty\" , _userLastName = \"Jones\" , _userPassword = \"82b054bd83ffad9b6cf8bdb98ce3cc2f\" } , Address { _addressId = 2 , _addressLine1 = \"222 Main Street\" , _addressLine2 = Just \"Ste 1\" , _addressCity = \"Houston\" , _addressState = \"TX\" , _addressZip = \"8888\" , _addressForUser = UserId \"betty@example.com\" } ) ( User { _userEmail = \"betty@example.com\" , _userFirstName = \"Betty\" , _userLastName = \"Jones\" , _userPassword = \"82b054bd83ffad9b6cf8bdb98ce3cc2f\" } , Address { _addressId = 3 , _addressLine1 = \"9999 Residence Ave\" , _addressLine2 = Nothing , _addressCity = \"Sugarland\" , _addressState = \"TX\" , _addressZip = \"8989\" , _addressForUser = UserId \"betty@example.com\" } ) Of course this is kind of messy because it involves manually matching the primary key of User with the reference in Address . Alternatively, we can use the references_ predicate to have beam automatically generate a QExpr expression that can match primary keys together. Haskell Sql Output usersAndRelatedAddressesUsingReferences <- runBeamSqliteDebug putStrLn conn $ runSelectReturningList $ select $ do user <- all_ ( shoppingCartDb ^. shoppingCartUsers ) address <- all_ ( shoppingCartDb ^. shoppingCartUserAddresses ) guard_ ( _addressForUser address ` references_ ` user ) pure ( user , address ) mapM_ print usersAndRelatedAddressesUsingReferences SELECT \"t0\" . \"email\" AS \"res0\" , \"t0\" . \"first_name\" AS \"res1\" , \"t0\" . \"last_name\" AS \"res2\" , \"t0\" . \"password\" AS \"res3\" , \"t1\" . \"id\" AS \"res4\" , \"t1\" . \"address1\" AS \"res5\" , \"t1\" . \"address2\" AS \"res6\" , \"t1\" . \"city\" AS \"res7\" , \"t1\" . \"state\" AS \"res8\" , \"t1\" . \"zip\" AS \"res9\" , \"t1\" . \"for_user__email\" AS \"res10\" FROM \"cart_users\" AS \"t0\" INNER JOIN \"addresses\" AS \"t1\" WHERE ( \"t1\" . \"for_user__email\" ) = ( \"t0\" . \"email\" ); -- With values: [] ( User { _userEmail = \"james@example.com\" , _userFirstName = \"James\" , _userLastName = \"Smith\" , _userPassword = \"b4cc344d25a2efe540adbf2678e2304c\" } , Address { _addressId = 1 , _addressLine1 = \"123 Little Street\" , _addressLine2 = Nothing , _addressCity = \"Boston\" , _addressState = \"MA\" , _addressZip = \"12345\" , _addressForUser = UserId \"james@example.com\" } ) ( User { _userEmail = \"betty@example.com\" , _userFirstName = \"Betty\" , _userLastName = \"Jones\" , _userPassword = \"82b054bd83ffad9b6cf8bdb98ce3cc2f\" } , Address { _addressId = 2 , _addressLine1 = \"222 Main Street\" , _addressLine2 = Just \"Ste 1\" , _addressCity = \"Houston\" , _addressState = \"TX\" , _addressZip = \"8888\" , _addressForUser = UserId \"betty@example.com\" } ) ( User { _userEmail = \"betty@example.com\" , _userFirstName = \"Betty\" , _userLastName = \"Jones\" , _userPassword = \"82b054bd83ffad9b6cf8bdb98ce3cc2f\" } , Address { _addressId = 3 , _addressLine1 = \"9999 Residence Ave\" , _addressLine2 = Nothing , _addressCity = \"Sugarland\" , _addressState = \"TX\" , _addressZip = \"8989\" , _addressForUser = UserId \"betty@example.com\" } ) You may have noticed that the joins up until now did not include a SQL ON clause. Instead we joined the tables together, and then used the WHERE clause to filter out results we don't want. If you'd like to use the ON clause to make the SQL clearer or save a line in your code, beam offers the related_ combinator to pull related tables directly into the query monad. Haskell Sql Output usersAndRelatedAddressesUsingRelated <- runBeamSqliteDebug putStrLn conn $ runSelectReturningList $ select $ do address <- all_ ( shoppingCartDb ^. shoppingCartUserAddresses ) user <- related_ ( shoppingCartDb ^. shoppingCartUsers ) ( _addressForUser address ) pure ( user , address ) mapM_ print usersAndRelatedAddressesUsingRelated SELECT \"t1\" . \"email\" AS \"res0\" , \"t1\" . \"first_name\" AS \"res1\" , \"t1\" . \"last_name\" AS \"res2\" , \"t1\" . \"password\" AS \"res3\" , \"t0\" . \"id\" AS \"res4\" , \"t0\" . \"address1\" AS \"res5\" , \"t0\" . \"address2\" AS \"res6\" , \"t0\" . \"city\" AS \"res7\" , \"t0\" . \"state\" AS \"res8\" , \"t0\" . \"zip\" AS \"res9\" , \"t0\" . \"for_user__email\" AS \"res10\" FROM \"addresses\" AS \"t0\" INNER JOIN \"cart_users\" AS \"t1\" ON ( \"t0\" . \"for_user__email\" ) = ( \"t1\" . \"email\" ); -- With values: [] ( User { _userEmail = \"james@example.com\" , _userFirstName = \"James\" , _userLastName = \"Smith\" , _userPassword = \"b4cc344d25a2efe540adbf2678e2304c\" } , Address { _addressId = 1 , _addressLine1 = \"123 Little Street\" , _addressLine2 = Nothing , _addressCity = \"Boston\" , _addressState = \"MA\" , _addressZip = \"12345\" , _addressForUser = UserId \"james@example.com\" } ) ( User { _userEmail = \"betty@example.com\" , _userFirstName = \"Betty\" , _userLastName = \"Jones\" , _userPassword = \"82b054bd83ffad9b6cf8bdb98ce3cc2f\" } , Address { _addressId = 2 , _addressLine1 = \"222 Main Street\" , _addressLine2 = Just \"Ste 1\" , _addressCity = \"Houston\" , _addressState = \"TX\" , _addressZip = \"8888\" , _addressForUser = UserId \"betty@example.com\" } ) ( User { _userEmail = \"betty@example.com\" , _userFirstName = \"Betty\" , _userLastName = \"Jones\" , _userPassword = \"82b054bd83ffad9b6cf8bdb98ce3cc2f\" } , Address { _addressId = 3 , _addressLine1 = \"9999 Residence Ave\" , _addressLine2 = Nothing , _addressCity = \"Sugarland\" , _addressState = \"TX\" , _addressZip = \"8989\" , _addressForUser = UserId \"betty@example.com\" } ) We can also query the addresses for a particular user given a UserId . Haskell Sql Output -- This is a contrived example to show how we can use an arbitrary UserId to fetch a particular user. -- We don't always have access to the full 'User' lying around. For example we may be in a function that -- only accepts 'UserId's. let bettyId = UserId \"betty@example.com\" :: UserId bettysAddresses <- runBeamSqliteDebug putStrLn conn $ runSelectReturningList $ select $ do address <- all_ ( shoppingCartDb ^. shoppingCartUserAddresses ) guard_ ( _addressForUser address ==. val_ bettyId ) pure address mapM_ print bettysAddresses SELECT \"t0\" . \"id\" AS \"res0\" , \"t0\" . \"address1\" AS \"res1\" , \"t0\" . \"address2\" AS \"res2\" , \"t0\" . \"city\" AS \"res3\" , \"t0\" . \"state\" AS \"res4\" , \"t0\" . \"zip\" AS \"res5\" , \"t0\" . \"for_user__email\" AS \"res6\" FROM \"addresses\" AS \"t0\" WHERE ( \"t0\" . \"for_user__email\" ) = ( ? ); -- With values: [SQLText \"betty@example.com\"] Address { _addressId = 2 , _addressLine1 = \"222 Main Street\" , _addressLine2 = Just \"Ste 1\" , _addressCity = \"Houston\" , _addressState = \"TX\" , _addressZip = \"8888\" , _addressForUser = UserId \"betty@example.com\" } Address { _addressId = 3 , _addressLine1 = \"9999 Residence Ave\" , _addressLine2 = Nothing , _addressCity = \"Sugarland\" , _addressState = \"TX\" , _addressZip = \"8989\" , _addressForUser = UserId \"betty@example.com\" } Tip More complicated joins are also supported. See the section on relationships","title":"The query monad"},{"location":"tutorials/tutorial2/#updates-and-deletions","text":"So far we've only seen how to insert data and query it. There are two other SQL operations that we have not covered: updates and deletions. Beam has full support for these manipulations as well.","title":"Updates and deletions"},{"location":"tutorials/tutorial2/#updates","text":"Like INSERT and SELECT , to run an UPDATE command, we use the runUpdate function, with a value of SqlUpdate . The save function constructs a value of SqlUpdate given a full record. It will generate an UPDATE that will set every field (except for the primary key fields) for the row that completely matches the primary key. Let's first look at updating passwords given a User . For this we can use the saveTo function. Suppose James wants to change his password to the md5 hash of \"supersecure\", which is 52a516ca6df436828d9c0d26e31ef704 . We have a User object representing James so we can simply call saveTo on the update value to update the corresponding record in the database. Haskell Sql Output [ james ] <- runBeamSqliteDebug putStrLn conn $ do runUpdate $ save ( shoppingCartDb ^. shoppingCartUsers ) ( james { _userPassword = \"52a516ca6df436828d9c0d26e31ef704\" }) runSelectReturningList $ lookup_ ( shoppingCartDb ^. shoppingCartUsers ) ( UserId \"james@example.com\" ) putStrLn ( \"James's new password is \" ++ show ( james ^. userPassword )) UPDATE \"cart_users\" SET \"first_name\" =? , \"last_name\" =? , \"password\" =? WHERE ( ? ) = ( \"email\" ); -- With values: [SQLText \"James\",SQLText \"Smith\",SQLText \"52a516ca6df436828d9c0d26e31ef704\",SQLText \"james@example.com\"] SELECT \"t0\" . \"email\" AS \"res0\" , \"t0\" . \"first_name\" AS \"res1\" , \"t0\" . \"last_name\" AS \"res2\" , \"t0\" . \"password\" AS \"res3\" FROM \"cart_users\" AS \"t0\" WHERE ( \"t0\" . \"email\" ) = ( ? ); -- With values: [SQLText \"james@example.com\"] James's new password is \"52a516ca6df436828d9c0d26e31ef704\" Tip lookup_ (defined in Database.Beam.Query ) can be used to easily lookup a single entity given a table entity in a database and a primary key. This works great, but save requires that we have the whole User object at our disposal. Additionally, you'll notice that it causes every field to be set in the UPDATE query. Typically, this doesn't matter, but sometimes we'd like to update fewer fields, multiple rows, or use criteria other than a primary key match. The update function offers finer-grained control over the command submitted to the database. To illustrate use of this function, let's suppose the city of \"Sugarland, TX\" was renamed \"Sugarville, TX\" and had its ZIP code changed to be \"12345\" citywide. The following beam command will update all addresses in the old city to use the new name and ZIP code. Haskell Sql Output addresses <- runBeamSqliteDebug putStrLn conn $ do runUpdate $ update ( shoppingCartDb ^. shoppingCartUserAddresses ) ( \\ address -> mconcat [ address ^. addressCity <-. val_ \"Sugarville\" , address ^. addressZip <-. val_ \"12345\" ]) ( \\ address -> address ^. addressCity ==. val_ \"Sugarland\" &&. address ^. addressState ==. val_ \"TX\" ) runSelectReturningList $ select $ all_ ( shoppingCartDb ^. shoppingCartUserAddresses ) mapM_ print addresses UPDATE \"addresses\" SET \"city\" =? , \"zip\" =? WHERE (( \"city\" ) = ( ? )) AND (( \"state\" ) = ( ? )); -- With values: [SQLText \"Sugarville\",SQLText \"12345\",SQLText \"Sugarland\",SQLText \"TX\"] SELECT \"t0\" . \"id\" AS \"res0\" , \"t0\" . \"address1\" AS \"res1\" , \"t0\" . \"address2\" AS \"res2\" , \"t0\" . \"city\" AS \"res3\" , \"t0\" . \"state\" AS \"res4\" , \"t0\" . \"zip\" AS \"res5\" , \"t0\" . \"for_user__email\" AS \"res6\" FROM \"addresses\" AS \"t0\" ; -- With values: [] Address { _addressId = 1 , _addressLine1 = \"123 Little Street\" , _addressLine2 = Nothing , _addressCity = \"Boston\" , _addressState = \"MA\" , _addressZip = \"12345\" , _addressForUser = UserId \"james@example.com\" } Address { _addressId = 2 , _addressLine1 = \"222 Main Street\" , _addressLine2 = Just \"Ste 1\" , _addressCity = \"Houston\" , _addressState = \"TX\" , _addressZip = \"8888\" , _addressForUser = UserId \"betty@example.com\" } Address { _addressId = 3 , _addressLine1 = \"9999 Residence Ave\" , _addressLine2 = Nothing , _addressCity = \"Sugarville\" , _addressState = \"TX\" , _addressZip = \"12345\" , _addressForUser = UserId \"betty@example.com\" }","title":"Updates"},{"location":"tutorials/tutorial2/#deletions","text":"Now suppose that Betty has decided to give up her place in Houston. We can use runDelete to run a DELETE command. Haskell Sql runBeamSqliteDebug putStrLn conn $ runDelete $ delete ( shoppingCartDb ^. shoppingCartUserAddresses ) ( \\ address -> address ^. addressCity ==. \"Houston\" &&. _addressForUser address ` references_ ` betty ) DELETE FROM \"addresses\" WHERE (( \"city\" ) = ( ? )) AND (( \"for_user__email\" ) = ( ? )); -- With values: [SQLText \"Houston\",SQLText \"betty@example.com\"]","title":"Deletions"},{"location":"tutorials/tutorial2/#conclusion","text":"In this tutorial we created our first beam relationship. We saw how to use the modifications system to override the default names given to database entities. We saw how to use tableLenses to generate lenses that can be used with any lens library. We used the monadic query interface to write queries that used SQL joins, and we saw how beam makes it easy to automatically pull related tables into our queries. Finally we introduced the runUpdate and runDelete functions and demonstrated several ways to construct UPDATEs and DELETEs. At this point, we've covered enough of the beam interface to start writing interesting programs. Take some time to explore beam and create your own databases. Afterwards, read on for the last part of the tutorial. Actually, any Beamable type can be wholly embedded in another. See the section on models in the user guide for more information. \u21a9 The models guide explains the exact mechanisms used \u21a9","title":"Conclusion"},{"location":"tutorials/tutorial3/","text":"Introduction In the last part, we extended our shopping cart database to let users add multiple addresses. We saw how to establish one-to-many relations between two tables, and how to use the monadic query interface to write SQL JOINs. In this installment, we'll be adding support for products and orders to our database schema. We'll see how to use an intermediary table to create many-to-many relations and how to write LEFT JOINs. Finally, we'll see how to use Nullable to create optional foreign key references. Creating tables is easy now Let's create our products table. By now, the pattern for adding a new table to the schema should be pretty familiar, so I'm going to skip the explanation. data ProductT f = Product { _productId :: C f Int32 , _productTitle :: C f Text , _productDescription :: C f Text , _productPrice :: C f Int32 {- Price in cents -} } deriving ( Generic , Beamable ) type Product = ProductT Identity deriving instance Show Product instance Table ProductT where data PrimaryKey ProductT f = ProductId ( Columnar f Int32 ) deriving ( Generic , Beamable ) primaryKey = ProductId . _productId For orders, we want to store an id, date created, and the user who made the order. We'd also like to create an optional link to a shipping information table. When the shipping information is created, we'll fill in the shipping information in the order. In order to create the optional reference, we're going to use the Nullable tag modifier to modify the column tag. Nullable will turn all fields of type x into Maybe x . Note that we could also create this relation by installing a primary key on the shipping info table, and this is arguably the better option. However, we'll go with a nullable foreign key here to show the full breadth of beam's features, and because this sort of relation exists in many existing databases. import Data.Time deriving instance Show ( PrimaryKey AddressT Identity ) data OrderT f = Order { _orderId :: Columnar f Int32 , _orderDate :: Columnar f LocalTime , _orderForUser :: PrimaryKey UserT f , _orderShipToAddress :: PrimaryKey AddressT f , _orderShippingInfo :: PrimaryKey ShippingInfoT ( Nullable f ) } deriving ( Generic , Beamable ) type Order = OrderT Identity deriving instance Show Order instance Table OrderT where data PrimaryKey OrderT f = OrderId ( Columnar f Int32 ) deriving ( Generic , Beamable ) primaryKey = OrderId . _orderId data ShippingCarrier = USPS | FedEx | UPS | DHL deriving ( Show , Read , Eq , Ord , Enum ) data ShippingInfoT f = ShippingInfo { _shippingInfoId :: Columnar f Int32 , _shippingInfoCarrier :: Columnar f ShippingCarrier , _shippingInfoTrackingNumber :: Columnar f Text } deriving ( Generic , Beamable ) type ShippingInfo = ShippingInfoT Identity deriving instance Show ShippingInfo instance Table ShippingInfoT where data PrimaryKey ShippingInfoT f = ShippingInfoId ( Columnar f Int32 ) deriving ( Generic , Beamable ) primaryKey = ShippingInfoId . _shippingInfoId deriving instance Show ( PrimaryKey ShippingInfoT ( Nullable Identity )) In the above example, we show how to use a custom data type as a beam column. Recall that beam lets you store any Haskell type in a Columnar . However, at some point, we will need to demonstrate to SQLite how to store values of type ShippingCarrier . We will come back to this later. We would also like to be able to associate a list of products with each order as line items. To do this we will create a table with two foreign keys. This table will establish a many-to-many relationship between orders and products. deriving instance Show ( PrimaryKey OrderT Identity ) deriving instance Show ( PrimaryKey ProductT Identity ) data LineItemT f = LineItem { _lineItemInOrder :: PrimaryKey OrderT f , _lineItemForProduct :: PrimaryKey ProductT f , _lineItemQuantity :: Columnar f Int32 } deriving ( Generic , Beamable ) type LineItem = LineItemT Identity deriving instance Show LineItem instance Table LineItemT where data PrimaryKey LineItemT f = LineItemId ( PrimaryKey OrderT f ) ( PrimaryKey ProductT f ) deriving ( Generic , Beamable ) primaryKey = LineItemId <$> _lineItemInOrder <*> _lineItemForProduct Tip We used the Applicative instance for (->) a above to write the primaryKey function. The Applicative ((->) a) instance operates like an unwrapper Reader of a . The applicative actions are then functions from a -> x that inject values from the a into the applicative bind. Now we'll add all these tables to our database. -- Some convenience lenses LineItem _ _ ( LensFor lineItemQuantity ) = tableLenses Product ( LensFor productId ) ( LensFor productTitle ) ( LensFor productDescription ) ( LensFor productPrice ) = tableLenses data ShoppingCartDb f = ShoppingCartDb { _shoppingCartUsers :: f ( TableEntity UserT ) , _shoppingCartUserAddresses :: f ( TableEntity AddressT ) , _shoppingCartProducts :: f ( TableEntity ProductT ) , _shoppingCartOrders :: f ( TableEntity OrderT ) , _shoppingCartShippingInfos :: f ( TableEntity ShippingInfoT ) , _shoppingCartLineItems :: f ( TableEntity LineItemT ) } deriving ( Generic , Database be ) ShoppingCartDb ( TableLens shoppingCartUsers ) ( TableLens shoppingCartUserAddresses ) ( TableLens shoppingCartProducts ) ( TableLens shoppingCartOrders ) ( TableLens shoppingCartShippingInfos ) ( TableLens shoppingCartLineItems ) = dbLenses shoppingCartDb :: DatabaseSettings be ShoppingCartDb shoppingCartDb = defaultDbSettings ` withDbModification ` dbModification { _shoppingCartUserAddresses = setEntityName \"addresses\" <> modifyTableFields tableModification { _addressLine1 = \"address1\" , _addressLine2 = \"address2\" }, _shoppingCartProducts = setEntityName \"products\" , _shoppingCartOrders = setEntityName \"orders\" <> modifyTableFields tableModification { _orderShippingInfo = ShippingInfoId \"shipping_info__id\" }, _shoppingCartShippingInfos = setEntityName \"shipping_info\" <> modifyTableFields tableModification { _shippingInfoId = \"id\" , _shippingInfoCarrier = \"carrier\" , _shippingInfoTrackingNumber = \"tracking_number\" }, _shoppingCartLineItems = setEntityName \"line_items\" } Fixtures Let's put some sample data into a new database. conn <- open \"shoppingcart3.db\" execute_ conn \"CREATE TABLE cart_users (email VARCHAR NOT NULL, first_name VARCHAR NOT NULL, last_name VARCHAR NOT NULL, password VARCHAR NOT NULL, PRIMARY KEY( email ));\" execute_ conn \"CREATE TABLE addresses ( id INTEGER PRIMARY KEY AUTOINCREMENT, address1 VARCHAR NOT NULL, address2 VARCHAR, city VARCHAR NOT NULL, state VARCHAR NOT NULL, zip VARCHAR NOT NULL, for_user__email VARCHAR NOT NULL );\" execute_ conn \"CREATE TABLE products ( id INTEGER PRIMARY KEY AUTOINCREMENT, title VARCHAR NOT NULL, description VARCHAR NOT NULL, price INT NOT NULL );\" execute_ conn \"CREATE TABLE orders ( id INTEGER PRIMARY KEY AUTOINCREMENT, date TIMESTAMP NOT NULL, for_user__email VARCHAR NOT NULL, ship_to_address__id INT NOT NULL, shipping_info__id INT);\" execute_ conn \"CREATE TABLE shipping_info ( id INTEGER PRIMARY KEY AUTOINCREMENT, carrier VARCHAR NOT NULL, tracking_number VARCHAR NOT NULL);\" execute_ conn \"CREATE TABLE line_items (item_in_order__id INTEGER NOT NULL, item_for_product__id INTEGER NOT NULL, item_quantity INTEGER NOT NULL)\" Let's put some sample data into our database. Below, we will use the beam-sqlite functions insertReturning and runInsertReturningList to insert rows and retrieve the inserted rows from the database. This will let us see what values the auto-incremented id columns took on, which will allow us to create references to these inserted rows. let users @ [ james , betty , sam ] = [ User \"james@example.com\" \"James\" \"Smith\" \"b4cc344d25a2efe540adbf2678e2304c\" {- james -} , User \"betty@example.com\" \"Betty\" \"Jones\" \"82b054bd83ffad9b6cf8bdb98ce3cc2f\" {- betty -} , User \"sam@example.com\" \"Sam\" \"Taylor\" \"332532dcfaa1cbf61e2a266bd723612c\" {- sam -} ] addresses = [ Address default_ ( val_ \"123 Little Street\" ) ( val_ Nothing ) ( val_ \"Boston\" ) ( val_ \"MA\" ) ( val_ \"12345\" ) ( pk james ) , Address default_ ( val_ \"222 Main Street\" ) ( val_ ( Just \"Ste 1\" )) ( val_ \"Houston\" ) ( val_ \"TX\" ) ( val_ \"8888\" ) ( pk betty ) , Address default_ ( val_ \"9999 Residence Ave\" ) ( val_ Nothing ) ( val_ \"Sugarland\" ) ( val_ \"TX\" ) ( val_ \"8989\" ) ( pk betty ) ] products = [ Product default_ ( val_ \"Red Ball\" ) ( val_ \"A bright red, very spherical ball\" ) ( val_ 1000 ) , Product default_ ( val_ \"Math Textbook\" ) ( val_ \"Contains a lot of important math theorems and formulae\" ) ( val_ 2500 ) , Product default_ ( val_ \"Intro to Haskell\" ) ( val_ \"Learn the best programming language in the world\" ) ( val_ 3000 ) , Product default_ ( val_ \"Suitcase\" ) \"A hard durable suitcase\" 15000 ] ( jamesAddress1 , bettyAddress1 , bettyAddress2 , redBall , mathTextbook , introToHaskell , suitcase ) <- runBeamSqliteDebug putStrLn conn $ do runInsert $ insert ( shoppingCartDb ^. shoppingCartUsers ) $ insertValues users [ jamesAddress1 , bettyAddress1 , bettyAddress2 ] <- runInsertReturningList $ insertReturning ( shoppingCartDb ^. shoppingCartUserAddresses ) $ insertExpressions addresses [ redBall , mathTextbook , introToHaskell , suitcase ] <- runInsertReturningList $ insertReturning ( shoppingCartDb ^. shoppingCartProducts ) $ insertExpressions products pure ( jamesAddress1 , bettyAddress1 , bettyAddress2 , redBall , mathTextbook , introToHaskell , suitcase ) Now, if we take a look at one of the returned addresses, like jamesAddress1 , we see it has had the default_ values assigned correctly. Prelude Database . Beam Database . Beam . Sqlite Data . Time Database . SQLite . Simple Data . Text Lens . Micro > jamesAddress1 Address { _addressId = 1 , _addressLine1 = \"123 Little Street\" , _addressLine2 = Nothing , _addressCity = \"Boston\" , _addressState = \"MA\" , _addressZip = \"12345\" , _addressForUser = UserId \"james@example.com\" } Marshalling a custom type Now we can insert shipping information. Of course, the shipping information contains the ShippingCarrier enumeration. bettyShippingInfo <- runBeamSqliteDebug putStrLn conn $ do [ bettyShippingInfo ] <- runInsertReturningList $ insertReturning ( shoppingCartDb ^. shoppingCartShippingInfos ) $ insertExpressions [ ShippingInfo default_ ( val_ USPS ) ( val_ \"12345790ABCDEFGHI\" ) ] pure bettyShippingInfo If you run this, you'll get an error from GHCi. < interactive > : 845 : 7 : error : \u2022 No instance for ( FromBackendRow Sqlite ShippingCarrier ) arising from a use of \u2018 runInsertReturningList \u2019 \u2022 In a stmt of a 'do' block : [ bettyShippingInfo ] <- runInsertReturningList $ insertReturning ( shoppingCartDb ^ . shoppingCartShippingInfos ) $ insertExpressions [ ShippingInfo default_ (val_ USPS) (val_ \"12345790ABCDEFGHI\") ] ... < interactive > : 847 : 50 : error : \u2022 No instance for ( Database . Beam . Backend . SQL . SQL92 . HasSqlValueSyntax Database . Beam . Sqlite . Syntax . SqliteValueSyntax ShippingCarrier ) These errors are because there's no way to express a ShippingCarrier in the backend syntax. We can fix this by writing instances for beam. We can re-use the functionality we already have for String . The HasSqlValueSyntax class tells us how to convert a Haskell value into a corresponding backend value. import Database.Beam.Backend.SQL : set - XUndecidableInstances instance HasSqlValueSyntax be String => HasSqlValueSyntax be ShippingCarrier where sqlValueSyntax = autoSqlValueSyntax autoSqlValueSyntax uses the underlying Show instance to serialize a type to a string representation. The FromBackendRow class tells us how to convert a value from the database into a corresponding Haskell value. import qualified Data.Text as T -- for unpack instance FromBackendRow Sqlite ShippingCarrier where fromBackendRow = read . T . unpack <$> fromBackendRow Since, autoSqlValueSyntax uses the Show instance, we can simply use the Read instance. Now, if we try to insert the shipping info again, it works. bettyShippingInfo <- runBeamSqliteDebug putStrLn conn $ do [ bettyShippingInfo ] <- runInsertReturningList $ insertReturning ( shoppingCartDb ^. shoppingCartShippingInfos ) $ insertExpressions [ ShippingInfo default_ ( val_ USPS ) ( val_ \"12345790ABCDEFGHI\" ) ] pure bettyShippingInfo And if we look at the value of bettyShippingInfo , ShippingCarrier has been stored correctly. > bettyShippingInfo ShippingInfo { _shippingInfoId = 1 , _shippingInfoCarrier = USPS , _shippingInfoTrackingNumber = \"12345790ABCDEFGHI\" } Now, let's insert some orders that just came in. We want to insert transactions with the current database timestamp (i.e., CURRENT_TIMESTAMP in SQL). We can do this using insertExpressions . If you run the example below, you'll see the resulting rows have a timestamp set by the database. Haskell Sql Console [ jamesOrder1 , bettyOrder1 , jamesOrder2 ] <- runBeamSqliteDebug putStrLn conn $ do runInsertReturningList $ insertReturning ( shoppingCartDb ^. shoppingCartOrders ) $ insertExpressions $ [ Order default_ currentTimestamp_ ( val_ ( pk james )) ( val_ ( pk jamesAddress1 )) nothing_ , Order default_ currentTimestamp_ ( val_ ( pk betty )) ( val_ ( pk bettyAddress1 )) ( just_ ( val_ ( pk bettyShippingInfo ))) , Order default_ currentTimestamp_ ( val_ ( pk james )) ( val_ ( pk jamesAddress1 )) nothing_ ] print jamesOrder1 print bettyOrder1 print jamesOrder2 INSERT INTO \"orders\" ( \"date\" , \"for_user__email\" , \"ship_to_address__id\" , \"shipping_info__id\" ) VALUES ( CURRENT_TIMESTAMP , ? , ? , NULL ); -- With values: [SQLText \"james@example.com\",SQLInteger 1] INSERT INTO \"orders\" ( \"date\" , \"for_user__email\" , \"ship_to_address__id\" , \"shipping_info__id\" ) VALUES ( CURRENT_TIMESTAMP , ? , ? , ? ); -- With values: [SQLText \"betty@example.com\",SQLInteger 2,SQLInteger 1] INSERT INTO \"orders\" ( \"date\" , \"for_user__email\" , \"ship_to_address__id\" , \"shipping_info__id\" ) VALUES ( CURRENT_TIMESTAMP , ? , ? , NULL ); -- With values: [SQLText \"james@example.com\",SQLInteger 1] Order {_orderId = 1, _orderDate = 2022-01-18 01:28:55, _orderForUser = UserId \"james@example.com\", _orderShipToAddress = AddressId 1, _orderShippingInfo = ShippingInfoId Nothing} Order {_orderId = 2, _orderDate = 2022-01-18 01:28:55, _orderForUser = UserId \"betty@example.com\", _orderShipToAddress = AddressId 2, _orderShippingInfo = ShippingInfoId (Just 1)} Order {_orderId = 3, _orderDate = 2022-01-18 01:28:55, _orderForUser = UserId \"james@example.com\", _orderShipToAddress = AddressId 1, _orderShippingInfo = ShippingInfoId Nothing} Finally, let's add some line items Haskell Sql let lineItems = [ LineItem ( pk jamesOrder1 ) ( pk redBall ) 10 , LineItem ( pk jamesOrder1 ) ( pk mathTextbook ) 1 , LineItem ( pk jamesOrder1 ) ( pk introToHaskell ) 4 , LineItem ( pk bettyOrder1 ) ( pk mathTextbook ) 3 , LineItem ( pk bettyOrder1 ) ( pk introToHaskell ) 3 , LineItem ( pk jamesOrder2 ) ( pk mathTextbook ) 1 ] runBeamSqliteDebug putStrLn conn $ do runInsert $ insert ( shoppingCartDb ^. shoppingCartLineItems ) $ insertValues lineItems INSERT INTO \"line_items\" ( \"item_in_order__id\" , \"item_for_product__id\" , \"item_quantity\" ) VALUES ( ? , ? , ? ), ( ? , ? , ? ), ( ? , ? , ? ), ( ? , ? , ? ), ( ? , ? , ? ), ( ? , ? , ? ); -- With values: [SQLInteger 1,SQLInteger 1,SQLInteger 10,SQLInteger 1,SQLInteger 2,SQLInteger 1,SQLInteger 1,SQLInteger 3,SQLInteger 4,SQLInteger 2,SQLInteger 2,SQLInteger 3,SQLInteger 2,SQLInteger 3,SQLInteger 3,SQLInteger 3,SQLInteger 2,SQLInteger 1] Phew! Let's write some queries on this data! Would you like some left joins with that? Suppose we want to do some analytics on our users, and so we want to know how many orders each user has made in our system. We can write a query to list every user along with the orders they've made. We can use leftJoin_ to include all users in our result set, even those who have no orders. Haskell Sql Output usersAndOrders <- runBeamSqliteDebug putStrLn conn $ runSelectReturningList $ select $ do user <- all_ ( shoppingCartDb ^. shoppingCartUsers ) order <- leftJoin_ ( all_ ( shoppingCartDb ^. shoppingCartOrders )) ( \\ order -> _orderForUser order ` references_ ` user ) pure ( user , order ) mapM_ print usersAndOrders SELECT \"t0\" . \"email\" AS \"res0\" , \"t0\" . \"first_name\" AS \"res1\" , \"t0\" . \"last_name\" AS \"res2\" , \"t0\" . \"password\" AS \"res3\" , \"t1\" . \"id\" AS \"res4\" , \"t1\" . \"date\" AS \"res5\" , \"t1\" . \"for_user__email\" AS \"res6\" , \"t1\" . \"ship_to_address__id\" AS \"res7\" , \"t1\" . \"shipping_info__id\" AS \"res8\" FROM \"cart_users\" AS \"t0\" LEFT JOIN \"orders\" AS \"t1\" ON ( \"t1\" . \"for_user__email\" ) = ( \"t0\" . \"email\" ); -- With values: [] ( User { _userEmail = \"james@example.com\" , _userFirstName = \"James\" , _userLastName = \"Smith\" , _userPassword = \"b4cc344d25a2efe540adbf2678e2304c\" } , Just ( Order { _orderId = 1 , _orderDate = 2022 - 01 - 18 01 : 29 : 02 , _orderForUser = UserId \"james@example.com\" , _orderShipToAddress = AddressId 1 , _orderShippingInfo = ShippingInfoId Nothing } )) ( User { _userEmail = \"james@example.com\" , _userFirstName = \"James\" , _userLastName = \"Smith\" , _userPassword = \"b4cc344d25a2efe540adbf2678e2304c\" } , Just ( Order { _orderId = 3 , _orderDate = 2022 - 01 - 18 01 : 29 : 02 , _orderForUser = UserId \"james@example.com\" , _orderShipToAddress = AddressId 1 , _orderShippingInfo = ShippingInfoId Nothing } )) ( User { _userEmail = \"betty@example.com\" , _userFirstName = \"Betty\" , _userLastName = \"Jones\" , _userPassword = \"82b054bd83ffad9b6cf8bdb98ce3cc2f\" } , Just ( Order { _orderId = 2 , _orderDate = 2022 - 01 - 18 01 : 29 : 02 , _orderForUser = UserId \"betty@example.com\" , _orderShipToAddress = AddressId 2 , _orderShippingInfo = ShippingInfoId ( Just 1 ) } )) ( User { _userEmail = \"sam@example.com\" , _userFirstName = \"Sam\" , _userLastName = \"Taylor\" , _userPassword = \"332532dcfaa1cbf61e2a266bd723612c\" } , Nothing ) Notice that Sam is included in the result set, even though he doesn't have any associated orders. Instead of a Just (Order ..) , Nothing is returned instead. Next, perhaps our marketing team wanted to send e-mails out to all users with no orders. We can use isNothing_ or isJust_ to determine the status if a nullable table or QExpr s (Maybe x) . The following query uses isNothing_ to find users who have no associated orders. Haskell Sql Output usersWithNoOrders <- runBeamSqliteDebug putStrLn conn $ runSelectReturningList $ select $ do user <- all_ ( shoppingCartDb ^. shoppingCartUsers ) order <- leftJoin_ ( all_ ( shoppingCartDb ^. shoppingCartOrders )) ( \\ order -> _orderForUser order ` references_ ` user ) guard_ ( isNothing_ order ) pure user mapM_ print usersWithNoOrders SELECT \"t0\" . \"email\" AS \"res0\" , \"t0\" . \"first_name\" AS \"res1\" , \"t0\" . \"last_name\" AS \"res2\" , \"t0\" . \"password\" AS \"res3\" FROM \"cart_users\" AS \"t0\" LEFT JOIN \"orders\" AS \"t1\" ON ( \"t1\" . \"for_user__email\" ) = ( \"t0\" . \"email\" ) WHERE ((((( \"t1\" . \"id\" ) IS NULL ) AND (( \"t1\" . \"date\" ) IS NULL )) AND (( \"t1\" . \"for_user__email\" ) IS NULL )) AND (( \"t1\" . \"ship_to_address__id\" ) IS NULL )) AND (( \"t1\" . \"shipping_info__id\" ) IS NULL ); -- With values: [] User { _userEmail = \"sam@example.com\" , _userFirstName = \"Sam\" , _userLastName = \"Taylor\" , _userPassword = \"332532dcfaa1cbf61e2a266bd723612c\" } We see that beam generates a sensible SQL SELECT and WHERE clause. We can also use the exists_ combinator to utilize the SQL EXISTS clause. Haskell Sql Output usersWithNoOrders <- runBeamSqliteDebug putStrLn conn $ runSelectReturningList $ select $ do user <- all_ ( shoppingCartDb ^. shoppingCartUsers ) guard_ ( not_ ( exists_ ( filter_ ( \\ order -> _orderForUser order ` references_ ` user ) ( all_ ( shoppingCartDb ^. shoppingCartOrders ))))) pure user mapM_ print usersWithNoOrders SELECT \"t0\" . \"email\" AS \"res0\" , \"t0\" . \"first_name\" AS \"res1\" , \"t0\" . \"last_name\" AS \"res2\" , \"t0\" . \"password\" AS \"res3\" FROM \"cart_users\" AS \"t0\" WHERE NOT ( EXISTS ( SELECT \"sub_t0\" . \"id\" AS \"res0\" , \"sub_t0\" . \"date\" AS \"res1\" , \"sub_t0\" . \"for_user__email\" AS \"res2\" , \"sub_t0\" . \"ship_to_address__id\" AS \"res3\" , \"sub_t0\" . \"shipping_info__id\" AS \"res4\" FROM \"orders\" AS \"sub_t0\" WHERE ( \"sub_t0\" . \"for_user__email\" ) = ( \"t0\" . \"email\" ))); -- With values: [] User { _userEmail = \"sam@example.com\" , _userFirstName = \"Sam\" , _userLastName = \"Taylor\" , _userPassword = \"332532dcfaa1cbf61e2a266bd723612c\" } Now suppose we wanted to do some analysis on the orders themselves. To start, we want to get the orders sorted by their portion of revenue. We can use aggregate_ to list every order and the total amount of all products in that order. Haskell Sql Output ordersWithCostOrdered <- runBeamSqliteDebug putStrLn conn $ runSelectReturningList $ select $ orderBy_ ( \\ ( order , total ) -> desc_ total ) $ aggregate_ ( \\ ( order , lineItem , product ) -> ( group_ order , sum_ ( lineItem ^. lineItemQuantity * product ^. productPrice ))) $ do lineItem <- all_ ( shoppingCartDb ^. shoppingCartLineItems ) order <- related_ ( shoppingCartDb ^. shoppingCartOrders ) ( _lineItemInOrder lineItem ) product <- related_ ( shoppingCartDb ^. shoppingCartProducts ) ( _lineItemForProduct lineItem ) pure ( order , lineItem , product ) mapM_ print ordersWithCostOrdered SELECT \"t1\" . \"id\" AS \"res0\" , \"t1\" . \"date\" AS \"res1\" , \"t1\" . \"for_user__email\" AS \"res2\" , \"t1\" . \"ship_to_address__id\" AS \"res3\" , \"t1\" . \"shipping_info__id\" AS \"res4\" , SUM (( \"t0\" . \"item_quantity\" ) * ( \"t2\" . \"price\" )) AS \"res5\" FROM \"line_items\" AS \"t0\" INNER JOIN \"orders\" AS \"t1\" ON ( \"t0\" . \"item_in_order__id\" ) = ( \"t1\" . \"id\" ) INNER JOIN \"products\" AS \"t2\" ON ( \"t0\" . \"item_for_product__id\" ) = ( \"t2\" . \"id\" ) GROUP BY \"t1\" . \"id\" , \"t1\" . \"date\" , \"t1\" . \"for_user__email\" , \"t1\" . \"ship_to_address__id\" , \"t1\" . \"shipping_info__id\" ORDER BY SUM (( \"t0\" . \"item_quantity\" ) * ( \"t2\" . \"price\" )) DESC ; -- With values: [] ( Order { _orderId = 1 , _orderDate = 2022 - 01 - 18 01 : 29 : 15 , _orderForUser = UserId \"james@example.com\" , _orderShipToAddress = AddressId 1 , _orderShippingInfo = ShippingInfoId Nothing } , Just 24500 ) ( Order { _orderId = 2 , _orderDate = 2022 - 01 - 18 01 : 29 : 15 , _orderForUser = UserId \"betty@example.com\" , _orderShipToAddress = AddressId 2 , _orderShippingInfo = ShippingInfoId ( Just 1 ) } , Just 16500 ) ( Order { _orderId = 3 , _orderDate = 2022 - 01 - 18 01 : 29 : 15 , _orderForUser = UserId \"james@example.com\" , _orderShipToAddress = AddressId 1 , _orderShippingInfo = ShippingInfoId Nothing } , Just 2500 ) We can also get the total amount spent by each user, even including users with no orders. Notice that we have to use maybe_ below in order to handle the fact that some tables have been introduced into our query with a left join. maybe_ is to QExpr what maybe is to normal Haskell values. maybe_ is polymorphic to either QExpr s or full on tables of QExpr s. For our purposes, the type of maybe_ is maybe_ :: QExpr be s a -> ( QExpr be s b -> QExpr be s a ) -> QExpr be s ( Maybe b ) -> QExpr be s a With that in mind, we can write the query to get the total spent by user Haskell Sql Output allUsersAndTotals <- runBeamSqliteDebug putStrLn conn $ runSelectReturningList $ select $ orderBy_ ( \\ ( user , total ) -> desc_ total ) $ aggregate_ ( \\ ( user , lineItem , product ) -> ( group_ user , sum_ ( maybe_ 0 id ( _lineItemQuantity lineItem ) * maybe_ 0 id ( product ^. productPrice )))) $ do user <- all_ ( shoppingCartDb ^. shoppingCartUsers ) order <- leftJoin_ ( all_ ( shoppingCartDb ^. shoppingCartOrders )) ( \\ order -> _orderForUser order ` references_ ` user ) lineItem <- leftJoin_ ( all_ ( shoppingCartDb ^. shoppingCartLineItems )) ( \\ lineItem -> maybe_ ( val_ False ) ( \\ order -> _lineItemInOrder lineItem ` references_ ` order ) order ) product <- leftJoin_ ( all_ ( shoppingCartDb ^. shoppingCartProducts )) ( \\ product -> maybe_ ( val_ False ) ( \\ lineItem -> _lineItemForProduct lineItem ` references_ ` product ) lineItem ) pure ( user , lineItem , product ) mapM_ print allUsersAndTotals SELECT \"t0\" . \"email\" AS \"res0\" , \"t0\" . \"first_name\" AS \"res1\" , \"t0\" . \"last_name\" AS \"res2\" , \"t0\" . \"password\" AS \"res3\" , SUM (( CASE WHEN ( \"t2\" . \"item_quantity\" ) IS NOT NULL THEN \"t2\" . \"item_quantity\" ELSE ? END ) * ( CASE WHEN ( \"t3\" . \"price\" ) IS NOT NULL THEN \"t3\" . \"price\" ELSE ? END )) AS \"res4\" FROM \"cart_users\" AS \"t0\" LEFT JOIN \"orders\" AS \"t1\" ON ( \"t1\" . \"for_user__email\" ) = ( \"t0\" . \"email\" ) LEFT JOIN \"line_items\" AS \"t2\" ON CASE WHEN ((((( \"t1\" . \"id\" ) IS NOT NULL ) AND (( \"t1\" . \"date\" ) IS NOT NULL )) AND (( \"t1\" . \"for_user__email\" ) IS NOT NULL )) AND (( \"t1\" . \"ship_to_address__id\" ) IS NOT NULL )) AND (( \"t1\" . \"shipping_info__id\" ) IS NOT NULL ) THEN ( \"t2\" . \"item_in_order__id\" ) = ( \"t1\" . \"id\" ) ELSE ? END LEFT JOIN \"products\" AS \"t3\" ON CASE WHEN ((( \"t2\" . \"item_in_order__id\" ) IS NOT NULL ) AND (( \"t2\" . \"item_for_product__id\" ) IS NOT NULL )) AND (( \"t2\" . \"item_quantity\" ) IS NOT NULL ) THEN ( \"t2\" . \"item_for_product__id\" ) = ( \"t3\" . \"id\" ) ELSE ? END GROUP BY \"t0\" . \"email\" , \"t0\" . \"first_name\" , \"t0\" . \"last_name\" , \"t0\" . \"password\" ORDER BY SUM (( CASE WHEN ( \"t2\" . \"item_quantity\" ) IS NOT NULL THEN \"t2\" . \"item_quantity\" ELSE ? END ) * ( CASE WHEN ( \"t3\" . \"price\" ) IS NOT NULL THEN \"t3\" . \"price\" ELSE ? END )) DESC ; -- With values: [SQLInteger 0,SQLInteger 0,SQLInteger 0,SQLInteger 0,SQLInteger 0,SQLInteger 0] ( User { _userEmail = \"betty@example.com\" , _userFirstName = \"Betty\" , _userLastName = \"Jones\" , _userPassword = \"82b054bd83ffad9b6cf8bdb98ce3cc2f\" } , Just 16500 ) ( User { _userEmail = \"james@example.com\" , _userFirstName = \"James\" , _userLastName = \"Smith\" , _userPassword = \"b4cc344d25a2efe540adbf2678e2304c\" } , Just 0 ) ( User { _userEmail = \"sam@example.com\" , _userFirstName = \"Sam\" , _userLastName = \"Taylor\" , _userPassword = \"332532dcfaa1cbf61e2a266bd723612c\" } , Just 0 ) Take a couple seconds to examine the SQL generated by this query. Notice how every time we used maybe_ a CASE statement was emitted. While this provides a good match for Haskell semantics we are used to, it is also not always desireable in practice due to severe performance implications. Some RDBMSs, like Postgres, given such a query will be unable to utilize available indexes to perform join operations - this translates to extremely poor perfomance for even moderately sized data. Luckily, Beam also provides an alternate way to phrase things that directly maps to SQL semantics Haskell Sql Output allUsersAndTotals2 <- runBeamSqliteDebug putStrLn conn $ runSelectReturningList $ select $ orderBy_ ( \\ ( user , total ) -> desc_ total ) $ aggregate_ ( \\ ( user , lineItem , product ) -> ( group_ user , sum_ ( maybe_ 0 id ( _lineItemQuantity lineItem ) * maybe_ 0 id ( product ^. productPrice )))) $ do user <- all_ ( shoppingCartDb ^. shoppingCartUsers ) order <- leftJoin_ ( all_ ( shoppingCartDb ^. shoppingCartOrders )) ( \\ order -> _orderForUser order ` references_ ` user ) lineItem <- leftJoin_' ( all_ ( shoppingCartDb ^. shoppingCartLineItems )) ( \\ lineItem -> just_ ( _lineItemInOrder lineItem ) ==?. pk order ) product <- leftJoin_' ( all_ ( shoppingCartDb ^. shoppingCartProducts )) ( \\ product -> _lineItemForProduct lineItem ==?. just_ ( pk product )) pure ( user , lineItem , product ) mapM_ print allUsersAndTotals2 SELECT \"t0\" . \"email\" AS \"res0\" , \"t0\" . \"first_name\" AS \"res1\" , \"t0\" . \"last_name\" AS \"res2\" , \"t0\" . \"password\" AS \"res3\" , SUM (( CASE WHEN ( \"t2\" . \"item_quantity\" ) IS NOT NULL THEN \"t2\" . \"item_quantity\" ELSE ? END ) * ( CASE WHEN ( \"t3\" . \"price\" ) IS NOT NULL THEN \"t3\" . \"price\" ELSE ? END )) AS \"res4\" FROM \"cart_users\" AS \"t0\" LEFT JOIN \"orders\" AS \"t1\" ON ( \"t1\" . \"for_user__email\" ) = ( \"t0\" . \"email\" ) LEFT JOIN \"line_items\" AS \"t2\" ON ( \"t2\" . \"item_in_order__id\" ) = ( \"t1\" . \"id\" ) LEFT JOIN \"products\" AS \"t3\" ON ( \"t2\" . \"item_for_product__id\" ) = ( \"t3\" . \"id\" ) GROUP BY \"t0\" . \"email\" , \"t0\" . \"first_name\" , \"t0\" . \"last_name\" , \"t0\" . \"password\" ORDER BY SUM (( CASE WHEN ( \"t2\" . \"item_quantity\" ) IS NOT NULL THEN \"t2\" . \"item_quantity\" ELSE ? END ) * ( CASE WHEN ( \"t3\" . \"price\" ) IS NOT NULL THEN \"t3\" . \"price\" ELSE ? END )) DESC ; -- With values: [SQLInteger 0,SQLInteger 0,SQLInteger 0,SQLInteger 0] ( User { _userEmail = \"james@example.com\" , _userFirstName = \"James\" , _userLastName = \"Smith\" , _userPassword = \"b4cc344d25a2efe540adbf2678e2304c\" } , Just 27000 ) ( User { _userEmail = \"betty@example.com\" , _userFirstName = \"Betty\" , _userLastName = \"Jones\" , _userPassword = \"82b054bd83ffad9b6cf8bdb98ce3cc2f\" } , Just 16500 ) ( User { _userEmail = \"sam@example.com\" , _userFirstName = \"Sam\" , _userLastName = \"Taylor\" , _userPassword = \"332532dcfaa1cbf61e2a266bd723612c\" } , Just 0 ) Notice how we managed to eliminate maybe_ from the join conditions by using the SqlBool version of leftJoin_ , leftJoin_' together with just_ and the SqlBool version of the equality operator ==?. . Compare the generated SQL with the previous query. You can read more about how Beam handles NULL values in the Queries, Relationships section in the User Guide. Queries with nullable foreign keys Recall that our schema contains a nullable foreign key from OrderT to ShippingInfoT . Above, we've seen how leftJoin_ introduces nullable tables into our queries. Below, we'll see how to use nullable primary keys to optionally include information. Suppose we want to find all orders who have not been shipped. We can do this by simply writing a query over the orders. Haskell Sql Output allUnshippedOrders <- runBeamSqliteDebug putStrLn conn $ runSelectReturningList $ select $ filter_ ( isNothing_ . _orderShippingInfo ) $ all_ ( shoppingCartDb ^. shoppingCartOrders ) mapM_ print allUnshippedOrders SELECT \"t0\" . \"id\" AS \"res0\" , \"t0\" . \"date\" AS \"res1\" , \"t0\" . \"for_user__email\" AS \"res2\" , \"t0\" . \"ship_to_address__id\" AS \"res3\" , \"t0\" . \"shipping_info__id\" AS \"res4\" FROM \"orders\" AS \"t0\" WHERE ( \"t0\" . \"shipping_info__id\" ) IS NULL ; -- With values: [] Order { _orderId = 1 , _orderDate = 2022 - 01 - 18 01 : 29 : 29 , _orderForUser = UserId \"james@example.com\" , _orderShipToAddress = AddressId 1 , _orderShippingInfo = ShippingInfoId Nothing } Order { _orderId = 3 , _orderDate = 2022 - 01 - 18 01 : 29 : 29 , _orderForUser = UserId \"james@example.com\" , _orderShipToAddress = AddressId 1 , _orderShippingInfo = ShippingInfoId Nothing } Let's count up all shipped and unshipped orders by user, including users who have no orders. Haskell Sql Out shippingInformationByUser <- runBeamSqliteDebug putStrLn conn $ runSelectReturningList $ select $ aggregate_ ( \\ ( user , order ) -> let ShippingInfoId shippingInfoId = _orderShippingInfo order in ( group_ user , as_ @ Int32 $ count_ ( as_ @ ( Maybe Int32 ) ( maybe_ ( just_ 1 ) ( \\ _ -> nothing_ ) shippingInfoId )) , as_ @ Int32 $ count_ shippingInfoId ) ) $ do user <- all_ ( shoppingCartDb ^. shoppingCartUsers ) order <- leftJoin_ ( all_ ( shoppingCartDb ^. shoppingCartOrders )) ( \\ order -> _orderForUser order ` references_ ` user ) pure ( user , order ) mapM_ print shippingInformationByUser SELECT \"t0\" . \"email\" AS \"res0\" , \"t0\" . \"first_name\" AS \"res1\" , \"t0\" . \"last_name\" AS \"res2\" , \"t0\" . \"password\" AS \"res3\" , COUNT ( CASE WHEN ( \"t1\" . \"shipping_info__id\" ) IS NOT NULL THEN NULL ELSE ? END ) AS \"res4\" , COUNT ( \"t1\" . \"shipping_info__id\" ) AS \"res5\" FROM \"cart_users\" AS \"t0\" LEFT JOIN \"orders\" AS \"t1\" ON ( \"t1\" . \"for_user__email\" ) = ( \"t0\" . \"email\" ) GROUP BY \"t0\" . \"email\" , \"t0\" . \"first_name\" , \"t0\" . \"last_name\" , \"t0\" . \"password\" ; -- With values: [SQLInteger 1] ( User { _userEmail = \"betty@example.com\" , _userFirstName = \"Betty\" , _userLastName = \"Jones\" , _userPassword = \"82b054bd83ffad9b6cf8bdb98ce3cc2f\" } , 0 , 1 ) ( User { _userEmail = \"james@example.com\" , _userFirstName = \"James\" , _userLastName = \"Smith\" , _userPassword = \"b4cc344d25a2efe540adbf2678e2304c\" } , 2 , 0 ) ( User { _userEmail = \"sam@example.com\" , _userFirstName = \"Sam\" , _userLastName = \"Taylor\" , _userPassword = \"332532dcfaa1cbf61e2a266bd723612c\" } , 1 , 0 ) Uh-oh! There's an error in the result set! Sam is reported as having one unshipped order, instead of zero. Here we hit one of the limitations of beam's mapping to SQL, and really one of the limitations of SQL itself. Namely, the NULL in the result rows for Sam is not distinguished from the NULL in the shipping info key itself. Beam however does make the distinction. When beam deserializes a NULL in a Maybe field, the outermost Maybe is the one populated with Nothing . Thus it is impossible to retrieve a value like Just Nothing from the database using the default serializers and deserializers. In general, it's best to avoid highly nested Maybe s in your queries because it makes them more difficult to understand. One way to work around this issue in the above query is to use subselects. Haskell Sql Output shippingInformationByUser <- runBeamSqliteDebug putStrLn conn $ runSelectReturningList $ select $ do user <- all_ ( shoppingCartDb ^. shoppingCartUsers ) ( userEmail , unshippedCount ) <- aggregate_ ( \\ ( userEmail , order ) -> ( group_ userEmail , as_ @ Int32 countAll_ )) $ do user <- all_ ( shoppingCartDb ^. shoppingCartUsers ) order <- leftJoin_ ( all_ ( shoppingCartDb ^. shoppingCartOrders )) ( \\ order -> _orderForUser order ` references_ ` user &&. isNothing_ ( _orderShippingInfo order )) pure ( pk user , order ) guard_ ( userEmail ` references_ ` user ) ( userEmail , shippedCount ) <- aggregate_ ( \\ ( userEmail , order ) -> ( group_ userEmail , as_ @ Int32 countAll_ )) $ do user <- all_ ( shoppingCartDb ^. shoppingCartUsers ) order <- leftJoin_ ( all_ ( shoppingCartDb ^. shoppingCartOrders )) ( \\ order -> _orderForUser order ` references_ ` user &&. isJust_ ( _orderShippingInfo order )) pure ( pk user , order ) guard_ ( userEmail ` references_ ` user ) pure ( user , unshippedCount , shippedCount ) mapM_ print shippingInformationByUser SELECT \"t0\" . \"email\" AS \"res0\" , \"t0\" . \"first_name\" AS \"res1\" , \"t0\" . \"last_name\" AS \"res2\" , \"t0\" . \"password\" AS \"res3\" , \"t1\" . \"res1\" AS \"res4\" , \"t2\" . \"res1\" AS \"res5\" FROM \"cart_users\" AS \"t0\" INNER JOIN ( SELECT \"t0\" . \"email\" AS \"res0\" , COUNT ( * ) AS \"res1\" FROM \"cart_users\" AS \"t0\" LEFT JOIN \"orders\" AS \"t1\" ON (( \"t1\" . \"for_user__email\" ) = ( \"t0\" . \"email\" )) AND (( \"t1\" . \"shipping_info__id\" ) IS NULL ) GROUP BY \"t0\" . \"email\" ) AS \"t1\" INNER JOIN ( SELECT \"t0\" . \"email\" AS \"res0\" , COUNT ( * ) AS \"res1\" FROM \"cart_users\" AS \"t0\" LEFT JOIN \"orders\" AS \"t1\" ON (( \"t1\" . \"for_user__email\" ) = ( \"t0\" . \"email\" )) AND (( \"t1\" . \"shipping_info__id\" ) IS NOT NULL ) GROUP BY \"t0\" . \"email\" ) AS \"t2\" WHERE (( \"t1\" . \"res0\" ) = ( \"t0\" . \"email\" )) AND (( \"t2\" . \"res0\" ) = ( \"t0\" . \"email\" )); -- With values: [] ( User { _userEmail = \"betty@example.com\" , _userFirstName = \"Betty\" , _userLastName = \"Jones\" , _userPassword = \"82b054bd83ffad9b6cf8bdb98ce3cc2f\" } , 1 , 1 ) ( User { _userEmail = \"james@example.com\" , _userFirstName = \"James\" , _userLastName = \"Smith\" , _userPassword = \"b4cc344d25a2efe540adbf2678e2304c\" } , 2 , 1 ) ( User { _userEmail = \"sam@example.com\" , _userFirstName = \"Sam\" , _userLastName = \"Taylor\" , _userPassword = \"332532dcfaa1cbf61e2a266bd723612c\" } , 1 , 1 ) Notice that the aggregate_ s embedded in the Q monad were automatically converted into sub SELECT s. This is because beam queries are composable -- you can use them wherever they type check and sensible SQL will result. Of course, if you want more control, you can also use the subselect_ combinator to force generation of a sub SELECT . Haskell Sql Output shippingInformationByUser <- runBeamSqliteDebug putStrLn conn $ runSelectReturningList $ select $ do user <- all_ ( shoppingCartDb ^. shoppingCartUsers ) ( userEmail , unshippedCount ) <- subselect_ $ aggregate_ ( \\ ( userEmail , order ) -> ( group_ userEmail , as_ @ Int32 countAll_ )) $ do user <- all_ ( shoppingCartDb ^. shoppingCartUsers ) order <- leftJoin_ ( all_ ( shoppingCartDb ^. shoppingCartOrders )) ( \\ order -> _orderForUser order ` references_ ` user &&. isNothing_ ( _orderShippingInfo order )) pure ( pk user , order ) guard_ ( userEmail ` references_ ` user ) ( userEmail , shippedCount ) <- subselect_ $ aggregate_ ( \\ ( userEmail , order ) -> ( group_ userEmail , as_ @ Int32 countAll_ )) $ do user <- all_ ( shoppingCartDb ^. shoppingCartUsers ) order <- leftJoin_ ( all_ ( shoppingCartDb ^. shoppingCartOrders )) ( \\ order -> _orderForUser order ` references_ ` user &&. isJust_ ( _orderShippingInfo order )) pure ( pk user , order ) guard_ ( userEmail ` references_ ` user ) pure ( user , unshippedCount , shippedCount ) mapM_ print shippingInformationByUser SELECT \"t0\" . \"email\" AS \"res0\" , \"t0\" . \"first_name\" AS \"res1\" , \"t0\" . \"last_name\" AS \"res2\" , \"t0\" . \"password\" AS \"res3\" , \"t1\" . \"res1\" AS \"res4\" , \"t2\" . \"res1\" AS \"res5\" FROM \"cart_users\" AS \"t0\" INNER JOIN ( SELECT \"t0\" . \"res0\" AS \"res0\" , \"t0\" . \"res1\" AS \"res1\" FROM ( SELECT \"t0\" . \"email\" AS \"res0\" , COUNT ( * ) AS \"res1\" FROM \"cart_users\" AS \"t0\" LEFT JOIN \"orders\" AS \"t1\" ON (( \"t1\" . \"for_user__email\" ) = ( \"t0\" . \"email\" )) AND (( \"t1\" . \"shipping_info__id\" ) IS NULL ) GROUP BY \"t0\" . \"email\" ) AS \"t0\" ) AS \"t1\" INNER JOIN ( SELECT \"t0\" . \"res0\" AS \"res0\" , \"t0\" . \"res1\" AS \"res1\" FROM ( SELECT \"t0\" . \"email\" AS \"res0\" , COUNT ( * ) AS \"res1\" FROM \"cart_users\" AS \"t0\" LEFT JOIN \"orders\" AS \"t1\" ON (( \"t1\" . \"for_user__email\" ) = ( \"t0\" . \"email\" )) AND (( \"t1\" . \"shipping_info__id\" ) IS NOT NULL ) GROUP BY \"t0\" . \"email\" ) AS \"t0\" ) AS \"t2\" WHERE (( \"t1\" . \"res0\" ) = ( \"t0\" . \"email\" )) AND (( \"t2\" . \"res0\" ) = ( \"t0\" . \"email\" )); -- With values: [] ( User { _userEmail = \"betty@example.com\" , _userFirstName = \"Betty\" , _userLastName = \"Jones\" , _userPassword = \"82b054bd83ffad9b6cf8bdb98ce3cc2f\" } , 1 , 1 ) ( User { _userEmail = \"james@example.com\" , _userFirstName = \"James\" , _userLastName = \"Smith\" , _userPassword = \"b4cc344d25a2efe540adbf2678e2304c\" } , 2 , 1 ) ( User { _userEmail = \"sam@example.com\" , _userFirstName = \"Sam\" , _userLastName = \"Taylor\" , _userPassword = \"332532dcfaa1cbf61e2a266bd723612c\" } , 1 , 1 ) Conclusion This tutorial completes our sequence on creating a shopping cart. Throughout the tutorials, we saw how to create tables using regular Haskell data types, how to link those tables up using relations, how to query tables using both the monadic interface and the list-like functions on queries. We saw ha few examples of using beam to generate advanced queries. More information on the Beam API is havailable on hackage . Happy beaming! Beam is a work in progress. Please submit bugs and patches on GitHub .","title":"Part 3"},{"location":"tutorials/tutorial3/#introduction","text":"In the last part, we extended our shopping cart database to let users add multiple addresses. We saw how to establish one-to-many relations between two tables, and how to use the monadic query interface to write SQL JOINs. In this installment, we'll be adding support for products and orders to our database schema. We'll see how to use an intermediary table to create many-to-many relations and how to write LEFT JOINs. Finally, we'll see how to use Nullable to create optional foreign key references.","title":"Introduction"},{"location":"tutorials/tutorial3/#creating-tables-is-easy-now","text":"Let's create our products table. By now, the pattern for adding a new table to the schema should be pretty familiar, so I'm going to skip the explanation. data ProductT f = Product { _productId :: C f Int32 , _productTitle :: C f Text , _productDescription :: C f Text , _productPrice :: C f Int32 {- Price in cents -} } deriving ( Generic , Beamable ) type Product = ProductT Identity deriving instance Show Product instance Table ProductT where data PrimaryKey ProductT f = ProductId ( Columnar f Int32 ) deriving ( Generic , Beamable ) primaryKey = ProductId . _productId For orders, we want to store an id, date created, and the user who made the order. We'd also like to create an optional link to a shipping information table. When the shipping information is created, we'll fill in the shipping information in the order. In order to create the optional reference, we're going to use the Nullable tag modifier to modify the column tag. Nullable will turn all fields of type x into Maybe x . Note that we could also create this relation by installing a primary key on the shipping info table, and this is arguably the better option. However, we'll go with a nullable foreign key here to show the full breadth of beam's features, and because this sort of relation exists in many existing databases. import Data.Time deriving instance Show ( PrimaryKey AddressT Identity ) data OrderT f = Order { _orderId :: Columnar f Int32 , _orderDate :: Columnar f LocalTime , _orderForUser :: PrimaryKey UserT f , _orderShipToAddress :: PrimaryKey AddressT f , _orderShippingInfo :: PrimaryKey ShippingInfoT ( Nullable f ) } deriving ( Generic , Beamable ) type Order = OrderT Identity deriving instance Show Order instance Table OrderT where data PrimaryKey OrderT f = OrderId ( Columnar f Int32 ) deriving ( Generic , Beamable ) primaryKey = OrderId . _orderId data ShippingCarrier = USPS | FedEx | UPS | DHL deriving ( Show , Read , Eq , Ord , Enum ) data ShippingInfoT f = ShippingInfo { _shippingInfoId :: Columnar f Int32 , _shippingInfoCarrier :: Columnar f ShippingCarrier , _shippingInfoTrackingNumber :: Columnar f Text } deriving ( Generic , Beamable ) type ShippingInfo = ShippingInfoT Identity deriving instance Show ShippingInfo instance Table ShippingInfoT where data PrimaryKey ShippingInfoT f = ShippingInfoId ( Columnar f Int32 ) deriving ( Generic , Beamable ) primaryKey = ShippingInfoId . _shippingInfoId deriving instance Show ( PrimaryKey ShippingInfoT ( Nullable Identity )) In the above example, we show how to use a custom data type as a beam column. Recall that beam lets you store any Haskell type in a Columnar . However, at some point, we will need to demonstrate to SQLite how to store values of type ShippingCarrier . We will come back to this later. We would also like to be able to associate a list of products with each order as line items. To do this we will create a table with two foreign keys. This table will establish a many-to-many relationship between orders and products. deriving instance Show ( PrimaryKey OrderT Identity ) deriving instance Show ( PrimaryKey ProductT Identity ) data LineItemT f = LineItem { _lineItemInOrder :: PrimaryKey OrderT f , _lineItemForProduct :: PrimaryKey ProductT f , _lineItemQuantity :: Columnar f Int32 } deriving ( Generic , Beamable ) type LineItem = LineItemT Identity deriving instance Show LineItem instance Table LineItemT where data PrimaryKey LineItemT f = LineItemId ( PrimaryKey OrderT f ) ( PrimaryKey ProductT f ) deriving ( Generic , Beamable ) primaryKey = LineItemId <$> _lineItemInOrder <*> _lineItemForProduct Tip We used the Applicative instance for (->) a above to write the primaryKey function. The Applicative ((->) a) instance operates like an unwrapper Reader of a . The applicative actions are then functions from a -> x that inject values from the a into the applicative bind. Now we'll add all these tables to our database. -- Some convenience lenses LineItem _ _ ( LensFor lineItemQuantity ) = tableLenses Product ( LensFor productId ) ( LensFor productTitle ) ( LensFor productDescription ) ( LensFor productPrice ) = tableLenses data ShoppingCartDb f = ShoppingCartDb { _shoppingCartUsers :: f ( TableEntity UserT ) , _shoppingCartUserAddresses :: f ( TableEntity AddressT ) , _shoppingCartProducts :: f ( TableEntity ProductT ) , _shoppingCartOrders :: f ( TableEntity OrderT ) , _shoppingCartShippingInfos :: f ( TableEntity ShippingInfoT ) , _shoppingCartLineItems :: f ( TableEntity LineItemT ) } deriving ( Generic , Database be ) ShoppingCartDb ( TableLens shoppingCartUsers ) ( TableLens shoppingCartUserAddresses ) ( TableLens shoppingCartProducts ) ( TableLens shoppingCartOrders ) ( TableLens shoppingCartShippingInfos ) ( TableLens shoppingCartLineItems ) = dbLenses shoppingCartDb :: DatabaseSettings be ShoppingCartDb shoppingCartDb = defaultDbSettings ` withDbModification ` dbModification { _shoppingCartUserAddresses = setEntityName \"addresses\" <> modifyTableFields tableModification { _addressLine1 = \"address1\" , _addressLine2 = \"address2\" }, _shoppingCartProducts = setEntityName \"products\" , _shoppingCartOrders = setEntityName \"orders\" <> modifyTableFields tableModification { _orderShippingInfo = ShippingInfoId \"shipping_info__id\" }, _shoppingCartShippingInfos = setEntityName \"shipping_info\" <> modifyTableFields tableModification { _shippingInfoId = \"id\" , _shippingInfoCarrier = \"carrier\" , _shippingInfoTrackingNumber = \"tracking_number\" }, _shoppingCartLineItems = setEntityName \"line_items\" }","title":"Creating tables is easy now"},{"location":"tutorials/tutorial3/#fixtures","text":"Let's put some sample data into a new database. conn <- open \"shoppingcart3.db\" execute_ conn \"CREATE TABLE cart_users (email VARCHAR NOT NULL, first_name VARCHAR NOT NULL, last_name VARCHAR NOT NULL, password VARCHAR NOT NULL, PRIMARY KEY( email ));\" execute_ conn \"CREATE TABLE addresses ( id INTEGER PRIMARY KEY AUTOINCREMENT, address1 VARCHAR NOT NULL, address2 VARCHAR, city VARCHAR NOT NULL, state VARCHAR NOT NULL, zip VARCHAR NOT NULL, for_user__email VARCHAR NOT NULL );\" execute_ conn \"CREATE TABLE products ( id INTEGER PRIMARY KEY AUTOINCREMENT, title VARCHAR NOT NULL, description VARCHAR NOT NULL, price INT NOT NULL );\" execute_ conn \"CREATE TABLE orders ( id INTEGER PRIMARY KEY AUTOINCREMENT, date TIMESTAMP NOT NULL, for_user__email VARCHAR NOT NULL, ship_to_address__id INT NOT NULL, shipping_info__id INT);\" execute_ conn \"CREATE TABLE shipping_info ( id INTEGER PRIMARY KEY AUTOINCREMENT, carrier VARCHAR NOT NULL, tracking_number VARCHAR NOT NULL);\" execute_ conn \"CREATE TABLE line_items (item_in_order__id INTEGER NOT NULL, item_for_product__id INTEGER NOT NULL, item_quantity INTEGER NOT NULL)\" Let's put some sample data into our database. Below, we will use the beam-sqlite functions insertReturning and runInsertReturningList to insert rows and retrieve the inserted rows from the database. This will let us see what values the auto-incremented id columns took on, which will allow us to create references to these inserted rows. let users @ [ james , betty , sam ] = [ User \"james@example.com\" \"James\" \"Smith\" \"b4cc344d25a2efe540adbf2678e2304c\" {- james -} , User \"betty@example.com\" \"Betty\" \"Jones\" \"82b054bd83ffad9b6cf8bdb98ce3cc2f\" {- betty -} , User \"sam@example.com\" \"Sam\" \"Taylor\" \"332532dcfaa1cbf61e2a266bd723612c\" {- sam -} ] addresses = [ Address default_ ( val_ \"123 Little Street\" ) ( val_ Nothing ) ( val_ \"Boston\" ) ( val_ \"MA\" ) ( val_ \"12345\" ) ( pk james ) , Address default_ ( val_ \"222 Main Street\" ) ( val_ ( Just \"Ste 1\" )) ( val_ \"Houston\" ) ( val_ \"TX\" ) ( val_ \"8888\" ) ( pk betty ) , Address default_ ( val_ \"9999 Residence Ave\" ) ( val_ Nothing ) ( val_ \"Sugarland\" ) ( val_ \"TX\" ) ( val_ \"8989\" ) ( pk betty ) ] products = [ Product default_ ( val_ \"Red Ball\" ) ( val_ \"A bright red, very spherical ball\" ) ( val_ 1000 ) , Product default_ ( val_ \"Math Textbook\" ) ( val_ \"Contains a lot of important math theorems and formulae\" ) ( val_ 2500 ) , Product default_ ( val_ \"Intro to Haskell\" ) ( val_ \"Learn the best programming language in the world\" ) ( val_ 3000 ) , Product default_ ( val_ \"Suitcase\" ) \"A hard durable suitcase\" 15000 ] ( jamesAddress1 , bettyAddress1 , bettyAddress2 , redBall , mathTextbook , introToHaskell , suitcase ) <- runBeamSqliteDebug putStrLn conn $ do runInsert $ insert ( shoppingCartDb ^. shoppingCartUsers ) $ insertValues users [ jamesAddress1 , bettyAddress1 , bettyAddress2 ] <- runInsertReturningList $ insertReturning ( shoppingCartDb ^. shoppingCartUserAddresses ) $ insertExpressions addresses [ redBall , mathTextbook , introToHaskell , suitcase ] <- runInsertReturningList $ insertReturning ( shoppingCartDb ^. shoppingCartProducts ) $ insertExpressions products pure ( jamesAddress1 , bettyAddress1 , bettyAddress2 , redBall , mathTextbook , introToHaskell , suitcase ) Now, if we take a look at one of the returned addresses, like jamesAddress1 , we see it has had the default_ values assigned correctly. Prelude Database . Beam Database . Beam . Sqlite Data . Time Database . SQLite . Simple Data . Text Lens . Micro > jamesAddress1 Address { _addressId = 1 , _addressLine1 = \"123 Little Street\" , _addressLine2 = Nothing , _addressCity = \"Boston\" , _addressState = \"MA\" , _addressZip = \"12345\" , _addressForUser = UserId \"james@example.com\" }","title":"Fixtures"},{"location":"tutorials/tutorial3/#marshalling-a-custom-type","text":"Now we can insert shipping information. Of course, the shipping information contains the ShippingCarrier enumeration. bettyShippingInfo <- runBeamSqliteDebug putStrLn conn $ do [ bettyShippingInfo ] <- runInsertReturningList $ insertReturning ( shoppingCartDb ^. shoppingCartShippingInfos ) $ insertExpressions [ ShippingInfo default_ ( val_ USPS ) ( val_ \"12345790ABCDEFGHI\" ) ] pure bettyShippingInfo If you run this, you'll get an error from GHCi. < interactive > : 845 : 7 : error : \u2022 No instance for ( FromBackendRow Sqlite ShippingCarrier ) arising from a use of \u2018 runInsertReturningList \u2019 \u2022 In a stmt of a 'do' block : [ bettyShippingInfo ] <- runInsertReturningList $ insertReturning ( shoppingCartDb ^ . shoppingCartShippingInfos ) $ insertExpressions [ ShippingInfo default_ (val_ USPS) (val_ \"12345790ABCDEFGHI\") ] ... < interactive > : 847 : 50 : error : \u2022 No instance for ( Database . Beam . Backend . SQL . SQL92 . HasSqlValueSyntax Database . Beam . Sqlite . Syntax . SqliteValueSyntax ShippingCarrier ) These errors are because there's no way to express a ShippingCarrier in the backend syntax. We can fix this by writing instances for beam. We can re-use the functionality we already have for String . The HasSqlValueSyntax class tells us how to convert a Haskell value into a corresponding backend value. import Database.Beam.Backend.SQL : set - XUndecidableInstances instance HasSqlValueSyntax be String => HasSqlValueSyntax be ShippingCarrier where sqlValueSyntax = autoSqlValueSyntax autoSqlValueSyntax uses the underlying Show instance to serialize a type to a string representation. The FromBackendRow class tells us how to convert a value from the database into a corresponding Haskell value. import qualified Data.Text as T -- for unpack instance FromBackendRow Sqlite ShippingCarrier where fromBackendRow = read . T . unpack <$> fromBackendRow Since, autoSqlValueSyntax uses the Show instance, we can simply use the Read instance. Now, if we try to insert the shipping info again, it works. bettyShippingInfo <- runBeamSqliteDebug putStrLn conn $ do [ bettyShippingInfo ] <- runInsertReturningList $ insertReturning ( shoppingCartDb ^. shoppingCartShippingInfos ) $ insertExpressions [ ShippingInfo default_ ( val_ USPS ) ( val_ \"12345790ABCDEFGHI\" ) ] pure bettyShippingInfo And if we look at the value of bettyShippingInfo , ShippingCarrier has been stored correctly. > bettyShippingInfo ShippingInfo { _shippingInfoId = 1 , _shippingInfoCarrier = USPS , _shippingInfoTrackingNumber = \"12345790ABCDEFGHI\" } Now, let's insert some orders that just came in. We want to insert transactions with the current database timestamp (i.e., CURRENT_TIMESTAMP in SQL). We can do this using insertExpressions . If you run the example below, you'll see the resulting rows have a timestamp set by the database. Haskell Sql Console [ jamesOrder1 , bettyOrder1 , jamesOrder2 ] <- runBeamSqliteDebug putStrLn conn $ do runInsertReturningList $ insertReturning ( shoppingCartDb ^. shoppingCartOrders ) $ insertExpressions $ [ Order default_ currentTimestamp_ ( val_ ( pk james )) ( val_ ( pk jamesAddress1 )) nothing_ , Order default_ currentTimestamp_ ( val_ ( pk betty )) ( val_ ( pk bettyAddress1 )) ( just_ ( val_ ( pk bettyShippingInfo ))) , Order default_ currentTimestamp_ ( val_ ( pk james )) ( val_ ( pk jamesAddress1 )) nothing_ ] print jamesOrder1 print bettyOrder1 print jamesOrder2 INSERT INTO \"orders\" ( \"date\" , \"for_user__email\" , \"ship_to_address__id\" , \"shipping_info__id\" ) VALUES ( CURRENT_TIMESTAMP , ? , ? , NULL ); -- With values: [SQLText \"james@example.com\",SQLInteger 1] INSERT INTO \"orders\" ( \"date\" , \"for_user__email\" , \"ship_to_address__id\" , \"shipping_info__id\" ) VALUES ( CURRENT_TIMESTAMP , ? , ? , ? ); -- With values: [SQLText \"betty@example.com\",SQLInteger 2,SQLInteger 1] INSERT INTO \"orders\" ( \"date\" , \"for_user__email\" , \"ship_to_address__id\" , \"shipping_info__id\" ) VALUES ( CURRENT_TIMESTAMP , ? , ? , NULL ); -- With values: [SQLText \"james@example.com\",SQLInteger 1] Order {_orderId = 1, _orderDate = 2022-01-18 01:28:55, _orderForUser = UserId \"james@example.com\", _orderShipToAddress = AddressId 1, _orderShippingInfo = ShippingInfoId Nothing} Order {_orderId = 2, _orderDate = 2022-01-18 01:28:55, _orderForUser = UserId \"betty@example.com\", _orderShipToAddress = AddressId 2, _orderShippingInfo = ShippingInfoId (Just 1)} Order {_orderId = 3, _orderDate = 2022-01-18 01:28:55, _orderForUser = UserId \"james@example.com\", _orderShipToAddress = AddressId 1, _orderShippingInfo = ShippingInfoId Nothing} Finally, let's add some line items Haskell Sql let lineItems = [ LineItem ( pk jamesOrder1 ) ( pk redBall ) 10 , LineItem ( pk jamesOrder1 ) ( pk mathTextbook ) 1 , LineItem ( pk jamesOrder1 ) ( pk introToHaskell ) 4 , LineItem ( pk bettyOrder1 ) ( pk mathTextbook ) 3 , LineItem ( pk bettyOrder1 ) ( pk introToHaskell ) 3 , LineItem ( pk jamesOrder2 ) ( pk mathTextbook ) 1 ] runBeamSqliteDebug putStrLn conn $ do runInsert $ insert ( shoppingCartDb ^. shoppingCartLineItems ) $ insertValues lineItems INSERT INTO \"line_items\" ( \"item_in_order__id\" , \"item_for_product__id\" , \"item_quantity\" ) VALUES ( ? , ? , ? ), ( ? , ? , ? ), ( ? , ? , ? ), ( ? , ? , ? ), ( ? , ? , ? ), ( ? , ? , ? ); -- With values: [SQLInteger 1,SQLInteger 1,SQLInteger 10,SQLInteger 1,SQLInteger 2,SQLInteger 1,SQLInteger 1,SQLInteger 3,SQLInteger 4,SQLInteger 2,SQLInteger 2,SQLInteger 3,SQLInteger 2,SQLInteger 3,SQLInteger 3,SQLInteger 3,SQLInteger 2,SQLInteger 1] Phew! Let's write some queries on this data!","title":"Marshalling a custom type"},{"location":"tutorials/tutorial3/#would-you-like-some-left-joins-with-that","text":"Suppose we want to do some analytics on our users, and so we want to know how many orders each user has made in our system. We can write a query to list every user along with the orders they've made. We can use leftJoin_ to include all users in our result set, even those who have no orders. Haskell Sql Output usersAndOrders <- runBeamSqliteDebug putStrLn conn $ runSelectReturningList $ select $ do user <- all_ ( shoppingCartDb ^. shoppingCartUsers ) order <- leftJoin_ ( all_ ( shoppingCartDb ^. shoppingCartOrders )) ( \\ order -> _orderForUser order ` references_ ` user ) pure ( user , order ) mapM_ print usersAndOrders SELECT \"t0\" . \"email\" AS \"res0\" , \"t0\" . \"first_name\" AS \"res1\" , \"t0\" . \"last_name\" AS \"res2\" , \"t0\" . \"password\" AS \"res3\" , \"t1\" . \"id\" AS \"res4\" , \"t1\" . \"date\" AS \"res5\" , \"t1\" . \"for_user__email\" AS \"res6\" , \"t1\" . \"ship_to_address__id\" AS \"res7\" , \"t1\" . \"shipping_info__id\" AS \"res8\" FROM \"cart_users\" AS \"t0\" LEFT JOIN \"orders\" AS \"t1\" ON ( \"t1\" . \"for_user__email\" ) = ( \"t0\" . \"email\" ); -- With values: [] ( User { _userEmail = \"james@example.com\" , _userFirstName = \"James\" , _userLastName = \"Smith\" , _userPassword = \"b4cc344d25a2efe540adbf2678e2304c\" } , Just ( Order { _orderId = 1 , _orderDate = 2022 - 01 - 18 01 : 29 : 02 , _orderForUser = UserId \"james@example.com\" , _orderShipToAddress = AddressId 1 , _orderShippingInfo = ShippingInfoId Nothing } )) ( User { _userEmail = \"james@example.com\" , _userFirstName = \"James\" , _userLastName = \"Smith\" , _userPassword = \"b4cc344d25a2efe540adbf2678e2304c\" } , Just ( Order { _orderId = 3 , _orderDate = 2022 - 01 - 18 01 : 29 : 02 , _orderForUser = UserId \"james@example.com\" , _orderShipToAddress = AddressId 1 , _orderShippingInfo = ShippingInfoId Nothing } )) ( User { _userEmail = \"betty@example.com\" , _userFirstName = \"Betty\" , _userLastName = \"Jones\" , _userPassword = \"82b054bd83ffad9b6cf8bdb98ce3cc2f\" } , Just ( Order { _orderId = 2 , _orderDate = 2022 - 01 - 18 01 : 29 : 02 , _orderForUser = UserId \"betty@example.com\" , _orderShipToAddress = AddressId 2 , _orderShippingInfo = ShippingInfoId ( Just 1 ) } )) ( User { _userEmail = \"sam@example.com\" , _userFirstName = \"Sam\" , _userLastName = \"Taylor\" , _userPassword = \"332532dcfaa1cbf61e2a266bd723612c\" } , Nothing ) Notice that Sam is included in the result set, even though he doesn't have any associated orders. Instead of a Just (Order ..) , Nothing is returned instead. Next, perhaps our marketing team wanted to send e-mails out to all users with no orders. We can use isNothing_ or isJust_ to determine the status if a nullable table or QExpr s (Maybe x) . The following query uses isNothing_ to find users who have no associated orders. Haskell Sql Output usersWithNoOrders <- runBeamSqliteDebug putStrLn conn $ runSelectReturningList $ select $ do user <- all_ ( shoppingCartDb ^. shoppingCartUsers ) order <- leftJoin_ ( all_ ( shoppingCartDb ^. shoppingCartOrders )) ( \\ order -> _orderForUser order ` references_ ` user ) guard_ ( isNothing_ order ) pure user mapM_ print usersWithNoOrders SELECT \"t0\" . \"email\" AS \"res0\" , \"t0\" . \"first_name\" AS \"res1\" , \"t0\" . \"last_name\" AS \"res2\" , \"t0\" . \"password\" AS \"res3\" FROM \"cart_users\" AS \"t0\" LEFT JOIN \"orders\" AS \"t1\" ON ( \"t1\" . \"for_user__email\" ) = ( \"t0\" . \"email\" ) WHERE ((((( \"t1\" . \"id\" ) IS NULL ) AND (( \"t1\" . \"date\" ) IS NULL )) AND (( \"t1\" . \"for_user__email\" ) IS NULL )) AND (( \"t1\" . \"ship_to_address__id\" ) IS NULL )) AND (( \"t1\" . \"shipping_info__id\" ) IS NULL ); -- With values: [] User { _userEmail = \"sam@example.com\" , _userFirstName = \"Sam\" , _userLastName = \"Taylor\" , _userPassword = \"332532dcfaa1cbf61e2a266bd723612c\" } We see that beam generates a sensible SQL SELECT and WHERE clause. We can also use the exists_ combinator to utilize the SQL EXISTS clause. Haskell Sql Output usersWithNoOrders <- runBeamSqliteDebug putStrLn conn $ runSelectReturningList $ select $ do user <- all_ ( shoppingCartDb ^. shoppingCartUsers ) guard_ ( not_ ( exists_ ( filter_ ( \\ order -> _orderForUser order ` references_ ` user ) ( all_ ( shoppingCartDb ^. shoppingCartOrders ))))) pure user mapM_ print usersWithNoOrders SELECT \"t0\" . \"email\" AS \"res0\" , \"t0\" . \"first_name\" AS \"res1\" , \"t0\" . \"last_name\" AS \"res2\" , \"t0\" . \"password\" AS \"res3\" FROM \"cart_users\" AS \"t0\" WHERE NOT ( EXISTS ( SELECT \"sub_t0\" . \"id\" AS \"res0\" , \"sub_t0\" . \"date\" AS \"res1\" , \"sub_t0\" . \"for_user__email\" AS \"res2\" , \"sub_t0\" . \"ship_to_address__id\" AS \"res3\" , \"sub_t0\" . \"shipping_info__id\" AS \"res4\" FROM \"orders\" AS \"sub_t0\" WHERE ( \"sub_t0\" . \"for_user__email\" ) = ( \"t0\" . \"email\" ))); -- With values: [] User { _userEmail = \"sam@example.com\" , _userFirstName = \"Sam\" , _userLastName = \"Taylor\" , _userPassword = \"332532dcfaa1cbf61e2a266bd723612c\" } Now suppose we wanted to do some analysis on the orders themselves. To start, we want to get the orders sorted by their portion of revenue. We can use aggregate_ to list every order and the total amount of all products in that order. Haskell Sql Output ordersWithCostOrdered <- runBeamSqliteDebug putStrLn conn $ runSelectReturningList $ select $ orderBy_ ( \\ ( order , total ) -> desc_ total ) $ aggregate_ ( \\ ( order , lineItem , product ) -> ( group_ order , sum_ ( lineItem ^. lineItemQuantity * product ^. productPrice ))) $ do lineItem <- all_ ( shoppingCartDb ^. shoppingCartLineItems ) order <- related_ ( shoppingCartDb ^. shoppingCartOrders ) ( _lineItemInOrder lineItem ) product <- related_ ( shoppingCartDb ^. shoppingCartProducts ) ( _lineItemForProduct lineItem ) pure ( order , lineItem , product ) mapM_ print ordersWithCostOrdered SELECT \"t1\" . \"id\" AS \"res0\" , \"t1\" . \"date\" AS \"res1\" , \"t1\" . \"for_user__email\" AS \"res2\" , \"t1\" . \"ship_to_address__id\" AS \"res3\" , \"t1\" . \"shipping_info__id\" AS \"res4\" , SUM (( \"t0\" . \"item_quantity\" ) * ( \"t2\" . \"price\" )) AS \"res5\" FROM \"line_items\" AS \"t0\" INNER JOIN \"orders\" AS \"t1\" ON ( \"t0\" . \"item_in_order__id\" ) = ( \"t1\" . \"id\" ) INNER JOIN \"products\" AS \"t2\" ON ( \"t0\" . \"item_for_product__id\" ) = ( \"t2\" . \"id\" ) GROUP BY \"t1\" . \"id\" , \"t1\" . \"date\" , \"t1\" . \"for_user__email\" , \"t1\" . \"ship_to_address__id\" , \"t1\" . \"shipping_info__id\" ORDER BY SUM (( \"t0\" . \"item_quantity\" ) * ( \"t2\" . \"price\" )) DESC ; -- With values: [] ( Order { _orderId = 1 , _orderDate = 2022 - 01 - 18 01 : 29 : 15 , _orderForUser = UserId \"james@example.com\" , _orderShipToAddress = AddressId 1 , _orderShippingInfo = ShippingInfoId Nothing } , Just 24500 ) ( Order { _orderId = 2 , _orderDate = 2022 - 01 - 18 01 : 29 : 15 , _orderForUser = UserId \"betty@example.com\" , _orderShipToAddress = AddressId 2 , _orderShippingInfo = ShippingInfoId ( Just 1 ) } , Just 16500 ) ( Order { _orderId = 3 , _orderDate = 2022 - 01 - 18 01 : 29 : 15 , _orderForUser = UserId \"james@example.com\" , _orderShipToAddress = AddressId 1 , _orderShippingInfo = ShippingInfoId Nothing } , Just 2500 ) We can also get the total amount spent by each user, even including users with no orders. Notice that we have to use maybe_ below in order to handle the fact that some tables have been introduced into our query with a left join. maybe_ is to QExpr what maybe is to normal Haskell values. maybe_ is polymorphic to either QExpr s or full on tables of QExpr s. For our purposes, the type of maybe_ is maybe_ :: QExpr be s a -> ( QExpr be s b -> QExpr be s a ) -> QExpr be s ( Maybe b ) -> QExpr be s a With that in mind, we can write the query to get the total spent by user Haskell Sql Output allUsersAndTotals <- runBeamSqliteDebug putStrLn conn $ runSelectReturningList $ select $ orderBy_ ( \\ ( user , total ) -> desc_ total ) $ aggregate_ ( \\ ( user , lineItem , product ) -> ( group_ user , sum_ ( maybe_ 0 id ( _lineItemQuantity lineItem ) * maybe_ 0 id ( product ^. productPrice )))) $ do user <- all_ ( shoppingCartDb ^. shoppingCartUsers ) order <- leftJoin_ ( all_ ( shoppingCartDb ^. shoppingCartOrders )) ( \\ order -> _orderForUser order ` references_ ` user ) lineItem <- leftJoin_ ( all_ ( shoppingCartDb ^. shoppingCartLineItems )) ( \\ lineItem -> maybe_ ( val_ False ) ( \\ order -> _lineItemInOrder lineItem ` references_ ` order ) order ) product <- leftJoin_ ( all_ ( shoppingCartDb ^. shoppingCartProducts )) ( \\ product -> maybe_ ( val_ False ) ( \\ lineItem -> _lineItemForProduct lineItem ` references_ ` product ) lineItem ) pure ( user , lineItem , product ) mapM_ print allUsersAndTotals SELECT \"t0\" . \"email\" AS \"res0\" , \"t0\" . \"first_name\" AS \"res1\" , \"t0\" . \"last_name\" AS \"res2\" , \"t0\" . \"password\" AS \"res3\" , SUM (( CASE WHEN ( \"t2\" . \"item_quantity\" ) IS NOT NULL THEN \"t2\" . \"item_quantity\" ELSE ? END ) * ( CASE WHEN ( \"t3\" . \"price\" ) IS NOT NULL THEN \"t3\" . \"price\" ELSE ? END )) AS \"res4\" FROM \"cart_users\" AS \"t0\" LEFT JOIN \"orders\" AS \"t1\" ON ( \"t1\" . \"for_user__email\" ) = ( \"t0\" . \"email\" ) LEFT JOIN \"line_items\" AS \"t2\" ON CASE WHEN ((((( \"t1\" . \"id\" ) IS NOT NULL ) AND (( \"t1\" . \"date\" ) IS NOT NULL )) AND (( \"t1\" . \"for_user__email\" ) IS NOT NULL )) AND (( \"t1\" . \"ship_to_address__id\" ) IS NOT NULL )) AND (( \"t1\" . \"shipping_info__id\" ) IS NOT NULL ) THEN ( \"t2\" . \"item_in_order__id\" ) = ( \"t1\" . \"id\" ) ELSE ? END LEFT JOIN \"products\" AS \"t3\" ON CASE WHEN ((( \"t2\" . \"item_in_order__id\" ) IS NOT NULL ) AND (( \"t2\" . \"item_for_product__id\" ) IS NOT NULL )) AND (( \"t2\" . \"item_quantity\" ) IS NOT NULL ) THEN ( \"t2\" . \"item_for_product__id\" ) = ( \"t3\" . \"id\" ) ELSE ? END GROUP BY \"t0\" . \"email\" , \"t0\" . \"first_name\" , \"t0\" . \"last_name\" , \"t0\" . \"password\" ORDER BY SUM (( CASE WHEN ( \"t2\" . \"item_quantity\" ) IS NOT NULL THEN \"t2\" . \"item_quantity\" ELSE ? END ) * ( CASE WHEN ( \"t3\" . \"price\" ) IS NOT NULL THEN \"t3\" . \"price\" ELSE ? END )) DESC ; -- With values: [SQLInteger 0,SQLInteger 0,SQLInteger 0,SQLInteger 0,SQLInteger 0,SQLInteger 0] ( User { _userEmail = \"betty@example.com\" , _userFirstName = \"Betty\" , _userLastName = \"Jones\" , _userPassword = \"82b054bd83ffad9b6cf8bdb98ce3cc2f\" } , Just 16500 ) ( User { _userEmail = \"james@example.com\" , _userFirstName = \"James\" , _userLastName = \"Smith\" , _userPassword = \"b4cc344d25a2efe540adbf2678e2304c\" } , Just 0 ) ( User { _userEmail = \"sam@example.com\" , _userFirstName = \"Sam\" , _userLastName = \"Taylor\" , _userPassword = \"332532dcfaa1cbf61e2a266bd723612c\" } , Just 0 ) Take a couple seconds to examine the SQL generated by this query. Notice how every time we used maybe_ a CASE statement was emitted. While this provides a good match for Haskell semantics we are used to, it is also not always desireable in practice due to severe performance implications. Some RDBMSs, like Postgres, given such a query will be unable to utilize available indexes to perform join operations - this translates to extremely poor perfomance for even moderately sized data. Luckily, Beam also provides an alternate way to phrase things that directly maps to SQL semantics Haskell Sql Output allUsersAndTotals2 <- runBeamSqliteDebug putStrLn conn $ runSelectReturningList $ select $ orderBy_ ( \\ ( user , total ) -> desc_ total ) $ aggregate_ ( \\ ( user , lineItem , product ) -> ( group_ user , sum_ ( maybe_ 0 id ( _lineItemQuantity lineItem ) * maybe_ 0 id ( product ^. productPrice )))) $ do user <- all_ ( shoppingCartDb ^. shoppingCartUsers ) order <- leftJoin_ ( all_ ( shoppingCartDb ^. shoppingCartOrders )) ( \\ order -> _orderForUser order ` references_ ` user ) lineItem <- leftJoin_' ( all_ ( shoppingCartDb ^. shoppingCartLineItems )) ( \\ lineItem -> just_ ( _lineItemInOrder lineItem ) ==?. pk order ) product <- leftJoin_' ( all_ ( shoppingCartDb ^. shoppingCartProducts )) ( \\ product -> _lineItemForProduct lineItem ==?. just_ ( pk product )) pure ( user , lineItem , product ) mapM_ print allUsersAndTotals2 SELECT \"t0\" . \"email\" AS \"res0\" , \"t0\" . \"first_name\" AS \"res1\" , \"t0\" . \"last_name\" AS \"res2\" , \"t0\" . \"password\" AS \"res3\" , SUM (( CASE WHEN ( \"t2\" . \"item_quantity\" ) IS NOT NULL THEN \"t2\" . \"item_quantity\" ELSE ? END ) * ( CASE WHEN ( \"t3\" . \"price\" ) IS NOT NULL THEN \"t3\" . \"price\" ELSE ? END )) AS \"res4\" FROM \"cart_users\" AS \"t0\" LEFT JOIN \"orders\" AS \"t1\" ON ( \"t1\" . \"for_user__email\" ) = ( \"t0\" . \"email\" ) LEFT JOIN \"line_items\" AS \"t2\" ON ( \"t2\" . \"item_in_order__id\" ) = ( \"t1\" . \"id\" ) LEFT JOIN \"products\" AS \"t3\" ON ( \"t2\" . \"item_for_product__id\" ) = ( \"t3\" . \"id\" ) GROUP BY \"t0\" . \"email\" , \"t0\" . \"first_name\" , \"t0\" . \"last_name\" , \"t0\" . \"password\" ORDER BY SUM (( CASE WHEN ( \"t2\" . \"item_quantity\" ) IS NOT NULL THEN \"t2\" . \"item_quantity\" ELSE ? END ) * ( CASE WHEN ( \"t3\" . \"price\" ) IS NOT NULL THEN \"t3\" . \"price\" ELSE ? END )) DESC ; -- With values: [SQLInteger 0,SQLInteger 0,SQLInteger 0,SQLInteger 0] ( User { _userEmail = \"james@example.com\" , _userFirstName = \"James\" , _userLastName = \"Smith\" , _userPassword = \"b4cc344d25a2efe540adbf2678e2304c\" } , Just 27000 ) ( User { _userEmail = \"betty@example.com\" , _userFirstName = \"Betty\" , _userLastName = \"Jones\" , _userPassword = \"82b054bd83ffad9b6cf8bdb98ce3cc2f\" } , Just 16500 ) ( User { _userEmail = \"sam@example.com\" , _userFirstName = \"Sam\" , _userLastName = \"Taylor\" , _userPassword = \"332532dcfaa1cbf61e2a266bd723612c\" } , Just 0 ) Notice how we managed to eliminate maybe_ from the join conditions by using the SqlBool version of leftJoin_ , leftJoin_' together with just_ and the SqlBool version of the equality operator ==?. . Compare the generated SQL with the previous query. You can read more about how Beam handles NULL values in the Queries, Relationships section in the User Guide.","title":"Would you like some left joins with that?"},{"location":"tutorials/tutorial3/#queries-with-nullable-foreign-keys","text":"Recall that our schema contains a nullable foreign key from OrderT to ShippingInfoT . Above, we've seen how leftJoin_ introduces nullable tables into our queries. Below, we'll see how to use nullable primary keys to optionally include information. Suppose we want to find all orders who have not been shipped. We can do this by simply writing a query over the orders. Haskell Sql Output allUnshippedOrders <- runBeamSqliteDebug putStrLn conn $ runSelectReturningList $ select $ filter_ ( isNothing_ . _orderShippingInfo ) $ all_ ( shoppingCartDb ^. shoppingCartOrders ) mapM_ print allUnshippedOrders SELECT \"t0\" . \"id\" AS \"res0\" , \"t0\" . \"date\" AS \"res1\" , \"t0\" . \"for_user__email\" AS \"res2\" , \"t0\" . \"ship_to_address__id\" AS \"res3\" , \"t0\" . \"shipping_info__id\" AS \"res4\" FROM \"orders\" AS \"t0\" WHERE ( \"t0\" . \"shipping_info__id\" ) IS NULL ; -- With values: [] Order { _orderId = 1 , _orderDate = 2022 - 01 - 18 01 : 29 : 29 , _orderForUser = UserId \"james@example.com\" , _orderShipToAddress = AddressId 1 , _orderShippingInfo = ShippingInfoId Nothing } Order { _orderId = 3 , _orderDate = 2022 - 01 - 18 01 : 29 : 29 , _orderForUser = UserId \"james@example.com\" , _orderShipToAddress = AddressId 1 , _orderShippingInfo = ShippingInfoId Nothing } Let's count up all shipped and unshipped orders by user, including users who have no orders. Haskell Sql Out shippingInformationByUser <- runBeamSqliteDebug putStrLn conn $ runSelectReturningList $ select $ aggregate_ ( \\ ( user , order ) -> let ShippingInfoId shippingInfoId = _orderShippingInfo order in ( group_ user , as_ @ Int32 $ count_ ( as_ @ ( Maybe Int32 ) ( maybe_ ( just_ 1 ) ( \\ _ -> nothing_ ) shippingInfoId )) , as_ @ Int32 $ count_ shippingInfoId ) ) $ do user <- all_ ( shoppingCartDb ^. shoppingCartUsers ) order <- leftJoin_ ( all_ ( shoppingCartDb ^. shoppingCartOrders )) ( \\ order -> _orderForUser order ` references_ ` user ) pure ( user , order ) mapM_ print shippingInformationByUser SELECT \"t0\" . \"email\" AS \"res0\" , \"t0\" . \"first_name\" AS \"res1\" , \"t0\" . \"last_name\" AS \"res2\" , \"t0\" . \"password\" AS \"res3\" , COUNT ( CASE WHEN ( \"t1\" . \"shipping_info__id\" ) IS NOT NULL THEN NULL ELSE ? END ) AS \"res4\" , COUNT ( \"t1\" . \"shipping_info__id\" ) AS \"res5\" FROM \"cart_users\" AS \"t0\" LEFT JOIN \"orders\" AS \"t1\" ON ( \"t1\" . \"for_user__email\" ) = ( \"t0\" . \"email\" ) GROUP BY \"t0\" . \"email\" , \"t0\" . \"first_name\" , \"t0\" . \"last_name\" , \"t0\" . \"password\" ; -- With values: [SQLInteger 1] ( User { _userEmail = \"betty@example.com\" , _userFirstName = \"Betty\" , _userLastName = \"Jones\" , _userPassword = \"82b054bd83ffad9b6cf8bdb98ce3cc2f\" } , 0 , 1 ) ( User { _userEmail = \"james@example.com\" , _userFirstName = \"James\" , _userLastName = \"Smith\" , _userPassword = \"b4cc344d25a2efe540adbf2678e2304c\" } , 2 , 0 ) ( User { _userEmail = \"sam@example.com\" , _userFirstName = \"Sam\" , _userLastName = \"Taylor\" , _userPassword = \"332532dcfaa1cbf61e2a266bd723612c\" } , 1 , 0 ) Uh-oh! There's an error in the result set! Sam is reported as having one unshipped order, instead of zero. Here we hit one of the limitations of beam's mapping to SQL, and really one of the limitations of SQL itself. Namely, the NULL in the result rows for Sam is not distinguished from the NULL in the shipping info key itself. Beam however does make the distinction. When beam deserializes a NULL in a Maybe field, the outermost Maybe is the one populated with Nothing . Thus it is impossible to retrieve a value like Just Nothing from the database using the default serializers and deserializers. In general, it's best to avoid highly nested Maybe s in your queries because it makes them more difficult to understand. One way to work around this issue in the above query is to use subselects. Haskell Sql Output shippingInformationByUser <- runBeamSqliteDebug putStrLn conn $ runSelectReturningList $ select $ do user <- all_ ( shoppingCartDb ^. shoppingCartUsers ) ( userEmail , unshippedCount ) <- aggregate_ ( \\ ( userEmail , order ) -> ( group_ userEmail , as_ @ Int32 countAll_ )) $ do user <- all_ ( shoppingCartDb ^. shoppingCartUsers ) order <- leftJoin_ ( all_ ( shoppingCartDb ^. shoppingCartOrders )) ( \\ order -> _orderForUser order ` references_ ` user &&. isNothing_ ( _orderShippingInfo order )) pure ( pk user , order ) guard_ ( userEmail ` references_ ` user ) ( userEmail , shippedCount ) <- aggregate_ ( \\ ( userEmail , order ) -> ( group_ userEmail , as_ @ Int32 countAll_ )) $ do user <- all_ ( shoppingCartDb ^. shoppingCartUsers ) order <- leftJoin_ ( all_ ( shoppingCartDb ^. shoppingCartOrders )) ( \\ order -> _orderForUser order ` references_ ` user &&. isJust_ ( _orderShippingInfo order )) pure ( pk user , order ) guard_ ( userEmail ` references_ ` user ) pure ( user , unshippedCount , shippedCount ) mapM_ print shippingInformationByUser SELECT \"t0\" . \"email\" AS \"res0\" , \"t0\" . \"first_name\" AS \"res1\" , \"t0\" . \"last_name\" AS \"res2\" , \"t0\" . \"password\" AS \"res3\" , \"t1\" . \"res1\" AS \"res4\" , \"t2\" . \"res1\" AS \"res5\" FROM \"cart_users\" AS \"t0\" INNER JOIN ( SELECT \"t0\" . \"email\" AS \"res0\" , COUNT ( * ) AS \"res1\" FROM \"cart_users\" AS \"t0\" LEFT JOIN \"orders\" AS \"t1\" ON (( \"t1\" . \"for_user__email\" ) = ( \"t0\" . \"email\" )) AND (( \"t1\" . \"shipping_info__id\" ) IS NULL ) GROUP BY \"t0\" . \"email\" ) AS \"t1\" INNER JOIN ( SELECT \"t0\" . \"email\" AS \"res0\" , COUNT ( * ) AS \"res1\" FROM \"cart_users\" AS \"t0\" LEFT JOIN \"orders\" AS \"t1\" ON (( \"t1\" . \"for_user__email\" ) = ( \"t0\" . \"email\" )) AND (( \"t1\" . \"shipping_info__id\" ) IS NOT NULL ) GROUP BY \"t0\" . \"email\" ) AS \"t2\" WHERE (( \"t1\" . \"res0\" ) = ( \"t0\" . \"email\" )) AND (( \"t2\" . \"res0\" ) = ( \"t0\" . \"email\" )); -- With values: [] ( User { _userEmail = \"betty@example.com\" , _userFirstName = \"Betty\" , _userLastName = \"Jones\" , _userPassword = \"82b054bd83ffad9b6cf8bdb98ce3cc2f\" } , 1 , 1 ) ( User { _userEmail = \"james@example.com\" , _userFirstName = \"James\" , _userLastName = \"Smith\" , _userPassword = \"b4cc344d25a2efe540adbf2678e2304c\" } , 2 , 1 ) ( User { _userEmail = \"sam@example.com\" , _userFirstName = \"Sam\" , _userLastName = \"Taylor\" , _userPassword = \"332532dcfaa1cbf61e2a266bd723612c\" } , 1 , 1 ) Notice that the aggregate_ s embedded in the Q monad were automatically converted into sub SELECT s. This is because beam queries are composable -- you can use them wherever they type check and sensible SQL will result. Of course, if you want more control, you can also use the subselect_ combinator to force generation of a sub SELECT . Haskell Sql Output shippingInformationByUser <- runBeamSqliteDebug putStrLn conn $ runSelectReturningList $ select $ do user <- all_ ( shoppingCartDb ^. shoppingCartUsers ) ( userEmail , unshippedCount ) <- subselect_ $ aggregate_ ( \\ ( userEmail , order ) -> ( group_ userEmail , as_ @ Int32 countAll_ )) $ do user <- all_ ( shoppingCartDb ^. shoppingCartUsers ) order <- leftJoin_ ( all_ ( shoppingCartDb ^. shoppingCartOrders )) ( \\ order -> _orderForUser order ` references_ ` user &&. isNothing_ ( _orderShippingInfo order )) pure ( pk user , order ) guard_ ( userEmail ` references_ ` user ) ( userEmail , shippedCount ) <- subselect_ $ aggregate_ ( \\ ( userEmail , order ) -> ( group_ userEmail , as_ @ Int32 countAll_ )) $ do user <- all_ ( shoppingCartDb ^. shoppingCartUsers ) order <- leftJoin_ ( all_ ( shoppingCartDb ^. shoppingCartOrders )) ( \\ order -> _orderForUser order ` references_ ` user &&. isJust_ ( _orderShippingInfo order )) pure ( pk user , order ) guard_ ( userEmail ` references_ ` user ) pure ( user , unshippedCount , shippedCount ) mapM_ print shippingInformationByUser SELECT \"t0\" . \"email\" AS \"res0\" , \"t0\" . \"first_name\" AS \"res1\" , \"t0\" . \"last_name\" AS \"res2\" , \"t0\" . \"password\" AS \"res3\" , \"t1\" . \"res1\" AS \"res4\" , \"t2\" . \"res1\" AS \"res5\" FROM \"cart_users\" AS \"t0\" INNER JOIN ( SELECT \"t0\" . \"res0\" AS \"res0\" , \"t0\" . \"res1\" AS \"res1\" FROM ( SELECT \"t0\" . \"email\" AS \"res0\" , COUNT ( * ) AS \"res1\" FROM \"cart_users\" AS \"t0\" LEFT JOIN \"orders\" AS \"t1\" ON (( \"t1\" . \"for_user__email\" ) = ( \"t0\" . \"email\" )) AND (( \"t1\" . \"shipping_info__id\" ) IS NULL ) GROUP BY \"t0\" . \"email\" ) AS \"t0\" ) AS \"t1\" INNER JOIN ( SELECT \"t0\" . \"res0\" AS \"res0\" , \"t0\" . \"res1\" AS \"res1\" FROM ( SELECT \"t0\" . \"email\" AS \"res0\" , COUNT ( * ) AS \"res1\" FROM \"cart_users\" AS \"t0\" LEFT JOIN \"orders\" AS \"t1\" ON (( \"t1\" . \"for_user__email\" ) = ( \"t0\" . \"email\" )) AND (( \"t1\" . \"shipping_info__id\" ) IS NOT NULL ) GROUP BY \"t0\" . \"email\" ) AS \"t0\" ) AS \"t2\" WHERE (( \"t1\" . \"res0\" ) = ( \"t0\" . \"email\" )) AND (( \"t2\" . \"res0\" ) = ( \"t0\" . \"email\" )); -- With values: [] ( User { _userEmail = \"betty@example.com\" , _userFirstName = \"Betty\" , _userLastName = \"Jones\" , _userPassword = \"82b054bd83ffad9b6cf8bdb98ce3cc2f\" } , 1 , 1 ) ( User { _userEmail = \"james@example.com\" , _userFirstName = \"James\" , _userLastName = \"Smith\" , _userPassword = \"b4cc344d25a2efe540adbf2678e2304c\" } , 2 , 1 ) ( User { _userEmail = \"sam@example.com\" , _userFirstName = \"Sam\" , _userLastName = \"Taylor\" , _userPassword = \"332532dcfaa1cbf61e2a266bd723612c\" } , 1 , 1 )","title":"Queries with nullable foreign keys"},{"location":"tutorials/tutorial3/#conclusion","text":"This tutorial completes our sequence on creating a shopping cart. Throughout the tutorials, we saw how to create tables using regular Haskell data types, how to link those tables up using relations, how to query tables using both the monadic interface and the list-like functions on queries. We saw ha few examples of using beam to generate advanced queries. More information on the Beam API is havailable on hackage . Happy beaming! Beam is a work in progress. Please submit bugs and patches on GitHub .","title":"Conclusion"},{"location":"user-guide/backends/","text":"Beam is backend-agnostic and doesn't provide any means to connect to a database. Beam backend libraries usually use well-used Haskell libraries to provide database connectivity. For example, the beam-sqlite backend uses the sqlite-simple backend. Beam distinguishes each backend via type indexes. Each backend defines a type that is used to enable backend-specific behavior. For example, the beam-sqlite backend ships with the Sqlite type that is used to distinguish sqlite specific constructs with generic or other backend-specific ones. Each backend can have one or more 'syntaxes', which are particular ways to query the database. While the beam-core library ships with a standard ANSI SQL builder, few real-world database implementations fully follow the standard. Most backends use their own custom syntax type. Internally, beam uses a finally-tagless representation for syntax trees that allow straightforward construction against any backend. Beam offers backend-generic functions for the most common operations against databases. These functions are meant to fit the lowest common denominator. For example, no control is offered over streaming results from SELECT statements. While these backend-generic functions are useful for ad-hoc querying and development, it is wisest to use backend-specific functions in production for maximum control. Refer to backend-specific documentation for more information. For our examples, we will use the beam-sqlite backend and demonstrate usage of the beam standard query functions. Connecting to a database Okay, so we can print out a SQL statement, but how do we execute it against a database? Beam provides a convenient MonadBeam type class that allows us to write queries in a backend agnostic manner. This is good-enough for most applications and preserves portability across databases. However, MonadBeam does not support features specific to each backend, nor does it guarantee the highest-performance. Most backends provide additional methods to query a database, and you should prefer these if you've committed to a particular backend. For tutorial purposes, we will use the beam-sqlite backend. First, install beam-sqlite with cabal or stack : $ cabal install beam-sqlite # or $ stack install beam-sqlite Now, load beam-sqlite in GHCi. Prelude > import Database.Beam.Sqlite Prelude Database . Beam . Sqlite > Now, in another terminal, load the example database provided. $ sqlite3 basics.db < beam-sqlite/examples/basics.sql Now, back in GHCi, we can create a connection to this database. Prelude Database.Beam.Sqlite > basics <- open \"basics.db\" Prelude Database.Beam.Sqlite > runBeamSqlite basics $ runSelectReturningList ( select ( all_ ( persons exampleDb ))) [ .. ] The runSelectReturningList function takes a SqlSelect for the given syntax and returns the results via a list. Voil\u00e0! We've successfully created our first query and run it against an example database. We have now seen the major functionalities of the beam library. In the next section we'll explore more advanced querying and using relationships between tables. Inserting data First, let's connect to a sqlite database, and create our schema. The beam-core does not offer any support for the SQL DDL language. There is a separate core library beam-migrate that offers complete support for ANSI-standard SQL DDL operations, as well as tools to manipulate database schemas. See the section on migrations for more information. For our example, we will simply issue a CREATE TABLE command directly against the database using sqlite-simple functionality: Prelude Schema> execute_ conn \"CREATE TABLE persons ( first_name TEXT NOT NULL, last_name TEXT NOT NULL, age INT NOT NULL, PRIMARY KEY(first_name, last_name) )\" Now we can insert some data into our database. beam-sqlite ships with a function runBeamSqlite , with the following signature: runBeamSqlite :: Connection -> SqliteM a -> IO a beam-sqlite uses the sqlite-simple library, so its handle type is Connection from Database.SQLite.Simple . SqliteM is a monad implementing MonadBeam which we can use to construct database actions from individual SQL commands (select, insert, update, delete). MonadBeam is a type class that relates a particular SQL syntax ( syntax ) to a backend ( be ), and a command monad ( m ). Inside the m monad, we can execute data query and manipulation commands. Let's insert some data into our database. We are going to use the runInsert function from MonadBeam . INSERTs are discussed in more detail in the data manipulation guide . Prelude Schema> :{ Prelude Schema| runBeamSqlite conn $ do Prelude Schema| runInsert $ insert (persons exampleDb) $ Prelude Schema| insertValues [ Person \"Bob\" \"Smith\" 50 Prelude Schema| , Person \"Alice\" \"Wong\" 55 Prelude Schema| , Person \"John\" \"Quincy\" 30 ] Prelude Schema| :} The runInsert function has the type signature runInsert :: MonadBeam syntax be m => SqlInsert syntax -> m () SqlInsert syntax represents a SQL INSERT command in the given syntax . We construct this value using the insert function from Database.Beam.Query . insert :: IsSql92InsertSyntax syntax => DatabaseEntity be db ( TableEntity table ) -> Sql92InsertValuesSyntax syntax -> SqlInsert syntax Intuitively, insert takes a database table descriptor and some values (particular to the given syntax) and returns a statement to insert these values. Sql92InsertValuesSyntax syntax always implements the IsSql92InsertValuesSyntax typeclass, which is where we get the insertValues function from. IsSql92InsertValuesSyntax also defines the insertSelect function for inserting values from the result of a SELECT statement. Other backends may provide other ways of specifying the source of values. Now, we can query the database, using the runSelect function. Like runInsert and insert , we use the select function to construct a value of type SqlSelect syntax , which can be run inside MonadBeam . We can use the runBeamSqliteDebug function to install a hook that beam will call with every SQL command it is about to run. In the following example, beam will print its query to stdout via putStrLn . You can use this functionality to hook beam in to a logging framework. Prelude Schema> runBeamSqliteDebug putStrLn conn $ runSelect (select (all_ (persons exampleDb))) [ Person { personFirstName = \"Bob\", personLastName=\"Smith\", personAge=50 }, ... ]","title":"Backends"},{"location":"user-guide/backends/#connecting-to-a-database","text":"Okay, so we can print out a SQL statement, but how do we execute it against a database? Beam provides a convenient MonadBeam type class that allows us to write queries in a backend agnostic manner. This is good-enough for most applications and preserves portability across databases. However, MonadBeam does not support features specific to each backend, nor does it guarantee the highest-performance. Most backends provide additional methods to query a database, and you should prefer these if you've committed to a particular backend. For tutorial purposes, we will use the beam-sqlite backend. First, install beam-sqlite with cabal or stack : $ cabal install beam-sqlite # or $ stack install beam-sqlite Now, load beam-sqlite in GHCi. Prelude > import Database.Beam.Sqlite Prelude Database . Beam . Sqlite > Now, in another terminal, load the example database provided. $ sqlite3 basics.db < beam-sqlite/examples/basics.sql Now, back in GHCi, we can create a connection to this database. Prelude Database.Beam.Sqlite > basics <- open \"basics.db\" Prelude Database.Beam.Sqlite > runBeamSqlite basics $ runSelectReturningList ( select ( all_ ( persons exampleDb ))) [ .. ] The runSelectReturningList function takes a SqlSelect for the given syntax and returns the results via a list. Voil\u00e0! We've successfully created our first query and run it against an example database. We have now seen the major functionalities of the beam library. In the next section we'll explore more advanced querying and using relationships between tables.","title":"Connecting to a database"},{"location":"user-guide/backends/#inserting-data","text":"First, let's connect to a sqlite database, and create our schema. The beam-core does not offer any support for the SQL DDL language. There is a separate core library beam-migrate that offers complete support for ANSI-standard SQL DDL operations, as well as tools to manipulate database schemas. See the section on migrations for more information. For our example, we will simply issue a CREATE TABLE command directly against the database using sqlite-simple functionality: Prelude Schema> execute_ conn \"CREATE TABLE persons ( first_name TEXT NOT NULL, last_name TEXT NOT NULL, age INT NOT NULL, PRIMARY KEY(first_name, last_name) )\" Now we can insert some data into our database. beam-sqlite ships with a function runBeamSqlite , with the following signature: runBeamSqlite :: Connection -> SqliteM a -> IO a beam-sqlite uses the sqlite-simple library, so its handle type is Connection from Database.SQLite.Simple . SqliteM is a monad implementing MonadBeam which we can use to construct database actions from individual SQL commands (select, insert, update, delete). MonadBeam is a type class that relates a particular SQL syntax ( syntax ) to a backend ( be ), and a command monad ( m ). Inside the m monad, we can execute data query and manipulation commands. Let's insert some data into our database. We are going to use the runInsert function from MonadBeam . INSERTs are discussed in more detail in the data manipulation guide . Prelude Schema> :{ Prelude Schema| runBeamSqlite conn $ do Prelude Schema| runInsert $ insert (persons exampleDb) $ Prelude Schema| insertValues [ Person \"Bob\" \"Smith\" 50 Prelude Schema| , Person \"Alice\" \"Wong\" 55 Prelude Schema| , Person \"John\" \"Quincy\" 30 ] Prelude Schema| :} The runInsert function has the type signature runInsert :: MonadBeam syntax be m => SqlInsert syntax -> m () SqlInsert syntax represents a SQL INSERT command in the given syntax . We construct this value using the insert function from Database.Beam.Query . insert :: IsSql92InsertSyntax syntax => DatabaseEntity be db ( TableEntity table ) -> Sql92InsertValuesSyntax syntax -> SqlInsert syntax Intuitively, insert takes a database table descriptor and some values (particular to the given syntax) and returns a statement to insert these values. Sql92InsertValuesSyntax syntax always implements the IsSql92InsertValuesSyntax typeclass, which is where we get the insertValues function from. IsSql92InsertValuesSyntax also defines the insertSelect function for inserting values from the result of a SELECT statement. Other backends may provide other ways of specifying the source of values. Now, we can query the database, using the runSelect function. Like runInsert and insert , we use the select function to construct a value of type SqlSelect syntax , which can be run inside MonadBeam . We can use the runBeamSqliteDebug function to install a hook that beam will call with every SQL command it is about to run. In the following example, beam will print its query to stdout via putStrLn . You can use this functionality to hook beam in to a logging framework. Prelude Schema> runBeamSqliteDebug putStrLn conn $ runSelect (select (all_ (persons exampleDb))) [ Person { personFirstName = \"Bob\", personLastName=\"Smith\", personAge=50 }, ... ]","title":"Inserting data"},{"location":"user-guide/custom-backends/","text":"Writing a custom backend","title":"Writing a Custom Backend"},{"location":"user-guide/custom-type-migration/","text":"Note : The code used in this guide is in beam-postgres/examples/Pagila/Schema/CustomMigrateExample.hs . Using Custom Types in migration In Beam Tutorial 3 we looked at marshalling custom types. Beam provides functionality to represent custom defined types in migration as well. From the tutorials, let us take our custom type ShippingCarrier . data ShippingCarrier = USPS | FedEx | UPS | DHL deriving ( Show , Read , Eq , Ord , Enum ) From this example , let us take the AddressT table and add a column addressShipper to it. -- | Address table data AddressT f = AddressT { addressId :: Columnar f ( SqlSerial Int32 ) , addressAddress1 :: Columnar f T . Text , addressAddress2 :: Columnar f ( Maybe T . Text ) , addressDistrict :: Columnar f T . Text , addressShipper :: Columnar f ShippingCarrier , addressPostalCode :: Columnar f T . Text , addressPhone :: Columnar f T . Text , addressLastUpdate :: Columnar f LocalTime } deriving Generic type Address = AddressT Identity deriving instance Show Address deriving instance Eq Address instance Table AddressT where data PrimaryKey AddressT f = AddressId ( Columnar f ( SqlSerial Int32 )) deriving Generic primaryKey = AddressId . addressId type AddressId = PrimaryKey AddressT Identity deriving instance Show AddressId deriving instance Eq AddressId instance Beamable ( PrimaryKey AddressT ) instance Beamable AddressT In order to use ShippingCarrier in migration, we need to define a value of DataType , which is defined in Database.Beam.Migrate.SQL.Types and re-exported by the exposed module Database.Beam.Migrate . If we want our ShippingCarrier type to take up the postgres TEXT type, import Database.Beam.Postgres.Syntax ( PgDataTypeSyntax , pgTextType ) import Database.Beam.Migrate ( DataType ( .. )) shippingCarrierType :: DataType PgDataTypeSyntax ShippingCarrier shippingCarrierType = DataType pgTextType Afterwards, we also need to instantiate FromBackendRow and SqlValueSyntax for ShippingCarrier for whichever backend and syntax we are using (postgres, in our case). import qualified Data.Text as T instance HasSqlValueSyntax be String => HasSqlValueSyntax be ShippingCarrier where sqlValueSyntax = autoSqlValueSyntax -- | An explicit definition of ``fromBackendRow`` is required for each custom type instance ( BeamBackend be , FromBackendRow be T . Text ) => FromBackendRow be ShippingCarrier where fromBackendRow = do val <- fromBackendRow case val :: T . Text of \"usps\" -> pure USPS \"fedex\" -> pure FedEx \"ups\" -> pure UPS \"dhl\" -> pure DHL _ -> fail ( \"Invalid value for ShippingCarrier: \" ++ T . unpack val ) For data type defaulting support, instantiate HasDefaultSqlDataType and HasDefaultSqlDataTypeConstraints . instance ( IsSql92ColumnSchemaSyntax be ) => HasDefaultSqlDataTypeConstraints be ShippingCarrier Finally, if we defined our Database like this: -- | Pagila db data PagilaDb f = PagilaDb { address :: f ( TableEntity AddressT ) } deriving Generic instance Database PagilaDb We can write our migration function like this: lastUpdateField :: TableFieldSchema PgColumnSchemaSyntax LocalTime lastUpdateField = field \"last_update\" timestamp ( defaultTo_ now_ ) notNull migration :: () -> Migration PgCommandSyntax ( CheckedDatabaseSettings Postgres PagilaDb ) migration () = do -- year_ <- createDomain \"year\" integer (check (\\yr -> yr >=. 1901 &&. yr <=. 2155)) PagilaDb <$> createTable \"address\" ( AddressT ( field \"address_id\" smallserial ) ( field \"address\" ( varchar ( Just 50 )) notNull ) ( field \"address2\" ( maybeType $ varchar ( Just 50 ))) ( field \"district\" ( varchar ( Just 20 )) notNull ) ( field \"shipper\" shippingCarrierType ) ( field \"postal_code\" ( varchar ( Just 10 ))) ( field \"phone\" ( varchar ( Just 20 )) notNull ) lastUpdateField ) The export list of the module Database.Beam.Postgres.Syntax should serve as a good guide for which existing types can be used to represent our custom data types in migration.","title":"Custom type migration"},{"location":"user-guide/custom-type-migration/#using-custom-types-in-migration","text":"In Beam Tutorial 3 we looked at marshalling custom types. Beam provides functionality to represent custom defined types in migration as well. From the tutorials, let us take our custom type ShippingCarrier . data ShippingCarrier = USPS | FedEx | UPS | DHL deriving ( Show , Read , Eq , Ord , Enum ) From this example , let us take the AddressT table and add a column addressShipper to it. -- | Address table data AddressT f = AddressT { addressId :: Columnar f ( SqlSerial Int32 ) , addressAddress1 :: Columnar f T . Text , addressAddress2 :: Columnar f ( Maybe T . Text ) , addressDistrict :: Columnar f T . Text , addressShipper :: Columnar f ShippingCarrier , addressPostalCode :: Columnar f T . Text , addressPhone :: Columnar f T . Text , addressLastUpdate :: Columnar f LocalTime } deriving Generic type Address = AddressT Identity deriving instance Show Address deriving instance Eq Address instance Table AddressT where data PrimaryKey AddressT f = AddressId ( Columnar f ( SqlSerial Int32 )) deriving Generic primaryKey = AddressId . addressId type AddressId = PrimaryKey AddressT Identity deriving instance Show AddressId deriving instance Eq AddressId instance Beamable ( PrimaryKey AddressT ) instance Beamable AddressT In order to use ShippingCarrier in migration, we need to define a value of DataType , which is defined in Database.Beam.Migrate.SQL.Types and re-exported by the exposed module Database.Beam.Migrate . If we want our ShippingCarrier type to take up the postgres TEXT type, import Database.Beam.Postgres.Syntax ( PgDataTypeSyntax , pgTextType ) import Database.Beam.Migrate ( DataType ( .. )) shippingCarrierType :: DataType PgDataTypeSyntax ShippingCarrier shippingCarrierType = DataType pgTextType Afterwards, we also need to instantiate FromBackendRow and SqlValueSyntax for ShippingCarrier for whichever backend and syntax we are using (postgres, in our case). import qualified Data.Text as T instance HasSqlValueSyntax be String => HasSqlValueSyntax be ShippingCarrier where sqlValueSyntax = autoSqlValueSyntax -- | An explicit definition of ``fromBackendRow`` is required for each custom type instance ( BeamBackend be , FromBackendRow be T . Text ) => FromBackendRow be ShippingCarrier where fromBackendRow = do val <- fromBackendRow case val :: T . Text of \"usps\" -> pure USPS \"fedex\" -> pure FedEx \"ups\" -> pure UPS \"dhl\" -> pure DHL _ -> fail ( \"Invalid value for ShippingCarrier: \" ++ T . unpack val ) For data type defaulting support, instantiate HasDefaultSqlDataType and HasDefaultSqlDataTypeConstraints . instance ( IsSql92ColumnSchemaSyntax be ) => HasDefaultSqlDataTypeConstraints be ShippingCarrier Finally, if we defined our Database like this: -- | Pagila db data PagilaDb f = PagilaDb { address :: f ( TableEntity AddressT ) } deriving Generic instance Database PagilaDb We can write our migration function like this: lastUpdateField :: TableFieldSchema PgColumnSchemaSyntax LocalTime lastUpdateField = field \"last_update\" timestamp ( defaultTo_ now_ ) notNull migration :: () -> Migration PgCommandSyntax ( CheckedDatabaseSettings Postgres PagilaDb ) migration () = do -- year_ <- createDomain \"year\" integer (check (\\yr -> yr >=. 1901 &&. yr <=. 2155)) PagilaDb <$> createTable \"address\" ( AddressT ( field \"address_id\" smallserial ) ( field \"address\" ( varchar ( Just 50 )) notNull ) ( field \"address2\" ( maybeType $ varchar ( Just 50 ))) ( field \"district\" ( varchar ( Just 20 )) notNull ) ( field \"shipper\" shippingCarrierType ) ( field \"postal_code\" ( varchar ( Just 10 ))) ( field \"phone\" ( varchar ( Just 20 )) notNull ) lastUpdateField ) The export list of the module Database.Beam.Postgres.Syntax should serve as a good guide for which existing types can be used to represent our custom data types in migration.","title":"Using Custom Types in migration"},{"location":"user-guide/databases/","text":"In addition to defining types for each of your tables, beam also requires you to declare your database as a type with fields for holding all entities in your database. This includes more than just tables. For example, user-defined types that you would like to work with must also be included in your database type. A simple database type Like tables, a database type takes a functor and applies it to each entity in the database. For example, a database type for the two tables defined above has the form. data ExampleDb f = ExampleDb { persons :: f ( TableEntity PersonT ) , posts :: f ( TableEntity PostT ) } deriving ( Generic , Database be ) exampleDb :: DatabaseSettings be ExampleDb exampleDb = defaultDbSettings Other database entities Views Some databases also offer the concept of 'views' -- pseudo-tables that are built from a pre-defined query. Suppose we wanted to create a view that returned the latest comments and their respective posters. data PostAndPosterView f = PostAndPosterView { post :: PostT f , poster :: PersonT f } deriving ( Generic , Beamable ) We can include this in our database: data ExampleDb f = ExampleDb { persons :: f ( TableEntity PersonT ) , posts :: f ( TableEntity PostT ) , postAndPosters :: f ( ViewEntity PostAndPosterView ) } deriving ( Generic , Database be ) Now we can use postAndPosters wherever we'd use a table. Note that you do not need to specify the definition of the view. The definition is not important to access the view, so beam does not need to know about it at the type-level. If you want to manipulate view definitions, use the migrations package. Note that the all_ query primitive requires a TableEntity . Thus, all_ (postAndPosters exampleDb) will fail to type-check. Use the allFromView_ combinator instead. Note You could also declare a view as a TableEntity . The main advantage of declaring an entity as ViewEntity is that you will be prevented by the Haskell type system from constructing INSERT s, UPDATE s, and DELETE s using your view. Also, beam-migrate will not recognize database schema equivalence if a view is declared as a table or vice versa. Domain types Domain types are a way of creating new database types with additional constraints. Beam supports declaring these types as part of your database, so they can be used anywhere a data type can. In order to use your domain type, you need to supply beam a Haskell newtype that is used to represent values of this type in Haskell. Character sets Beam does not yet support character sets. Support is planned in future releases. Collations Beam does not yet support collations. Support is planned in future releases. Translations Beam does not yet support translations. Support is planned in future releases. Other database entities Other standard SQL database entities (like triggers) are defined by beam-migrate as they have no effect on query semantics. Database descriptors In order to interact with the database, beam needs to know more about the data structure, it also needs to know how to refer to each entity in your database. For the most part, beam can figure out the names for you using its Generics-based defaulting mechanims. Once you have a database type defined, you can create a database descriptor using the defaultDbSettings function. For example, to create a backend-agnostic database descriptor for the ExampleDb type: exampleDb :: DatabaseSettings be ExampleDb exampleDb = defaultDbSettings The defaultDbSettings function produces a settings value where each entity is given a default name as explained in the previous section . Now, we can use the entities in exampleDb to write queries. The rules for name defaulting for database entities are the same as those for table fields Modifying the defaults The withDbModification function can be used to modify the output of the defaultDbSettings . It combines a database settings value with a database modifications value . The easiest way to construct a database modification value is with the dbModification function, which produces a modification that makes no changes. You can then use Haskell record syntax to specify table or other entity modifications. Modifications can be combined with the (<>) semigroup operator. One common operation is renaming an entity. The setEntityName function can be used to set the name of any entity (table, view, etc). The modifyEntityName function can be used to derive a new name based on the default-assigned beam one. Another common operation is renaming table fields. You can use the modifyTableFields modification for this. Simply pass a tableModification where each record contains the new name of the field. For example, to rename the persons table as people in the database above, exampleDb :: DatabaseSettings be ExampleDb exampleDb = defaultDbSettings ` withDbModification ` dbModification { persons = setEntityName \"people\" } Or, to keep the persons table named as it is, but change the name of the personEmail field from \"email\" to \"email_address\" exampleDb :: DatabaseSettings be ExampleDb exampleDb = defaultDbSettings ` withDbModification ` dbModification { persons = modifyTableFields tableModification { personEmail = fieldNamed \"email_address\" } } To do both, exampleDb :: DatabaseSettings be ExampleDb exampleDb = defaultDbSettings ` withDbModification ` dbModification { persons = setEntityName \"people\" <> modifyTableFields tableModification { personEmail = fieldNamed \"email_address\" } } An appropriate IsString instance is also given so you can avoid the use of fieldNamed . For example, the above is equivalent to exampleDb :: DatabaseSettings be ExampleDb exampleDb = defaultDbSettings ` withDbModification ` dbModification { persons = setEntityName \"people\" <> modifyTableFields tableModification { personEmail = \"email_address\" } }","title":"Databases"},{"location":"user-guide/databases/#a-simple-database-type","text":"Like tables, a database type takes a functor and applies it to each entity in the database. For example, a database type for the two tables defined above has the form. data ExampleDb f = ExampleDb { persons :: f ( TableEntity PersonT ) , posts :: f ( TableEntity PostT ) } deriving ( Generic , Database be ) exampleDb :: DatabaseSettings be ExampleDb exampleDb = defaultDbSettings","title":"A simple database type"},{"location":"user-guide/databases/#other-database-entities","text":"","title":"Other database entities"},{"location":"user-guide/databases/#views","text":"Some databases also offer the concept of 'views' -- pseudo-tables that are built from a pre-defined query. Suppose we wanted to create a view that returned the latest comments and their respective posters. data PostAndPosterView f = PostAndPosterView { post :: PostT f , poster :: PersonT f } deriving ( Generic , Beamable ) We can include this in our database: data ExampleDb f = ExampleDb { persons :: f ( TableEntity PersonT ) , posts :: f ( TableEntity PostT ) , postAndPosters :: f ( ViewEntity PostAndPosterView ) } deriving ( Generic , Database be ) Now we can use postAndPosters wherever we'd use a table. Note that you do not need to specify the definition of the view. The definition is not important to access the view, so beam does not need to know about it at the type-level. If you want to manipulate view definitions, use the migrations package. Note that the all_ query primitive requires a TableEntity . Thus, all_ (postAndPosters exampleDb) will fail to type-check. Use the allFromView_ combinator instead. Note You could also declare a view as a TableEntity . The main advantage of declaring an entity as ViewEntity is that you will be prevented by the Haskell type system from constructing INSERT s, UPDATE s, and DELETE s using your view. Also, beam-migrate will not recognize database schema equivalence if a view is declared as a table or vice versa.","title":"Views"},{"location":"user-guide/databases/#domain-types","text":"Domain types are a way of creating new database types with additional constraints. Beam supports declaring these types as part of your database, so they can be used anywhere a data type can. In order to use your domain type, you need to supply beam a Haskell newtype that is used to represent values of this type in Haskell.","title":"Domain types"},{"location":"user-guide/databases/#character-sets","text":"Beam does not yet support character sets. Support is planned in future releases.","title":"Character sets"},{"location":"user-guide/databases/#collations","text":"Beam does not yet support collations. Support is planned in future releases.","title":"Collations"},{"location":"user-guide/databases/#translations","text":"Beam does not yet support translations. Support is planned in future releases.","title":"Translations"},{"location":"user-guide/databases/#other-database-entities_1","text":"Other standard SQL database entities (like triggers) are defined by beam-migrate as they have no effect on query semantics.","title":"Other database entities"},{"location":"user-guide/databases/#database-descriptors","text":"In order to interact with the database, beam needs to know more about the data structure, it also needs to know how to refer to each entity in your database. For the most part, beam can figure out the names for you using its Generics-based defaulting mechanims. Once you have a database type defined, you can create a database descriptor using the defaultDbSettings function. For example, to create a backend-agnostic database descriptor for the ExampleDb type: exampleDb :: DatabaseSettings be ExampleDb exampleDb = defaultDbSettings The defaultDbSettings function produces a settings value where each entity is given a default name as explained in the previous section . Now, we can use the entities in exampleDb to write queries. The rules for name defaulting for database entities are the same as those for table fields","title":"Database descriptors"},{"location":"user-guide/databases/#modifying-the-defaults","text":"The withDbModification function can be used to modify the output of the defaultDbSettings . It combines a database settings value with a database modifications value . The easiest way to construct a database modification value is with the dbModification function, which produces a modification that makes no changes. You can then use Haskell record syntax to specify table or other entity modifications. Modifications can be combined with the (<>) semigroup operator. One common operation is renaming an entity. The setEntityName function can be used to set the name of any entity (table, view, etc). The modifyEntityName function can be used to derive a new name based on the default-assigned beam one. Another common operation is renaming table fields. You can use the modifyTableFields modification for this. Simply pass a tableModification where each record contains the new name of the field. For example, to rename the persons table as people in the database above, exampleDb :: DatabaseSettings be ExampleDb exampleDb = defaultDbSettings ` withDbModification ` dbModification { persons = setEntityName \"people\" } Or, to keep the persons table named as it is, but change the name of the personEmail field from \"email\" to \"email_address\" exampleDb :: DatabaseSettings be ExampleDb exampleDb = defaultDbSettings ` withDbModification ` dbModification { persons = modifyTableFields tableModification { personEmail = fieldNamed \"email_address\" } } To do both, exampleDb :: DatabaseSettings be ExampleDb exampleDb = defaultDbSettings ` withDbModification ` dbModification { persons = setEntityName \"people\" <> modifyTableFields tableModification { personEmail = fieldNamed \"email_address\" } } An appropriate IsString instance is also given so you can avoid the use of fieldNamed . For example, the above is equivalent to exampleDb :: DatabaseSettings be ExampleDb exampleDb = defaultDbSettings ` withDbModification ` dbModification { persons = setEntityName \"people\" <> modifyTableFields tableModification { personEmail = \"email_address\" } }","title":"Modifying the defaults"},{"location":"user-guide/expressions/","text":"Typing The type of all SQL-level expressions is QGenExpr . See the query tutorial for more information. In many cases, you'd like to type the SQL-level result of an expression without having to give explicit types for the other QGenExpr parameters. You can do this with the as_ combinator and -XTypeApplications . The following code types the literal 1 as a Double . as_ @ Double 1 This is rarely needed, but there are a few cases where the beam types are too general for the compiler to meaningfully infer types. Literals Integer literals can be constructed using fromIntegral in the Num typeclass. This means you can also just use a Haskell integer literal as a QGenExpr in any context. Rational literals can be constructed via fromRational in Rational . Regular Haskell rational literals will be automatically converted to QGenExprs . Text literals can be constructed via fromString in IsString . Again, Haskell string constants will automatically be converted to QGenExprs , although you may have to provide an explicit type, as different backends support different text types natively. All other literals can be constructed using the val_ function in SqlValable . This requires that there is an implementation of HasSqlValueSyntax (Sql92ExpressionValueSyntax syntax) x for the type x in the appropriate syntax for the QGenExpr . For example, to construct a value of type Vector Int32 in the beam-postgres backend. val_ ( V . fromList [ 1 , 2 , 3 :: Int32 ]) Explicit tables can be brought to the SQL value level by using val_ as well. For example, if you have an AddressT Identity named a , val_ a :: AddressT (QGenExpr context expr s) . UTF support All included beam backends play nicely with UTF. New backends should also support UTF, if they support syntaxes and deserializers for String or Text . Haskell Postgres Sqlite filter_ ( \\ s -> customerFirstName s ==. \"\u3042\u304d\u3089\" ) $ all_ ( customer chinookDb ) SELECT \"t0\" . \"CustomerId\" AS \"res0\" , \"t0\" . \"FirstName\" AS \"res1\" , \"t0\" . \"LastName\" AS \"res2\" , \"t0\" . \"Company\" AS \"res3\" , \"t0\" . \"Address\" AS \"res4\" , \"t0\" . \"City\" AS \"res5\" , \"t0\" . \"State\" AS \"res6\" , \"t0\" . \"Country\" AS \"res7\" , \"t0\" . \"PostalCode\" AS \"res8\" , \"t0\" . \"Phone\" AS \"res9\" , \"t0\" . \"Fax\" AS \"res10\" , \"t0\" . \"Email\" AS \"res11\" , \"t0\" . \"SupportRepId\" AS \"res12\" FROM \"Customer\" AS \"t0\" WHERE ( \"t0\" . \"FirstName\" ) = ( '\u3042\u304d\u3089' ) SELECT \"t0\" . \"CustomerId\" AS \"res0\" , \"t0\" . \"FirstName\" AS \"res1\" , \"t0\" . \"LastName\" AS \"res2\" , \"t0\" . \"Company\" AS \"res3\" , \"t0\" . \"Address\" AS \"res4\" , \"t0\" . \"City\" AS \"res5\" , \"t0\" . \"State\" AS \"res6\" , \"t0\" . \"Country\" AS \"res7\" , \"t0\" . \"PostalCode\" AS \"res8\" , \"t0\" . \"Phone\" AS \"res9\" , \"t0\" . \"Fax\" AS \"res10\" , \"t0\" . \"Email\" AS \"res11\" , \"t0\" . \"SupportRepId\" AS \"res12\" FROM \"Customer\" AS \"t0\" WHERE ( \"t0\" . \"FirstName\" ) = ( ? ); -- With values: [SQLText \"\\12354\\12365\\12425\"] Arithmetic Arithmetic operations that are part of the Fractional and Num classes can be used directly. For example, if a and b are QGenExpr s of the same type, then a + b is a QGenExpr of the same type. Because of the toInteger class method in Integral , QGenExpr s cannot implement Integral . Nevertheless, versions of div and mod are available as div_ and mod_ , respectively, having the corresponding type. Comparison SQL comparison is not as simple as you may think. NULL handling in particular actually makes things rather complicated. SQL comparison operators actually return a tri-state boolean , representing true, false, and unknown , which is the result when two nulls are compared. Boolean combinators ( AND and OR ) handle these values in different ways. Beam abstracts some of this difference away, if you ask it to. Haskell-like comparisons Haskell provides much more reasonable equality between potentially optional values. For example, Nothing == Nothing always! SQL does not provide a similar guarantee. However, beam can emulate Haskell-like equality in SQL using the ==. operator. This uses a CASE .. WHEN .. statement or a special operator that properly handles NULL s in your given backend. Depending on your backend, this can severely impact performance, but it's 'correct'. For example, to find all customers living in Berlin: Haskell Postgres Sqlite filter_ ( \\ s -> addressCity ( customerAddress s ) ==. val_ ( Just \"Berlin\" )) $ all_ ( customer chinookDb ) SELECT \"t0\" . \"CustomerId\" AS \"res0\" , \"t0\" . \"FirstName\" AS \"res1\" , \"t0\" . \"LastName\" AS \"res2\" , \"t0\" . \"Company\" AS \"res3\" , \"t0\" . \"Address\" AS \"res4\" , \"t0\" . \"City\" AS \"res5\" , \"t0\" . \"State\" AS \"res6\" , \"t0\" . \"Country\" AS \"res7\" , \"t0\" . \"PostalCode\" AS \"res8\" , \"t0\" . \"Phone\" AS \"res9\" , \"t0\" . \"Fax\" AS \"res10\" , \"t0\" . \"Email\" AS \"res11\" , \"t0\" . \"SupportRepId\" AS \"res12\" FROM \"Customer\" AS \"t0\" WHERE ( \"t0\" . \"City\" ) IS NOT DISTINCT FROM ( 'Berlin' ) SELECT \"t0\" . \"CustomerId\" AS \"res0\" , \"t0\" . \"FirstName\" AS \"res1\" , \"t0\" . \"LastName\" AS \"res2\" , \"t0\" . \"Company\" AS \"res3\" , \"t0\" . \"Address\" AS \"res4\" , \"t0\" . \"City\" AS \"res5\" , \"t0\" . \"State\" AS \"res6\" , \"t0\" . \"Country\" AS \"res7\" , \"t0\" . \"PostalCode\" AS \"res8\" , \"t0\" . \"Phone\" AS \"res9\" , \"t0\" . \"Fax\" AS \"res10\" , \"t0\" . \"Email\" AS \"res11\" , \"t0\" . \"SupportRepId\" AS \"res12\" FROM \"Customer\" AS \"t0\" WHERE CASE WHEN (( \"t0\" . \"City\" ) IS NULL ) AND (( ? ) IS NULL ) THEN ? WHEN (( \"t0\" . \"City\" ) IS NULL ) OR (( ? ) IS NULL ) THEN ? ELSE ( \"t0\" . \"City\" ) = ( ? ) END ; -- With values: [SQLText \"Berlin\",SQLInteger 1,SQLText \"Berlin\",SQLInteger 0,SQLText \"Berlin\"] Notice that SQLite uses a CASE .. WHEN .. statement, while Postgres uses the IS NOT DISTINCT FROM operator. The inequality operator is named /=. , as expected. Note that both ==. and /=. return a SQL expression whose type is Bool . SQL-like comparisons Beam also provides equality operators that act like their underlying SQL counterparts. These operators map most directly to the SQL = and <> operators, but they require you to explicitly handle the possibility of NULL s. These operators are named ==?. and /=?. respectively. Unlike ==. and /=. , these operators return an expression of type SqlBool . SqlBool is a type that can only be manipulated as part of a SQL expression, and cannot be serialized or deserialized to/from Haskell. You need to convert it to a Bool value explicitly in order to get the result or use it with more advanced operators, such as CASE .. WHEN .. . In SQL, you can handle potentially unknown comparisons using the IS TRUE , IS NOT TRUE , IS FALSE , IS NOT FALSE , IS UNKNOWN , and IS NOT UNKNOWN operators. These are provided as the beam functions isTrue_ , isNotTrue_ , etc. These each take a SQL expression of type SqlBool and return one of type Bool . For example, to join every employee and customer who live in the same city, but using SQL-like equality and making sure the comparison really is true (i.e., customers and employees who both have NULL cities will not be included). Haskell Postgres Sqlite do c <- all_ ( customer chinookDb ) e <- join_ ( employee chinookDb ) $ \\ e -> isTrue_ ( addressCity ( customerAddress c ) ==?. addressCity ( employeeAddress e )) pure ( c , e ) SELECT \"t0\" . \"CustomerId\" AS \"res0\" , \"t0\" . \"FirstName\" AS \"res1\" , \"t0\" . \"LastName\" AS \"res2\" , \"t0\" . \"Company\" AS \"res3\" , \"t0\" . \"Address\" AS \"res4\" , \"t0\" . \"City\" AS \"res5\" , \"t0\" . \"State\" AS \"res6\" , \"t0\" . \"Country\" AS \"res7\" , \"t0\" . \"PostalCode\" AS \"res8\" , \"t0\" . \"Phone\" AS \"res9\" , \"t0\" . \"Fax\" AS \"res10\" , \"t0\" . \"Email\" AS \"res11\" , \"t0\" . \"SupportRepId\" AS \"res12\" , \"t1\" . \"EmployeeId\" AS \"res13\" , \"t1\" . \"LastName\" AS \"res14\" , \"t1\" . \"FirstName\" AS \"res15\" , \"t1\" . \"Title\" AS \"res16\" , \"t1\" . \"ReportsTo\" AS \"res17\" , \"t1\" . \"BirthDate\" AS \"res18\" , \"t1\" . \"HireDate\" AS \"res19\" , \"t1\" . \"Address\" AS \"res20\" , \"t1\" . \"City\" AS \"res21\" , \"t1\" . \"State\" AS \"res22\" , \"t1\" . \"Country\" AS \"res23\" , \"t1\" . \"PostalCode\" AS \"res24\" , \"t1\" . \"Phone\" AS \"res25\" , \"t1\" . \"Fax\" AS \"res26\" , \"t1\" . \"Email\" AS \"res27\" FROM \"Customer\" AS \"t0\" INNER JOIN \"Employee\" AS \"t1\" ON (( \"t0\" . \"City\" ) = ( \"t1\" . \"City\" )) IS TRUE SELECT \"t0\" . \"CustomerId\" AS \"res0\" , \"t0\" . \"FirstName\" AS \"res1\" , \"t0\" . \"LastName\" AS \"res2\" , \"t0\" . \"Company\" AS \"res3\" , \"t0\" . \"Address\" AS \"res4\" , \"t0\" . \"City\" AS \"res5\" , \"t0\" . \"State\" AS \"res6\" , \"t0\" . \"Country\" AS \"res7\" , \"t0\" . \"PostalCode\" AS \"res8\" , \"t0\" . \"Phone\" AS \"res9\" , \"t0\" . \"Fax\" AS \"res10\" , \"t0\" . \"Email\" AS \"res11\" , \"t0\" . \"SupportRepId\" AS \"res12\" , \"t1\" . \"EmployeeId\" AS \"res13\" , \"t1\" . \"LastName\" AS \"res14\" , \"t1\" . \"FirstName\" AS \"res15\" , \"t1\" . \"Title\" AS \"res16\" , \"t1\" . \"ReportsTo\" AS \"res17\" , \"t1\" . \"BirthDate\" AS \"res18\" , \"t1\" . \"HireDate\" AS \"res19\" , \"t1\" . \"Address\" AS \"res20\" , \"t1\" . \"City\" AS \"res21\" , \"t1\" . \"State\" AS \"res22\" , \"t1\" . \"Country\" AS \"res23\" , \"t1\" . \"PostalCode\" AS \"res24\" , \"t1\" . \"Phone\" AS \"res25\" , \"t1\" . \"Fax\" AS \"res26\" , \"t1\" . \"Email\" AS \"res27\" FROM \"Customer\" AS \"t0\" INNER JOIN \"Employee\" AS \"t1\" ON (( \"t0\" . \"City\" ) = ( \"t1\" . \"City\" )) IS 1 ; -- With values: [] Thinking of which IS .. operator to use can be confusing. If you have a default value you'd like to return in the case of an unknown comparison, use the unknownAs_ function. For example, if we want to treat unknown values as True instead (i.e, we want customers and employees who both have NULL cities to be included) Haskell Postgres Sqlite do c <- all_ ( customer chinookDb ) e <- join_ ( employee chinookDb ) $ \\ e -> unknownAs_ True ( addressCity ( customerAddress c ) ==?. addressCity ( employeeAddress e )) pure ( c , e ) SELECT \"t0\" . \"CustomerId\" AS \"res0\" , \"t0\" . \"FirstName\" AS \"res1\" , \"t0\" . \"LastName\" AS \"res2\" , \"t0\" . \"Company\" AS \"res3\" , \"t0\" . \"Address\" AS \"res4\" , \"t0\" . \"City\" AS \"res5\" , \"t0\" . \"State\" AS \"res6\" , \"t0\" . \"Country\" AS \"res7\" , \"t0\" . \"PostalCode\" AS \"res8\" , \"t0\" . \"Phone\" AS \"res9\" , \"t0\" . \"Fax\" AS \"res10\" , \"t0\" . \"Email\" AS \"res11\" , \"t0\" . \"SupportRepId\" AS \"res12\" , \"t1\" . \"EmployeeId\" AS \"res13\" , \"t1\" . \"LastName\" AS \"res14\" , \"t1\" . \"FirstName\" AS \"res15\" , \"t1\" . \"Title\" AS \"res16\" , \"t1\" . \"ReportsTo\" AS \"res17\" , \"t1\" . \"BirthDate\" AS \"res18\" , \"t1\" . \"HireDate\" AS \"res19\" , \"t1\" . \"Address\" AS \"res20\" , \"t1\" . \"City\" AS \"res21\" , \"t1\" . \"State\" AS \"res22\" , \"t1\" . \"Country\" AS \"res23\" , \"t1\" . \"PostalCode\" AS \"res24\" , \"t1\" . \"Phone\" AS \"res25\" , \"t1\" . \"Fax\" AS \"res26\" , \"t1\" . \"Email\" AS \"res27\" FROM \"Customer\" AS \"t0\" INNER JOIN \"Employee\" AS \"t1\" ON (( \"t0\" . \"City\" ) = ( \"t1\" . \"City\" )) IS NOT FALSE SELECT \"t0\" . \"CustomerId\" AS \"res0\" , \"t0\" . \"FirstName\" AS \"res1\" , \"t0\" . \"LastName\" AS \"res2\" , \"t0\" . \"Company\" AS \"res3\" , \"t0\" . \"Address\" AS \"res4\" , \"t0\" . \"City\" AS \"res5\" , \"t0\" . \"State\" AS \"res6\" , \"t0\" . \"Country\" AS \"res7\" , \"t0\" . \"PostalCode\" AS \"res8\" , \"t0\" . \"Phone\" AS \"res9\" , \"t0\" . \"Fax\" AS \"res10\" , \"t0\" . \"Email\" AS \"res11\" , \"t0\" . \"SupportRepId\" AS \"res12\" , \"t1\" . \"EmployeeId\" AS \"res13\" , \"t1\" . \"LastName\" AS \"res14\" , \"t1\" . \"FirstName\" AS \"res15\" , \"t1\" . \"Title\" AS \"res16\" , \"t1\" . \"ReportsTo\" AS \"res17\" , \"t1\" . \"BirthDate\" AS \"res18\" , \"t1\" . \"HireDate\" AS \"res19\" , \"t1\" . \"Address\" AS \"res20\" , \"t1\" . \"City\" AS \"res21\" , \"t1\" . \"State\" AS \"res22\" , \"t1\" . \"Country\" AS \"res23\" , \"t1\" . \"PostalCode\" AS \"res24\" , \"t1\" . \"Phone\" AS \"res25\" , \"t1\" . \"Fax\" AS \"res26\" , \"t1\" . \"Email\" AS \"res27\" FROM \"Customer\" AS \"t0\" INNER JOIN \"Employee\" AS \"t1\" ON (( \"t0\" . \"City\" ) = ( \"t1\" . \"City\" )) IS NOT 0 ; -- With values: [] Quantified comparison SQL also allows comparisons to be quantified . For example, the SQL expression a == ANY(b) evaluates to true only if one row of b is equal to a . Similarly, a > ALL(b) returns true if a > x for every x in b . These are also supported using the ==*. , /=*. , <*. , >*. , <=*. , and >=*. operators. Like their unquantified counterparts, these operators yield a QGenExpr of type Bool . Unlike the unquantified operators, the second argument of these operators is of type QQuantified . You can create a QQuantified from a QGenExpr by using the anyOf_/anyIn_ or allOf_/allIn_ functions, which correspond to the ANY and ALL syntax respectively. anyOf_ and allOf_ take Q expressions (representing a query) and anyIn_ and allIn_ take lists of expressions. Quantified comparisons are always performed according to SQL semantics, meaning that they return values of type SqlBOol . This is because proper NULL handling with quantified comparisons cannot be expressed in a reasonable way. Use the functions described in the section above . For example, to get all invoice lines containing tracks longer than 3 minutes: Haskell Postgres let tracksLongerThanThreeMinutes = fmap trackId $ filter_ ( \\ t -> trackMilliseconds t >=. 180000 ) $ all_ ( track chinookDb ) in filter_ ( \\ ln -> let TrackId lnTrackId = invoiceLineTrack ln in unknownAs_ False ( lnTrackId ==*. anyOf_ tracksLongerThanThreeMinutes )) $ all_ ( invoiceLine chinookDb ) SELECT \"t0\" . \"InvoiceLineId\" AS \"res0\" , \"t0\" . \"InvoiceId\" AS \"res1\" , \"t0\" . \"TrackId\" AS \"res2\" , \"t0\" . \"UnitPrice\" AS \"res3\" , \"t0\" . \"Quantity\" AS \"res4\" FROM \"InvoiceLine\" AS \"t0\" WHERE (( \"t0\" . \"TrackId\" ) = ANY ( SELECT \"sub_t0\" . \"TrackId\" AS \"res0\" FROM \"Track\" AS \"sub_t0\" WHERE ( \"sub_t0\" . \"Milliseconds\" ) >= ( 180000 ))) IS TRUE We can also supply a concrete list of values. For example to get everyone living in either Los Angeles or Manila: Haskell Postgres filter_ ( \\ c -> unknownAs_ False ( addressCity ( customerAddress c ) ==*. anyIn_ [ just_ \"Los Angeles\" , just_ \"Manila\" ])) $ all_ ( customer chinookDb ) SELECT \"t0\" . \"CustomerId\" AS \"res0\" , \"t0\" . \"FirstName\" AS \"res1\" , \"t0\" . \"LastName\" AS \"res2\" , \"t0\" . \"Company\" AS \"res3\" , \"t0\" . \"Address\" AS \"res4\" , \"t0\" . \"City\" AS \"res5\" , \"t0\" . \"State\" AS \"res6\" , \"t0\" . \"Country\" AS \"res7\" , \"t0\" . \"PostalCode\" AS \"res8\" , \"t0\" . \"Phone\" AS \"res9\" , \"t0\" . \"Fax\" AS \"res10\" , \"t0\" . \"Email\" AS \"res11\" , \"t0\" . \"SupportRepId\" AS \"res12\" FROM \"Customer\" AS \"t0\" WHERE (( \"t0\" . \"City\" ) = ANY ( VALUES ( 'Los Angeles' ), ( 'Manila' ))) IS TRUE The IN predicate You can also use in_ to use the common IN predicate. Haskell Postgres Sqlite limit_ 10 $ filter_ ( \\ customer -> customerFirstName customer ` in_ ` [ val_ \"Johannes\" , val_ \"Aaron\" , val_ \"Ellie\" ]) $ all_ ( customer chinookDb ) SELECT \"t0\" . \"CustomerId\" AS \"res0\" , \"t0\" . \"FirstName\" AS \"res1\" , \"t0\" . \"LastName\" AS \"res2\" , \"t0\" . \"Company\" AS \"res3\" , \"t0\" . \"Address\" AS \"res4\" , \"t0\" . \"City\" AS \"res5\" , \"t0\" . \"State\" AS \"res6\" , \"t0\" . \"Country\" AS \"res7\" , \"t0\" . \"PostalCode\" AS \"res8\" , \"t0\" . \"Phone\" AS \"res9\" , \"t0\" . \"Fax\" AS \"res10\" , \"t0\" . \"Email\" AS \"res11\" , \"t0\" . \"SupportRepId\" AS \"res12\" FROM \"Customer\" AS \"t0\" WHERE ( \"t0\" . \"FirstName\" ) IN ( 'Johannes' , 'Aaron' , 'Ellie' ) LIMIT 10 SELECT \"t0\" . \"CustomerId\" AS \"res0\" , \"t0\" . \"FirstName\" AS \"res1\" , \"t0\" . \"LastName\" AS \"res2\" , \"t0\" . \"Company\" AS \"res3\" , \"t0\" . \"Address\" AS \"res4\" , \"t0\" . \"City\" AS \"res5\" , \"t0\" . \"State\" AS \"res6\" , \"t0\" . \"Country\" AS \"res7\" , \"t0\" . \"PostalCode\" AS \"res8\" , \"t0\" . \"Phone\" AS \"res9\" , \"t0\" . \"Fax\" AS \"res10\" , \"t0\" . \"Email\" AS \"res11\" , \"t0\" . \"SupportRepId\" AS \"res12\" FROM \"Customer\" AS \"t0\" WHERE ( \"t0\" . \"FirstName\" ) IN ( ? , ? , ? ) LIMIT 10 ; -- With values: [SQLText \"Johannes\",SQLText \"Aaron\",SQLText \"Ellie\"] CASE .. WHEN .. ELSE .. statements The SQL CASE .. WHEN .. ELSE construct can be used to implement a multi-way if. The corresponding beam syntax is if_ [ cond1 ` then_ ` result1 , cond2 ` then_ ` result2 , ... ] ( else_ elseResult ) where cond<n> are QGenExpr of type Bool , and result1 , result2 , and elseResult are QGenExprs of the same type. Manipulating types with CAST Oftentimes, you want to cast data between two different types. SQL provides the CAST function for this purpose. Beam exposes this functionality through the cast_ function which takes an expression and a datatype. For example, to select all line items where the first digit of the quantity is 2: Haskell Postgres Sqlite filter_ ( \\ ln -> cast_ ( invoiceLineQuantity ln ) ( varchar Nothing ) ` like_ ` \"2%\" ) $ all_ ( invoiceLine chinookDb ) SELECT \"t0\" . \"InvoiceLineId\" AS \"res0\" , \"t0\" . \"InvoiceId\" AS \"res1\" , \"t0\" . \"TrackId\" AS \"res2\" , \"t0\" . \"UnitPrice\" AS \"res3\" , \"t0\" . \"Quantity\" AS \"res4\" FROM \"InvoiceLine\" AS \"t0\" WHERE ( CAST (( \"t0\" . \"Quantity\" ) AS VARCHAR )) LIKE ( '2%' ) SELECT \"t0\" . \"InvoiceLineId\" AS \"res0\" , \"t0\" . \"InvoiceId\" AS \"res1\" , \"t0\" . \"TrackId\" AS \"res2\" , \"t0\" . \"UnitPrice\" AS \"res3\" , \"t0\" . \"Quantity\" AS \"res4\" FROM \"InvoiceLine\" AS \"t0\" WHERE ( CAST (( \"t0\" . \"Quantity\" ) AS VARCHAR )) LIKE ( ? ); -- With values: [SQLText \"2%\"] Subqueries When a query is used in place of an expression it's called a subquery . A query has the type Q in beam, while a beam expression has the type QGenExpr . Therefore, when using subqueries in beam, a function is needed to convert a Q (query) into a QGenExpr (expression). This function is called subquery_ . Using subquery_ , a query can be used where an expression is expected. For example, suppose we wish to offer a discount on all \"short\" tracks, where a track is considered short if its duration is less than the average track duration for all tracks. This is achieved using an update in which the predicate contains a subquery that calculates the average track duration. Haskell Postgres Sqlite runUpdate $ update ( track chinookDb ) ( \\ track' -> trackUnitPrice track' <-. current_ ( trackUnitPrice track' ) / 2 ) ( \\ track' -> let avgTrackDuration = aggregate_ ( avg_ . trackMilliseconds ) ( all_ $ track chinookDb ) in just_ ( trackMilliseconds track' ) <. subquery_ avgTrackDuration ) UPDATE \"Track\" SET \"UnitPrice\" = ( \"UnitPrice\" ) / ( '2.0' ) WHERE ( \"Milliseconds\" ) < ( ( SELECT AVG ( \"t0\" . \"Milliseconds\" ) AS \"res0\" FROM \"Track\" AS \"t0\" )); UPDATE \"Track\" SET \"UnitPrice\" = ( \"UnitPrice\" ) / ( ? ) WHERE ( \"Milliseconds\" ) < ( ( SELECT AVG ( \"t0\" . \"Milliseconds\" ) AS \"res0\" FROM \"Track\" AS \"t0\" )); -- With values: [SQLText \"2.0\"]; SQL Functions and operators SQL construct SQL standard Beam equivalent Notes EXISTS (x) SQL92 exists_ x Here, x is any query (of type Q ) UNIQUE (x) SQL92 unique_ x See note for EXISTS (x) DISTINCT (x) SQL99 distinct_ x See note for EXISTS (x) SELECT .. FROM ... as an expression (subqueries) SQL92 subquery_ x x is an query (of type Q ) COALESCE(a, b, c, ...) SQL92 coalesce_ [a, b, c, ...] a , b , and c must be of type Maybe a . The result has type a a BETWEEN b AND c SQL92 between_ a b c a LIKE b SQL92 a `like_` b a and b should be string types a SIMILAR TO b SQL99 a `similarTo_` b See note for LIKE POSITION(x IN y) SQL92 position_ x y x and y should be string types CHAR_LENGTH(x) SQL92 charLength_ x OCTET_LENGTH(x) SQL92 octetLength_ x BIT_LENGTH(x) SQL92 bitLength_ x x must be of the beam-specific SqlBitString type x IS TRUE / x IS NOT TRUE SQL92 isTrue_ x / isNotTrue_ x x IS FALSE / x IS NOT FALSE SQL92 isFalse_ x / isNotFalse_ x x IS UNKNOWN / x IS NOT UNKNOWN SQL92 isUnknown_ x / isNotUnknown_ x NOT x SQL92 not_ x LOWER (x) SQL92 lower_ x UPPER (x) SQL92 upper_ x TRIM (x) SQL92 trim_ x My favorite operator / function isn't listed here! If your favorite operator or function is not provided here, first ask yourself if it is part of any SQL standard. If it is not, then check the backend you are using to see if it provides a corresponding construct. If the backend does not or if the function / operator you need is part of a SQL standard, please open an issue on GitHub. Alternatively, implement the construct yourself and send us a pull request! See the section on adding your own functions","title":"Expressions"},{"location":"user-guide/expressions/#typing","text":"The type of all SQL-level expressions is QGenExpr . See the query tutorial for more information. In many cases, you'd like to type the SQL-level result of an expression without having to give explicit types for the other QGenExpr parameters. You can do this with the as_ combinator and -XTypeApplications . The following code types the literal 1 as a Double . as_ @ Double 1 This is rarely needed, but there are a few cases where the beam types are too general for the compiler to meaningfully infer types.","title":"Typing"},{"location":"user-guide/expressions/#literals","text":"Integer literals can be constructed using fromIntegral in the Num typeclass. This means you can also just use a Haskell integer literal as a QGenExpr in any context. Rational literals can be constructed via fromRational in Rational . Regular Haskell rational literals will be automatically converted to QGenExprs . Text literals can be constructed via fromString in IsString . Again, Haskell string constants will automatically be converted to QGenExprs , although you may have to provide an explicit type, as different backends support different text types natively. All other literals can be constructed using the val_ function in SqlValable . This requires that there is an implementation of HasSqlValueSyntax (Sql92ExpressionValueSyntax syntax) x for the type x in the appropriate syntax for the QGenExpr . For example, to construct a value of type Vector Int32 in the beam-postgres backend. val_ ( V . fromList [ 1 , 2 , 3 :: Int32 ]) Explicit tables can be brought to the SQL value level by using val_ as well. For example, if you have an AddressT Identity named a , val_ a :: AddressT (QGenExpr context expr s) .","title":"Literals"},{"location":"user-guide/expressions/#utf-support","text":"All included beam backends play nicely with UTF. New backends should also support UTF, if they support syntaxes and deserializers for String or Text . Haskell Postgres Sqlite filter_ ( \\ s -> customerFirstName s ==. \"\u3042\u304d\u3089\" ) $ all_ ( customer chinookDb ) SELECT \"t0\" . \"CustomerId\" AS \"res0\" , \"t0\" . \"FirstName\" AS \"res1\" , \"t0\" . \"LastName\" AS \"res2\" , \"t0\" . \"Company\" AS \"res3\" , \"t0\" . \"Address\" AS \"res4\" , \"t0\" . \"City\" AS \"res5\" , \"t0\" . \"State\" AS \"res6\" , \"t0\" . \"Country\" AS \"res7\" , \"t0\" . \"PostalCode\" AS \"res8\" , \"t0\" . \"Phone\" AS \"res9\" , \"t0\" . \"Fax\" AS \"res10\" , \"t0\" . \"Email\" AS \"res11\" , \"t0\" . \"SupportRepId\" AS \"res12\" FROM \"Customer\" AS \"t0\" WHERE ( \"t0\" . \"FirstName\" ) = ( '\u3042\u304d\u3089' ) SELECT \"t0\" . \"CustomerId\" AS \"res0\" , \"t0\" . \"FirstName\" AS \"res1\" , \"t0\" . \"LastName\" AS \"res2\" , \"t0\" . \"Company\" AS \"res3\" , \"t0\" . \"Address\" AS \"res4\" , \"t0\" . \"City\" AS \"res5\" , \"t0\" . \"State\" AS \"res6\" , \"t0\" . \"Country\" AS \"res7\" , \"t0\" . \"PostalCode\" AS \"res8\" , \"t0\" . \"Phone\" AS \"res9\" , \"t0\" . \"Fax\" AS \"res10\" , \"t0\" . \"Email\" AS \"res11\" , \"t0\" . \"SupportRepId\" AS \"res12\" FROM \"Customer\" AS \"t0\" WHERE ( \"t0\" . \"FirstName\" ) = ( ? ); -- With values: [SQLText \"\\12354\\12365\\12425\"]","title":"UTF support"},{"location":"user-guide/expressions/#arithmetic","text":"Arithmetic operations that are part of the Fractional and Num classes can be used directly. For example, if a and b are QGenExpr s of the same type, then a + b is a QGenExpr of the same type. Because of the toInteger class method in Integral , QGenExpr s cannot implement Integral . Nevertheless, versions of div and mod are available as div_ and mod_ , respectively, having the corresponding type.","title":"Arithmetic"},{"location":"user-guide/expressions/#comparison","text":"SQL comparison is not as simple as you may think. NULL handling in particular actually makes things rather complicated. SQL comparison operators actually return a tri-state boolean , representing true, false, and unknown , which is the result when two nulls are compared. Boolean combinators ( AND and OR ) handle these values in different ways. Beam abstracts some of this difference away, if you ask it to.","title":"Comparison"},{"location":"user-guide/expressions/#haskell-like-comparisons","text":"Haskell provides much more reasonable equality between potentially optional values. For example, Nothing == Nothing always! SQL does not provide a similar guarantee. However, beam can emulate Haskell-like equality in SQL using the ==. operator. This uses a CASE .. WHEN .. statement or a special operator that properly handles NULL s in your given backend. Depending on your backend, this can severely impact performance, but it's 'correct'. For example, to find all customers living in Berlin: Haskell Postgres Sqlite filter_ ( \\ s -> addressCity ( customerAddress s ) ==. val_ ( Just \"Berlin\" )) $ all_ ( customer chinookDb ) SELECT \"t0\" . \"CustomerId\" AS \"res0\" , \"t0\" . \"FirstName\" AS \"res1\" , \"t0\" . \"LastName\" AS \"res2\" , \"t0\" . \"Company\" AS \"res3\" , \"t0\" . \"Address\" AS \"res4\" , \"t0\" . \"City\" AS \"res5\" , \"t0\" . \"State\" AS \"res6\" , \"t0\" . \"Country\" AS \"res7\" , \"t0\" . \"PostalCode\" AS \"res8\" , \"t0\" . \"Phone\" AS \"res9\" , \"t0\" . \"Fax\" AS \"res10\" , \"t0\" . \"Email\" AS \"res11\" , \"t0\" . \"SupportRepId\" AS \"res12\" FROM \"Customer\" AS \"t0\" WHERE ( \"t0\" . \"City\" ) IS NOT DISTINCT FROM ( 'Berlin' ) SELECT \"t0\" . \"CustomerId\" AS \"res0\" , \"t0\" . \"FirstName\" AS \"res1\" , \"t0\" . \"LastName\" AS \"res2\" , \"t0\" . \"Company\" AS \"res3\" , \"t0\" . \"Address\" AS \"res4\" , \"t0\" . \"City\" AS \"res5\" , \"t0\" . \"State\" AS \"res6\" , \"t0\" . \"Country\" AS \"res7\" , \"t0\" . \"PostalCode\" AS \"res8\" , \"t0\" . \"Phone\" AS \"res9\" , \"t0\" . \"Fax\" AS \"res10\" , \"t0\" . \"Email\" AS \"res11\" , \"t0\" . \"SupportRepId\" AS \"res12\" FROM \"Customer\" AS \"t0\" WHERE CASE WHEN (( \"t0\" . \"City\" ) IS NULL ) AND (( ? ) IS NULL ) THEN ? WHEN (( \"t0\" . \"City\" ) IS NULL ) OR (( ? ) IS NULL ) THEN ? ELSE ( \"t0\" . \"City\" ) = ( ? ) END ; -- With values: [SQLText \"Berlin\",SQLInteger 1,SQLText \"Berlin\",SQLInteger 0,SQLText \"Berlin\"] Notice that SQLite uses a CASE .. WHEN .. statement, while Postgres uses the IS NOT DISTINCT FROM operator. The inequality operator is named /=. , as expected. Note that both ==. and /=. return a SQL expression whose type is Bool .","title":"Haskell-like comparisons"},{"location":"user-guide/expressions/#sql-like-comparisons","text":"Beam also provides equality operators that act like their underlying SQL counterparts. These operators map most directly to the SQL = and <> operators, but they require you to explicitly handle the possibility of NULL s. These operators are named ==?. and /=?. respectively. Unlike ==. and /=. , these operators return an expression of type SqlBool . SqlBool is a type that can only be manipulated as part of a SQL expression, and cannot be serialized or deserialized to/from Haskell. You need to convert it to a Bool value explicitly in order to get the result or use it with more advanced operators, such as CASE .. WHEN .. . In SQL, you can handle potentially unknown comparisons using the IS TRUE , IS NOT TRUE , IS FALSE , IS NOT FALSE , IS UNKNOWN , and IS NOT UNKNOWN operators. These are provided as the beam functions isTrue_ , isNotTrue_ , etc. These each take a SQL expression of type SqlBool and return one of type Bool . For example, to join every employee and customer who live in the same city, but using SQL-like equality and making sure the comparison really is true (i.e., customers and employees who both have NULL cities will not be included). Haskell Postgres Sqlite do c <- all_ ( customer chinookDb ) e <- join_ ( employee chinookDb ) $ \\ e -> isTrue_ ( addressCity ( customerAddress c ) ==?. addressCity ( employeeAddress e )) pure ( c , e ) SELECT \"t0\" . \"CustomerId\" AS \"res0\" , \"t0\" . \"FirstName\" AS \"res1\" , \"t0\" . \"LastName\" AS \"res2\" , \"t0\" . \"Company\" AS \"res3\" , \"t0\" . \"Address\" AS \"res4\" , \"t0\" . \"City\" AS \"res5\" , \"t0\" . \"State\" AS \"res6\" , \"t0\" . \"Country\" AS \"res7\" , \"t0\" . \"PostalCode\" AS \"res8\" , \"t0\" . \"Phone\" AS \"res9\" , \"t0\" . \"Fax\" AS \"res10\" , \"t0\" . \"Email\" AS \"res11\" , \"t0\" . \"SupportRepId\" AS \"res12\" , \"t1\" . \"EmployeeId\" AS \"res13\" , \"t1\" . \"LastName\" AS \"res14\" , \"t1\" . \"FirstName\" AS \"res15\" , \"t1\" . \"Title\" AS \"res16\" , \"t1\" . \"ReportsTo\" AS \"res17\" , \"t1\" . \"BirthDate\" AS \"res18\" , \"t1\" . \"HireDate\" AS \"res19\" , \"t1\" . \"Address\" AS \"res20\" , \"t1\" . \"City\" AS \"res21\" , \"t1\" . \"State\" AS \"res22\" , \"t1\" . \"Country\" AS \"res23\" , \"t1\" . \"PostalCode\" AS \"res24\" , \"t1\" . \"Phone\" AS \"res25\" , \"t1\" . \"Fax\" AS \"res26\" , \"t1\" . \"Email\" AS \"res27\" FROM \"Customer\" AS \"t0\" INNER JOIN \"Employee\" AS \"t1\" ON (( \"t0\" . \"City\" ) = ( \"t1\" . \"City\" )) IS TRUE SELECT \"t0\" . \"CustomerId\" AS \"res0\" , \"t0\" . \"FirstName\" AS \"res1\" , \"t0\" . \"LastName\" AS \"res2\" , \"t0\" . \"Company\" AS \"res3\" , \"t0\" . \"Address\" AS \"res4\" , \"t0\" . \"City\" AS \"res5\" , \"t0\" . \"State\" AS \"res6\" , \"t0\" . \"Country\" AS \"res7\" , \"t0\" . \"PostalCode\" AS \"res8\" , \"t0\" . \"Phone\" AS \"res9\" , \"t0\" . \"Fax\" AS \"res10\" , \"t0\" . \"Email\" AS \"res11\" , \"t0\" . \"SupportRepId\" AS \"res12\" , \"t1\" . \"EmployeeId\" AS \"res13\" , \"t1\" . \"LastName\" AS \"res14\" , \"t1\" . \"FirstName\" AS \"res15\" , \"t1\" . \"Title\" AS \"res16\" , \"t1\" . \"ReportsTo\" AS \"res17\" , \"t1\" . \"BirthDate\" AS \"res18\" , \"t1\" . \"HireDate\" AS \"res19\" , \"t1\" . \"Address\" AS \"res20\" , \"t1\" . \"City\" AS \"res21\" , \"t1\" . \"State\" AS \"res22\" , \"t1\" . \"Country\" AS \"res23\" , \"t1\" . \"PostalCode\" AS \"res24\" , \"t1\" . \"Phone\" AS \"res25\" , \"t1\" . \"Fax\" AS \"res26\" , \"t1\" . \"Email\" AS \"res27\" FROM \"Customer\" AS \"t0\" INNER JOIN \"Employee\" AS \"t1\" ON (( \"t0\" . \"City\" ) = ( \"t1\" . \"City\" )) IS 1 ; -- With values: [] Thinking of which IS .. operator to use can be confusing. If you have a default value you'd like to return in the case of an unknown comparison, use the unknownAs_ function. For example, if we want to treat unknown values as True instead (i.e, we want customers and employees who both have NULL cities to be included) Haskell Postgres Sqlite do c <- all_ ( customer chinookDb ) e <- join_ ( employee chinookDb ) $ \\ e -> unknownAs_ True ( addressCity ( customerAddress c ) ==?. addressCity ( employeeAddress e )) pure ( c , e ) SELECT \"t0\" . \"CustomerId\" AS \"res0\" , \"t0\" . \"FirstName\" AS \"res1\" , \"t0\" . \"LastName\" AS \"res2\" , \"t0\" . \"Company\" AS \"res3\" , \"t0\" . \"Address\" AS \"res4\" , \"t0\" . \"City\" AS \"res5\" , \"t0\" . \"State\" AS \"res6\" , \"t0\" . \"Country\" AS \"res7\" , \"t0\" . \"PostalCode\" AS \"res8\" , \"t0\" . \"Phone\" AS \"res9\" , \"t0\" . \"Fax\" AS \"res10\" , \"t0\" . \"Email\" AS \"res11\" , \"t0\" . \"SupportRepId\" AS \"res12\" , \"t1\" . \"EmployeeId\" AS \"res13\" , \"t1\" . \"LastName\" AS \"res14\" , \"t1\" . \"FirstName\" AS \"res15\" , \"t1\" . \"Title\" AS \"res16\" , \"t1\" . \"ReportsTo\" AS \"res17\" , \"t1\" . \"BirthDate\" AS \"res18\" , \"t1\" . \"HireDate\" AS \"res19\" , \"t1\" . \"Address\" AS \"res20\" , \"t1\" . \"City\" AS \"res21\" , \"t1\" . \"State\" AS \"res22\" , \"t1\" . \"Country\" AS \"res23\" , \"t1\" . \"PostalCode\" AS \"res24\" , \"t1\" . \"Phone\" AS \"res25\" , \"t1\" . \"Fax\" AS \"res26\" , \"t1\" . \"Email\" AS \"res27\" FROM \"Customer\" AS \"t0\" INNER JOIN \"Employee\" AS \"t1\" ON (( \"t0\" . \"City\" ) = ( \"t1\" . \"City\" )) IS NOT FALSE SELECT \"t0\" . \"CustomerId\" AS \"res0\" , \"t0\" . \"FirstName\" AS \"res1\" , \"t0\" . \"LastName\" AS \"res2\" , \"t0\" . \"Company\" AS \"res3\" , \"t0\" . \"Address\" AS \"res4\" , \"t0\" . \"City\" AS \"res5\" , \"t0\" . \"State\" AS \"res6\" , \"t0\" . \"Country\" AS \"res7\" , \"t0\" . \"PostalCode\" AS \"res8\" , \"t0\" . \"Phone\" AS \"res9\" , \"t0\" . \"Fax\" AS \"res10\" , \"t0\" . \"Email\" AS \"res11\" , \"t0\" . \"SupportRepId\" AS \"res12\" , \"t1\" . \"EmployeeId\" AS \"res13\" , \"t1\" . \"LastName\" AS \"res14\" , \"t1\" . \"FirstName\" AS \"res15\" , \"t1\" . \"Title\" AS \"res16\" , \"t1\" . \"ReportsTo\" AS \"res17\" , \"t1\" . \"BirthDate\" AS \"res18\" , \"t1\" . \"HireDate\" AS \"res19\" , \"t1\" . \"Address\" AS \"res20\" , \"t1\" . \"City\" AS \"res21\" , \"t1\" . \"State\" AS \"res22\" , \"t1\" . \"Country\" AS \"res23\" , \"t1\" . \"PostalCode\" AS \"res24\" , \"t1\" . \"Phone\" AS \"res25\" , \"t1\" . \"Fax\" AS \"res26\" , \"t1\" . \"Email\" AS \"res27\" FROM \"Customer\" AS \"t0\" INNER JOIN \"Employee\" AS \"t1\" ON (( \"t0\" . \"City\" ) = ( \"t1\" . \"City\" )) IS NOT 0 ; -- With values: []","title":"SQL-like comparisons"},{"location":"user-guide/expressions/#quantified-comparison","text":"SQL also allows comparisons to be quantified . For example, the SQL expression a == ANY(b) evaluates to true only if one row of b is equal to a . Similarly, a > ALL(b) returns true if a > x for every x in b . These are also supported using the ==*. , /=*. , <*. , >*. , <=*. , and >=*. operators. Like their unquantified counterparts, these operators yield a QGenExpr of type Bool . Unlike the unquantified operators, the second argument of these operators is of type QQuantified . You can create a QQuantified from a QGenExpr by using the anyOf_/anyIn_ or allOf_/allIn_ functions, which correspond to the ANY and ALL syntax respectively. anyOf_ and allOf_ take Q expressions (representing a query) and anyIn_ and allIn_ take lists of expressions. Quantified comparisons are always performed according to SQL semantics, meaning that they return values of type SqlBOol . This is because proper NULL handling with quantified comparisons cannot be expressed in a reasonable way. Use the functions described in the section above . For example, to get all invoice lines containing tracks longer than 3 minutes: Haskell Postgres let tracksLongerThanThreeMinutes = fmap trackId $ filter_ ( \\ t -> trackMilliseconds t >=. 180000 ) $ all_ ( track chinookDb ) in filter_ ( \\ ln -> let TrackId lnTrackId = invoiceLineTrack ln in unknownAs_ False ( lnTrackId ==*. anyOf_ tracksLongerThanThreeMinutes )) $ all_ ( invoiceLine chinookDb ) SELECT \"t0\" . \"InvoiceLineId\" AS \"res0\" , \"t0\" . \"InvoiceId\" AS \"res1\" , \"t0\" . \"TrackId\" AS \"res2\" , \"t0\" . \"UnitPrice\" AS \"res3\" , \"t0\" . \"Quantity\" AS \"res4\" FROM \"InvoiceLine\" AS \"t0\" WHERE (( \"t0\" . \"TrackId\" ) = ANY ( SELECT \"sub_t0\" . \"TrackId\" AS \"res0\" FROM \"Track\" AS \"sub_t0\" WHERE ( \"sub_t0\" . \"Milliseconds\" ) >= ( 180000 ))) IS TRUE We can also supply a concrete list of values. For example to get everyone living in either Los Angeles or Manila: Haskell Postgres filter_ ( \\ c -> unknownAs_ False ( addressCity ( customerAddress c ) ==*. anyIn_ [ just_ \"Los Angeles\" , just_ \"Manila\" ])) $ all_ ( customer chinookDb ) SELECT \"t0\" . \"CustomerId\" AS \"res0\" , \"t0\" . \"FirstName\" AS \"res1\" , \"t0\" . \"LastName\" AS \"res2\" , \"t0\" . \"Company\" AS \"res3\" , \"t0\" . \"Address\" AS \"res4\" , \"t0\" . \"City\" AS \"res5\" , \"t0\" . \"State\" AS \"res6\" , \"t0\" . \"Country\" AS \"res7\" , \"t0\" . \"PostalCode\" AS \"res8\" , \"t0\" . \"Phone\" AS \"res9\" , \"t0\" . \"Fax\" AS \"res10\" , \"t0\" . \"Email\" AS \"res11\" , \"t0\" . \"SupportRepId\" AS \"res12\" FROM \"Customer\" AS \"t0\" WHERE (( \"t0\" . \"City\" ) = ANY ( VALUES ( 'Los Angeles' ), ( 'Manila' ))) IS TRUE","title":"Quantified comparison"},{"location":"user-guide/expressions/#the-in-predicate","text":"You can also use in_ to use the common IN predicate. Haskell Postgres Sqlite limit_ 10 $ filter_ ( \\ customer -> customerFirstName customer ` in_ ` [ val_ \"Johannes\" , val_ \"Aaron\" , val_ \"Ellie\" ]) $ all_ ( customer chinookDb ) SELECT \"t0\" . \"CustomerId\" AS \"res0\" , \"t0\" . \"FirstName\" AS \"res1\" , \"t0\" . \"LastName\" AS \"res2\" , \"t0\" . \"Company\" AS \"res3\" , \"t0\" . \"Address\" AS \"res4\" , \"t0\" . \"City\" AS \"res5\" , \"t0\" . \"State\" AS \"res6\" , \"t0\" . \"Country\" AS \"res7\" , \"t0\" . \"PostalCode\" AS \"res8\" , \"t0\" . \"Phone\" AS \"res9\" , \"t0\" . \"Fax\" AS \"res10\" , \"t0\" . \"Email\" AS \"res11\" , \"t0\" . \"SupportRepId\" AS \"res12\" FROM \"Customer\" AS \"t0\" WHERE ( \"t0\" . \"FirstName\" ) IN ( 'Johannes' , 'Aaron' , 'Ellie' ) LIMIT 10 SELECT \"t0\" . \"CustomerId\" AS \"res0\" , \"t0\" . \"FirstName\" AS \"res1\" , \"t0\" . \"LastName\" AS \"res2\" , \"t0\" . \"Company\" AS \"res3\" , \"t0\" . \"Address\" AS \"res4\" , \"t0\" . \"City\" AS \"res5\" , \"t0\" . \"State\" AS \"res6\" , \"t0\" . \"Country\" AS \"res7\" , \"t0\" . \"PostalCode\" AS \"res8\" , \"t0\" . \"Phone\" AS \"res9\" , \"t0\" . \"Fax\" AS \"res10\" , \"t0\" . \"Email\" AS \"res11\" , \"t0\" . \"SupportRepId\" AS \"res12\" FROM \"Customer\" AS \"t0\" WHERE ( \"t0\" . \"FirstName\" ) IN ( ? , ? , ? ) LIMIT 10 ; -- With values: [SQLText \"Johannes\",SQLText \"Aaron\",SQLText \"Ellie\"]","title":"The IN predicate"},{"location":"user-guide/expressions/#case-when-else-statements","text":"The SQL CASE .. WHEN .. ELSE construct can be used to implement a multi-way if. The corresponding beam syntax is if_ [ cond1 ` then_ ` result1 , cond2 ` then_ ` result2 , ... ] ( else_ elseResult ) where cond<n> are QGenExpr of type Bool , and result1 , result2 , and elseResult are QGenExprs of the same type.","title":"CASE .. WHEN .. ELSE .. statements"},{"location":"user-guide/expressions/#manipulating-types-with-cast","text":"Oftentimes, you want to cast data between two different types. SQL provides the CAST function for this purpose. Beam exposes this functionality through the cast_ function which takes an expression and a datatype. For example, to select all line items where the first digit of the quantity is 2: Haskell Postgres Sqlite filter_ ( \\ ln -> cast_ ( invoiceLineQuantity ln ) ( varchar Nothing ) ` like_ ` \"2%\" ) $ all_ ( invoiceLine chinookDb ) SELECT \"t0\" . \"InvoiceLineId\" AS \"res0\" , \"t0\" . \"InvoiceId\" AS \"res1\" , \"t0\" . \"TrackId\" AS \"res2\" , \"t0\" . \"UnitPrice\" AS \"res3\" , \"t0\" . \"Quantity\" AS \"res4\" FROM \"InvoiceLine\" AS \"t0\" WHERE ( CAST (( \"t0\" . \"Quantity\" ) AS VARCHAR )) LIKE ( '2%' ) SELECT \"t0\" . \"InvoiceLineId\" AS \"res0\" , \"t0\" . \"InvoiceId\" AS \"res1\" , \"t0\" . \"TrackId\" AS \"res2\" , \"t0\" . \"UnitPrice\" AS \"res3\" , \"t0\" . \"Quantity\" AS \"res4\" FROM \"InvoiceLine\" AS \"t0\" WHERE ( CAST (( \"t0\" . \"Quantity\" ) AS VARCHAR )) LIKE ( ? ); -- With values: [SQLText \"2%\"]","title":"Manipulating types with CAST"},{"location":"user-guide/expressions/#subqueries","text":"When a query is used in place of an expression it's called a subquery . A query has the type Q in beam, while a beam expression has the type QGenExpr . Therefore, when using subqueries in beam, a function is needed to convert a Q (query) into a QGenExpr (expression). This function is called subquery_ . Using subquery_ , a query can be used where an expression is expected. For example, suppose we wish to offer a discount on all \"short\" tracks, where a track is considered short if its duration is less than the average track duration for all tracks. This is achieved using an update in which the predicate contains a subquery that calculates the average track duration. Haskell Postgres Sqlite runUpdate $ update ( track chinookDb ) ( \\ track' -> trackUnitPrice track' <-. current_ ( trackUnitPrice track' ) / 2 ) ( \\ track' -> let avgTrackDuration = aggregate_ ( avg_ . trackMilliseconds ) ( all_ $ track chinookDb ) in just_ ( trackMilliseconds track' ) <. subquery_ avgTrackDuration ) UPDATE \"Track\" SET \"UnitPrice\" = ( \"UnitPrice\" ) / ( '2.0' ) WHERE ( \"Milliseconds\" ) < ( ( SELECT AVG ( \"t0\" . \"Milliseconds\" ) AS \"res0\" FROM \"Track\" AS \"t0\" )); UPDATE \"Track\" SET \"UnitPrice\" = ( \"UnitPrice\" ) / ( ? ) WHERE ( \"Milliseconds\" ) < ( ( SELECT AVG ( \"t0\" . \"Milliseconds\" ) AS \"res0\" FROM \"Track\" AS \"t0\" )); -- With values: [SQLText \"2.0\"];","title":"Subqueries"},{"location":"user-guide/expressions/#sql-functions-and-operators","text":"SQL construct SQL standard Beam equivalent Notes EXISTS (x) SQL92 exists_ x Here, x is any query (of type Q ) UNIQUE (x) SQL92 unique_ x See note for EXISTS (x) DISTINCT (x) SQL99 distinct_ x See note for EXISTS (x) SELECT .. FROM ... as an expression (subqueries) SQL92 subquery_ x x is an query (of type Q ) COALESCE(a, b, c, ...) SQL92 coalesce_ [a, b, c, ...] a , b , and c must be of type Maybe a . The result has type a a BETWEEN b AND c SQL92 between_ a b c a LIKE b SQL92 a `like_` b a and b should be string types a SIMILAR TO b SQL99 a `similarTo_` b See note for LIKE POSITION(x IN y) SQL92 position_ x y x and y should be string types CHAR_LENGTH(x) SQL92 charLength_ x OCTET_LENGTH(x) SQL92 octetLength_ x BIT_LENGTH(x) SQL92 bitLength_ x x must be of the beam-specific SqlBitString type x IS TRUE / x IS NOT TRUE SQL92 isTrue_ x / isNotTrue_ x x IS FALSE / x IS NOT FALSE SQL92 isFalse_ x / isNotFalse_ x x IS UNKNOWN / x IS NOT UNKNOWN SQL92 isUnknown_ x / isNotUnknown_ x NOT x SQL92 not_ x LOWER (x) SQL92 lower_ x UPPER (x) SQL92 upper_ x TRIM (x) SQL92 trim_ x","title":"SQL Functions and operators"},{"location":"user-guide/expressions/#my-favorite-operator-function-isnt-listed-here","text":"If your favorite operator or function is not provided here, first ask yourself if it is part of any SQL standard. If it is not, then check the backend you are using to see if it provides a corresponding construct. If the backend does not or if the function / operator you need is part of a SQL standard, please open an issue on GitHub. Alternatively, implement the construct yourself and send us a pull request! See the section on adding your own functions","title":"My favorite operator / function isn't listed here!"},{"location":"user-guide/extensibility/","text":"The beam-core library and respective backends strive to expose the full power of each underlying database. If a particular feature is missing, please feel free to file a bug report on the GitHub issue tracker. However, in the meantime, beam offers a few options to inject raw SQL into your queries. Of course, beam cannot predict types of expressions and queries that were not created with its combinators, so caveat emptor . Custom expressions If you'd like to write an expression that beam currently does not support, you can use the customExpr_ function. Your backend's syntax must implement the IsSqlCustomExpressionSyntax type class. customExpr_ takes a function of arity n and n arguments, which must all be QGenExpr s with the same thread parameter. The expressions may be from different contexts (i.e., you can pass an aggregate and scalar into the same customExpr_ ). The function supplied must return a string-like expression that it can build using provided IsString and Monoid instances. The type of the expression is opaque to the user. The function's arguments will have the same type as the return type. Thus, they can be embedded into the returned expression using mappend . The arguments will be properly parenthesized and can be inserted whole into the final expression. You will likely need to explicitly supply a result type using the as_ function. For example, below, we use customExpr_ to access the regr_intercept and regr_slope functions in postgres. Haskell Postgres aggregate_ ( \\ t -> ( as_ @ Double @ QAggregateContext $ customExpr_ ( \\ bytes ms -> \"regr_intercept(\" <> bytes <> \", \" <> ms <> \")\" ) ( trackBytes t ) ( trackMilliseconds t ) , as_ @ Double @ QAggregateContext $ customExpr_ ( \\ bytes ms -> \"regr_slope(\" <> bytes <> \", \" <> ms <> \")\" ) ( trackBytes t ) ( trackMilliseconds t ) )) $ all_ ( track chinookDb ) SELECT regr_intercept (( \"t0\" . \"Bytes\" ), ( \"t0\" . \"Milliseconds\" )) AS \"res0\" , regr_slope (( \"t0\" . \"Bytes\" ), ( \"t0\" . \"Milliseconds\" )) AS \"res1\" FROM \"Track\" AS \"t0\" Note Custom queries (i.e., embedding arbitrary expressions into Q ) is currently being planned, but not implemented.","title":"Custom queries"},{"location":"user-guide/extensibility/#custom-expressions","text":"If you'd like to write an expression that beam currently does not support, you can use the customExpr_ function. Your backend's syntax must implement the IsSqlCustomExpressionSyntax type class. customExpr_ takes a function of arity n and n arguments, which must all be QGenExpr s with the same thread parameter. The expressions may be from different contexts (i.e., you can pass an aggregate and scalar into the same customExpr_ ). The function supplied must return a string-like expression that it can build using provided IsString and Monoid instances. The type of the expression is opaque to the user. The function's arguments will have the same type as the return type. Thus, they can be embedded into the returned expression using mappend . The arguments will be properly parenthesized and can be inserted whole into the final expression. You will likely need to explicitly supply a result type using the as_ function. For example, below, we use customExpr_ to access the regr_intercept and regr_slope functions in postgres. Haskell Postgres aggregate_ ( \\ t -> ( as_ @ Double @ QAggregateContext $ customExpr_ ( \\ bytes ms -> \"regr_intercept(\" <> bytes <> \", \" <> ms <> \")\" ) ( trackBytes t ) ( trackMilliseconds t ) , as_ @ Double @ QAggregateContext $ customExpr_ ( \\ bytes ms -> \"regr_slope(\" <> bytes <> \", \" <> ms <> \")\" ) ( trackBytes t ) ( trackMilliseconds t ) )) $ all_ ( track chinookDb ) SELECT regr_intercept (( \"t0\" . \"Bytes\" ), ( \"t0\" . \"Milliseconds\" )) AS \"res0\" , regr_slope (( \"t0\" . \"Bytes\" ), ( \"t0\" . \"Milliseconds\" )) AS \"res1\" FROM \"Track\" AS \"t0\" Note Custom queries (i.e., embedding arbitrary expressions into Q ) is currently being planned, but not implemented.","title":"Custom expressions"},{"location":"user-guide/models/","text":"A beam model is any single-constructor Haskell record type parameterized by a type of kind * -> * . The model must have an instance of Generic , Beamable , and Table . Generic can be derived using the DeriveGeneric extension of GHC. Beamable must be given an empty instance declaration ( instance Beamable Tbl for a table of type Tbl ). Table is discussed next. Each field in the record type must either be a sub-table (another parameterized type with a Beamable instance) or an explicit column. A column is specified using the Columnar type family applied to the type's parameter and the underlying Haskell type of the field. The Table type class Table is a type class that must be instantiated for all types that you would like to use as a table. It has one associated data instance and one function. You must create a type to represent the primary key of the table. The primary key of a table Tbl is the associated data type PrimaryKey Tbl . Like Tbl , it takes one type parameter of kind * -> * . It must have only one constructor which can hold all fields in the primary key. The constructor need not be a record constructor (although it can be). You must also write a function primaryKey that takes an instance of Tbl (parameterized over any functor f ) and returns the associated PrimaryKey type. It is sometimes easiest to use the Applicative instance for r -> to write this function. For example, if tblField1 and tblField2 are part of the primary key, you can write instance Table Tbl where data PrimaryKey Tbl f = TblKey ( Columnar f .. ) ( Columnar f .. ) primaryKey t = TblKey ( tblField1 t ) ( tblField2 t ) more simply as instance Table Tbl where data PrimaryKey Tbl f = TblKey ( Columnar f .. ) ( Columnar f .. ) primaryKey = TblKey <$> tblField1 <*> tblField2 The Identity trick Beam table types are commonly suffixed by a T to indicate the name of the generic table type. Usually, a type synonym named by leaving out the T is defined by applying the table to Identity . Recall each field in the table is either another table or an application of Columnar to the type parameter. When the type is parameterized by Identity , every column is also parameterized by Identity . Columnar is a type family defined such that Columnar Identity x ~ x . Thus, when parameterized over Identity , every field in the table type takes on the underlying Haskell type. Suppose you have a table type ModelT and a type synonym type Model = ModelT Identity . Notice that deriving Show , Eq , and other standard Haskell type classes won't generally work for ModelT . However, you can use the standalone deriving mechanism to derive these instances for Model . data ModelT f = Model { .. } deriving ( Generic , Beamable ) -- deriving instance Show (ModelT f) -- Won't work because GHC won't get the constraints right type Model = ModelT Identity deriving instance Show Model deriving instance Eq Model deriving instance Ord Model Allowed data types Any data type can be used within a Columnar . Beam does no checking that a field can be used against a particular database when the data type is defined. Instead, type errors will occur when the table is being used as a query. For example, the following is allowed, even though many backends will not work with array data types. import qualified Data.Vector as V data ArrayTable f = ArrayTable { arrayTablePoints :: Columnar f ( V . Vector Int32 ) } deriving Generic You can construct values of type ArrayTable Identity and even write queries over it (relying on type inference to get the constraints right). However, if you attempt to solve the constraints over a database that doesn't support columns of type V.Vector Int32 , GHC will throw an error. Thus, it's important to understand the limits of your backend when deciding which types to use. In general, numeric, floating-point, and text types are well supported. Maybe types Optional fields (those that allow a SQL NULL ) can usually be given a Maybe type. However, you cannot use Maybe around an embedded table (you will be unable to instantiate Beamable ). Beam offers a way around this. Instead of embedding the table applied to the type parameter f , apply it to Nullable f . Columnar (Nullable f) a ~ Maybe (Columnar f a) for all a . Thus, this will make every column in the embedded table take on the corresponding Maybe type. Warning Nullable will nest Maybe s. That is Columnar (Nullable f) (Maybe a) ~ Maybe (Maybe a) . This is bad from a SQL perspective, since SQL has no concept of a nested optional type. Beam treats a Nothing at any 'layer' of the Maybe stack as a corresponding SQL NULL . When marshalling data back, a SQL NULL is read in as a top-level Nothing . The reasons for this misfeature is basically code simplicity. Fixing this is a top priority of future versions of beam. Column tags Above, we saw that applying Identity to a table type results in a type whose columns are the underlying Haskell type. Beam uses other column tags for querying and describing databases. Below is a table of common column tags and their meaning. Converting between tags Suppose you have a Beamable type parameterized over a tag f and needed one parameterized over a tag g . Given a function conv :: forall a. Columnar f a -> Columnar g a , you can use changeBeamRep to convert between the tables. There is one caveat however -- since Columnar is a type family, the type of conv is actually ambiguous. We need a way to carry the type of f , g , and a into the code. For this reason, conv must actually be written over the Columnar' (notice the tick) newtype . Columnar' is a newtype defined as such newtype Columnar' f a = Columnar' ( Columnar f a ) Notice that, unlike Columnar (a non-injective type family), Columnar' is a full type. The type of conv' :: forall a. Columnar' f a -> Columnar' g a is now unambiguous. You can easily use conv to implement conv' : conv' ( Columnar' a ) = Columnar' ( conv a ) The Beamable type class All beam tables, primary keys, and shared data fields must be instances of the Beamable class. You cannot override the methods of Beamable . Rather, they are derived using GHC's generics mechanism. Once you've declared your data type, you can simply write instance Beamable <your-type-name> to instantiate the correct Beamable instance for your type. The Table type class All Beamable data types that you want to include as a TableEntity in your database must be members of the Table type class. The Table type class defines one associated type family PrimaryKey and a function primaryKey that takes a table over an arbitrary column tag and produces that table's PrimaryKey . For example, if you have a model data PersonT f = Person { personEmail :: Columnar f Text , personFirstName :: Columnar f Text , personLastName :: Columnar f Text , personAge :: Columnar f Int32 } deriving ( Generic , Beamable ) and you want the personEmail field to form the primary key, you would define a Table instance as such instance Table PersonT where data PrimaryKey PersonT f = PersonKey ( Columnar f Text ) deriving ( Generic , Beamable ) primaryKey person = PersonKey <$> personEmail Tip Many people find it useful to use the Applicative instance for (->) a to write primaryKey . For example, we could have written the above primaryKey person = PersonKey (personFirstName person) (personLastName person) as primaryKey = PersonKey <$> personFirstName <*> personLastName . Tip Typing Columnar may become tiresome. Database.Beam also exports C as a type alias for Columnar , which may make writing models easier. Since C may cause name clashes, all examples are given using Columnar . Many also like defining type synonyms for their table and primary key types. For example, for the table PersonT above, a programmer may define. type Person = PersonT Identity type PersonKey = PrimaryKey PersonT Identity deriving instance Show Person ; deriving instance Eq Person deriving instance Show PersonKey ; deriving instance Eq PersonKey By convention, beam table types are suffixed with T to distinguish their type names from the same type parameterized over Identity (the 'regular' Haskell data type). What about tables without primary keys? Tables without primary keys are considered bad style. However, sometimes you need to use beam with a schema that you have no control over. To declare a table without a primary key, simply instantiate the Table class and set PrimaryKey tbl to a type with no fields. Then just produce this type in primaryKey . For example data BadT f = BadT { badFirstName :: C f Text , badLastName :: C f Text } deriving ( Generic , Beamable ) instance Beamable BadT instance Table BadT where data PrimaryKey BadT f = BadNoId deriving ( Generic , Beamable ) primaryKey _ = BadNoId Foreign references Foreign references are also easily supported in models by simply embedding the PrimaryKey of the referred to table directly in the parent. For example, suppose we want to create a new model representing a post by a user. data PostT f = Post { postId :: Columnar f ( SqlSerial Int32 ) , postPostedAt :: Columnar f LocalTime , postContent :: Columnar f Text , postPoster :: PrimaryKey PersonT f } deriving ( Generic , Beamable ) instance Table PostT where data PrimaryKey PostT f = PostId ( Columnar f ( SqlSerial Int32 )) deriving ( Generic , Beamable ) primaryKey = PostId . postId type Post = PostT Identity type PostId = PrimaryKey PostT Identity deriving instance Show Post ; deriving instance Eq Post deriving instance Show PostId ; deriving instance Eq PostId Nullable foreign references Above, any non-bottom value of type PostT Identity must carry a concrete value of PrimaryKey PersonT Identity . Sometimes, you may want to optionally include a foreign key. You can make a foreign key nullable by embedding the primary key and adding the Nullable column tag modifier. For example, to make the poster optional above. data PostT f = Post { postId :: Columnar f ( SqlSerial Int32 ) , postPostedAt :: Columnar f LocalTime , postContent :: Columnar f Text , postPoster :: PrimaryKey PersonT ( Nullable f ) } deriving ( Generic , Beamable ) More complicated relationships This is the extent of beam's support for defining models. Although similar packages in other languages provide support for declaring one-to-many, many-to-one, and many-to-many relationships, beam's focused is providing a direct mapping of relational database concepts to Haskell, not on abstracting away the complexities of database querying. Thus, beam does not use 'lazy-loading' or other tricks that obfuscate performance. Because of this, the bulk of the functionality dealing with different types of relations is found in the querying support, rather than in the model declarations. Also, notice that beam does not allow you to specify any kind of reference constraints between tables in your data types. This is because references are a property of the database, not a particular table schema. Such relationships can be defined using the beam-migrate package. Embedding Sometimes, we want to declare multiple models with fields in common. Beam allows you to simple embed such fields in common types and embed those directly into models. For example, in the Chinook example schema , we define the following structure for addresses. data AddressMixin f = Address { address :: Columnar f ( Maybe Text ) , addressCity :: Columnar f ( Maybe Text ) , addressState :: Columnar f ( Maybe Text ) , addressCountry :: Columnar f ( Maybe Text ) , addressPostalCode :: Columnar f ( Maybe Text ) } deriving ( Generic , Beamable ) type Address = AddressMixin Identity deriving instance Show ( AddressMixin Identity ) We can then use AddressMixin in our models. data EmployeeT f = Employee { employeeId :: Columnar f Int32 , employeeLastName :: Columnar f Text , employeeFirstName :: Columnar f Text , employeeTitle :: Columnar f ( Maybe Text ) , employeeReportsTo :: PrimaryKey EmployeeT ( Nullable f ) , employeeBirthDate :: Columnar f ( Maybe LocalTime ) , employeeHireDate :: Columnar f ( Maybe LocalTime ) , employeeAddress :: AddressMixin f , employeePhone :: Columnar f ( Maybe Text ) , employeeFax :: Columnar f ( Maybe Text ) , employeeEmail :: Columnar f ( Maybe Text ) } deriving ( Generic , Beamable ) -- ... data CustomerT f = Customer { customerId :: Columnar f Int32 , customerFirstName :: Columnar f Text , customerLastName :: Columnar f Text , customerCompany :: Columnar f ( Maybe Text ) , customerAddress :: AddressMixin f , customerPhone :: Columnar f ( Maybe Text ) , customerFax :: Columnar f ( Maybe Text ) , customerEmail :: Columnar f Text , customerSupportRep :: PrimaryKey EmployeeT ( Nullable f ) } deriving ( Generic , Beamable ) Defaults Based on your data type declarations, beam can already guess a lot about your tables. For example, it already assumes that the personFirstName field is accessible in SQL as first_name . This defaulting behavior makes it very easy to interact with typical databases. For the easiest user experience, it's best to follow beam's conventions for declaring models. In particular, the defaulting mechanisms rely on each table type declaring only one constructor which has fields named in the camelCase style. When defaulting the name of a table field or column, beam un-camelCases the field name (after dropping leading underscores) and drops the first word. The remaining words are joined with underscores. If there is only one component, it is not dropped. Trailing and internal underscores are preserved in the name and if the name consists solely of underscores, beam makes no changes. A summary of these rules is given in the table below. Haskell field name Beam defaulted column name personFirstName first_name _personLastName last_name name name first_name first_name _first_name first_name ___ (three underscores) ___ (no changes) Note that beam only uses lower case in field names. While typically case does not matter for SQL queries, beam always quotes identifiers. Many DBMS's are case-sensitive for quoted identifiers. Thus, queries can sometimes fail if your tables use mixtures of lower- and upper-case to distinguish between fields. For information on modifying the defaults, see the next section .","title":"Models"},{"location":"user-guide/models/#the-table-type-class","text":"Table is a type class that must be instantiated for all types that you would like to use as a table. It has one associated data instance and one function. You must create a type to represent the primary key of the table. The primary key of a table Tbl is the associated data type PrimaryKey Tbl . Like Tbl , it takes one type parameter of kind * -> * . It must have only one constructor which can hold all fields in the primary key. The constructor need not be a record constructor (although it can be). You must also write a function primaryKey that takes an instance of Tbl (parameterized over any functor f ) and returns the associated PrimaryKey type. It is sometimes easiest to use the Applicative instance for r -> to write this function. For example, if tblField1 and tblField2 are part of the primary key, you can write instance Table Tbl where data PrimaryKey Tbl f = TblKey ( Columnar f .. ) ( Columnar f .. ) primaryKey t = TblKey ( tblField1 t ) ( tblField2 t ) more simply as instance Table Tbl where data PrimaryKey Tbl f = TblKey ( Columnar f .. ) ( Columnar f .. ) primaryKey = TblKey <$> tblField1 <*> tblField2","title":"The Table type class"},{"location":"user-guide/models/#the-identity-trick","text":"Beam table types are commonly suffixed by a T to indicate the name of the generic table type. Usually, a type synonym named by leaving out the T is defined by applying the table to Identity . Recall each field in the table is either another table or an application of Columnar to the type parameter. When the type is parameterized by Identity , every column is also parameterized by Identity . Columnar is a type family defined such that Columnar Identity x ~ x . Thus, when parameterized over Identity , every field in the table type takes on the underlying Haskell type. Suppose you have a table type ModelT and a type synonym type Model = ModelT Identity . Notice that deriving Show , Eq , and other standard Haskell type classes won't generally work for ModelT . However, you can use the standalone deriving mechanism to derive these instances for Model . data ModelT f = Model { .. } deriving ( Generic , Beamable ) -- deriving instance Show (ModelT f) -- Won't work because GHC won't get the constraints right type Model = ModelT Identity deriving instance Show Model deriving instance Eq Model deriving instance Ord Model","title":"The Identity trick"},{"location":"user-guide/models/#allowed-data-types","text":"Any data type can be used within a Columnar . Beam does no checking that a field can be used against a particular database when the data type is defined. Instead, type errors will occur when the table is being used as a query. For example, the following is allowed, even though many backends will not work with array data types. import qualified Data.Vector as V data ArrayTable f = ArrayTable { arrayTablePoints :: Columnar f ( V . Vector Int32 ) } deriving Generic You can construct values of type ArrayTable Identity and even write queries over it (relying on type inference to get the constraints right). However, if you attempt to solve the constraints over a database that doesn't support columns of type V.Vector Int32 , GHC will throw an error. Thus, it's important to understand the limits of your backend when deciding which types to use. In general, numeric, floating-point, and text types are well supported.","title":"Allowed data types"},{"location":"user-guide/models/#maybe-types","text":"Optional fields (those that allow a SQL NULL ) can usually be given a Maybe type. However, you cannot use Maybe around an embedded table (you will be unable to instantiate Beamable ). Beam offers a way around this. Instead of embedding the table applied to the type parameter f , apply it to Nullable f . Columnar (Nullable f) a ~ Maybe (Columnar f a) for all a . Thus, this will make every column in the embedded table take on the corresponding Maybe type. Warning Nullable will nest Maybe s. That is Columnar (Nullable f) (Maybe a) ~ Maybe (Maybe a) . This is bad from a SQL perspective, since SQL has no concept of a nested optional type. Beam treats a Nothing at any 'layer' of the Maybe stack as a corresponding SQL NULL . When marshalling data back, a SQL NULL is read in as a top-level Nothing . The reasons for this misfeature is basically code simplicity. Fixing this is a top priority of future versions of beam.","title":"Maybe types"},{"location":"user-guide/models/#column-tags","text":"Above, we saw that applying Identity to a table type results in a type whose columns are the underlying Haskell type. Beam uses other column tags for querying and describing databases. Below is a table of common column tags and their meaning.","title":"Column tags"},{"location":"user-guide/models/#converting-between-tags","text":"Suppose you have a Beamable type parameterized over a tag f and needed one parameterized over a tag g . Given a function conv :: forall a. Columnar f a -> Columnar g a , you can use changeBeamRep to convert between the tables. There is one caveat however -- since Columnar is a type family, the type of conv is actually ambiguous. We need a way to carry the type of f , g , and a into the code. For this reason, conv must actually be written over the Columnar' (notice the tick) newtype . Columnar' is a newtype defined as such newtype Columnar' f a = Columnar' ( Columnar f a ) Notice that, unlike Columnar (a non-injective type family), Columnar' is a full type. The type of conv' :: forall a. Columnar' f a -> Columnar' g a is now unambiguous. You can easily use conv to implement conv' : conv' ( Columnar' a ) = Columnar' ( conv a )","title":"Converting between tags"},{"location":"user-guide/models/#the-beamable-type-class","text":"All beam tables, primary keys, and shared data fields must be instances of the Beamable class. You cannot override the methods of Beamable . Rather, they are derived using GHC's generics mechanism. Once you've declared your data type, you can simply write instance Beamable <your-type-name> to instantiate the correct Beamable instance for your type.","title":"The Beamable type class"},{"location":"user-guide/models/#the-table-type-class_1","text":"All Beamable data types that you want to include as a TableEntity in your database must be members of the Table type class. The Table type class defines one associated type family PrimaryKey and a function primaryKey that takes a table over an arbitrary column tag and produces that table's PrimaryKey . For example, if you have a model data PersonT f = Person { personEmail :: Columnar f Text , personFirstName :: Columnar f Text , personLastName :: Columnar f Text , personAge :: Columnar f Int32 } deriving ( Generic , Beamable ) and you want the personEmail field to form the primary key, you would define a Table instance as such instance Table PersonT where data PrimaryKey PersonT f = PersonKey ( Columnar f Text ) deriving ( Generic , Beamable ) primaryKey person = PersonKey <$> personEmail Tip Many people find it useful to use the Applicative instance for (->) a to write primaryKey . For example, we could have written the above primaryKey person = PersonKey (personFirstName person) (personLastName person) as primaryKey = PersonKey <$> personFirstName <*> personLastName . Tip Typing Columnar may become tiresome. Database.Beam also exports C as a type alias for Columnar , which may make writing models easier. Since C may cause name clashes, all examples are given using Columnar . Many also like defining type synonyms for their table and primary key types. For example, for the table PersonT above, a programmer may define. type Person = PersonT Identity type PersonKey = PrimaryKey PersonT Identity deriving instance Show Person ; deriving instance Eq Person deriving instance Show PersonKey ; deriving instance Eq PersonKey By convention, beam table types are suffixed with T to distinguish their type names from the same type parameterized over Identity (the 'regular' Haskell data type).","title":"The Table type class"},{"location":"user-guide/models/#what-about-tables-without-primary-keys","text":"Tables without primary keys are considered bad style. However, sometimes you need to use beam with a schema that you have no control over. To declare a table without a primary key, simply instantiate the Table class and set PrimaryKey tbl to a type with no fields. Then just produce this type in primaryKey . For example data BadT f = BadT { badFirstName :: C f Text , badLastName :: C f Text } deriving ( Generic , Beamable ) instance Beamable BadT instance Table BadT where data PrimaryKey BadT f = BadNoId deriving ( Generic , Beamable ) primaryKey _ = BadNoId","title":"What about tables without primary keys?"},{"location":"user-guide/models/#foreign-references","text":"Foreign references are also easily supported in models by simply embedding the PrimaryKey of the referred to table directly in the parent. For example, suppose we want to create a new model representing a post by a user. data PostT f = Post { postId :: Columnar f ( SqlSerial Int32 ) , postPostedAt :: Columnar f LocalTime , postContent :: Columnar f Text , postPoster :: PrimaryKey PersonT f } deriving ( Generic , Beamable ) instance Table PostT where data PrimaryKey PostT f = PostId ( Columnar f ( SqlSerial Int32 )) deriving ( Generic , Beamable ) primaryKey = PostId . postId type Post = PostT Identity type PostId = PrimaryKey PostT Identity deriving instance Show Post ; deriving instance Eq Post deriving instance Show PostId ; deriving instance Eq PostId","title":"Foreign references"},{"location":"user-guide/models/#nullable-foreign-references","text":"Above, any non-bottom value of type PostT Identity must carry a concrete value of PrimaryKey PersonT Identity . Sometimes, you may want to optionally include a foreign key. You can make a foreign key nullable by embedding the primary key and adding the Nullable column tag modifier. For example, to make the poster optional above. data PostT f = Post { postId :: Columnar f ( SqlSerial Int32 ) , postPostedAt :: Columnar f LocalTime , postContent :: Columnar f Text , postPoster :: PrimaryKey PersonT ( Nullable f ) } deriving ( Generic , Beamable )","title":"Nullable foreign references"},{"location":"user-guide/models/#more-complicated-relationships","text":"This is the extent of beam's support for defining models. Although similar packages in other languages provide support for declaring one-to-many, many-to-one, and many-to-many relationships, beam's focused is providing a direct mapping of relational database concepts to Haskell, not on abstracting away the complexities of database querying. Thus, beam does not use 'lazy-loading' or other tricks that obfuscate performance. Because of this, the bulk of the functionality dealing with different types of relations is found in the querying support, rather than in the model declarations. Also, notice that beam does not allow you to specify any kind of reference constraints between tables in your data types. This is because references are a property of the database, not a particular table schema. Such relationships can be defined using the beam-migrate package.","title":"More complicated relationships"},{"location":"user-guide/models/#embedding","text":"Sometimes, we want to declare multiple models with fields in common. Beam allows you to simple embed such fields in common types and embed those directly into models. For example, in the Chinook example schema , we define the following structure for addresses. data AddressMixin f = Address { address :: Columnar f ( Maybe Text ) , addressCity :: Columnar f ( Maybe Text ) , addressState :: Columnar f ( Maybe Text ) , addressCountry :: Columnar f ( Maybe Text ) , addressPostalCode :: Columnar f ( Maybe Text ) } deriving ( Generic , Beamable ) type Address = AddressMixin Identity deriving instance Show ( AddressMixin Identity ) We can then use AddressMixin in our models. data EmployeeT f = Employee { employeeId :: Columnar f Int32 , employeeLastName :: Columnar f Text , employeeFirstName :: Columnar f Text , employeeTitle :: Columnar f ( Maybe Text ) , employeeReportsTo :: PrimaryKey EmployeeT ( Nullable f ) , employeeBirthDate :: Columnar f ( Maybe LocalTime ) , employeeHireDate :: Columnar f ( Maybe LocalTime ) , employeeAddress :: AddressMixin f , employeePhone :: Columnar f ( Maybe Text ) , employeeFax :: Columnar f ( Maybe Text ) , employeeEmail :: Columnar f ( Maybe Text ) } deriving ( Generic , Beamable ) -- ... data CustomerT f = Customer { customerId :: Columnar f Int32 , customerFirstName :: Columnar f Text , customerLastName :: Columnar f Text , customerCompany :: Columnar f ( Maybe Text ) , customerAddress :: AddressMixin f , customerPhone :: Columnar f ( Maybe Text ) , customerFax :: Columnar f ( Maybe Text ) , customerEmail :: Columnar f Text , customerSupportRep :: PrimaryKey EmployeeT ( Nullable f ) } deriving ( Generic , Beamable )","title":"Embedding"},{"location":"user-guide/models/#defaults","text":"Based on your data type declarations, beam can already guess a lot about your tables. For example, it already assumes that the personFirstName field is accessible in SQL as first_name . This defaulting behavior makes it very easy to interact with typical databases. For the easiest user experience, it's best to follow beam's conventions for declaring models. In particular, the defaulting mechanisms rely on each table type declaring only one constructor which has fields named in the camelCase style. When defaulting the name of a table field or column, beam un-camelCases the field name (after dropping leading underscores) and drops the first word. The remaining words are joined with underscores. If there is only one component, it is not dropped. Trailing and internal underscores are preserved in the name and if the name consists solely of underscores, beam makes no changes. A summary of these rules is given in the table below. Haskell field name Beam defaulted column name personFirstName first_name _personLastName last_name name name first_name first_name _first_name first_name ___ (three underscores) ___ (no changes) Note that beam only uses lower case in field names. While typically case does not matter for SQL queries, beam always quotes identifiers. Many DBMS's are case-sensitive for quoted identifiers. Thus, queries can sometimes fail if your tables use mixtures of lower- and upper-case to distinguish between fields. For information on modifying the defaults, see the next section .","title":"Defaults"},{"location":"user-guide/backends/beam-postgres/","text":"The beam-postgres backend is the most feature complete SQL backend for beam. The Postgres RDBMS supports most of the standards beam follows, so you can usually expect most queries to simply work. Additionally, beam-postgres is part of the standard Beam distribution, and so upgrades are applied periodically, and new functions are added to achieve feature-parity with the latest Postgres stable Postgres-specific data types Postgres has several data types not available from beam-core . The beam-postgres library provides several types and functions to make working with these easier. The tsvector and tsquery types The tsvector and tsquery types form the basis of full-text search in Postgres. They correspond to the haskell types TsVector and TsQuery , which are just newtype-wrappers over ByteString . Haskell Postgres pure ( Pg . toTsVector ( Just Pg . english ) ( as_ @ String ( val_ \"The quick brown fox jumps over the lazy dog\" ))) SELECT to_tsvector ( 'english' , 'The quick brown fox jumps over the lazy dog' ) AS \"res0\" Postgres extensions SELECT locking clause Postgres allows you to explicitly lock rows retrieved during a select using the locking clause . Beam supports most of the Postgres locking clause. However, there are some invariants that are currently not checked at compile time. For example, Postgres does not allow locking clauses with queries that use UNION , EXCEPT , or INTERSECT or those with aggregates. Since all these queries have the same type in Beam, we cannot catch these errors at compile-time. Current guidance is to only use the locking clause in top-level queries that you know to be safe. The following example finds all customers living in Dublin, and requests a ROW SHARE lock for each row. This prevents concurrent updates from updating these rows until the current transaction is complete. Haskell Postgres Pg . lockingAllTablesFor_ Pg . PgSelectLockingStrengthShare Nothing $ filter_ ( \\ c -> fromMaybe_ \"\" ( addressCity ( customerAddress c )) ==. \"Dublin\" ) $ all_ ( customer chinookDb ) SELECT \"t0\" . \"CustomerId\" AS \"res0\" , \"t0\" . \"FirstName\" AS \"res1\" , \"t0\" . \"LastName\" AS \"res2\" , \"t0\" . \"Company\" AS \"res3\" , \"t0\" . \"Address\" AS \"res4\" , \"t0\" . \"City\" AS \"res5\" , \"t0\" . \"State\" AS \"res6\" , \"t0\" . \"Country\" AS \"res7\" , \"t0\" . \"PostalCode\" AS \"res8\" , \"t0\" . \"Phone\" AS \"res9\" , \"t0\" . \"Fax\" AS \"res10\" , \"t0\" . \"Email\" AS \"res11\" , \"t0\" . \"SupportRepId\" AS \"res12\" FROM \"Customer\" AS \"t0\" WHERE ( COALESCE ( \"t0\" . \"City\" , '' )) = ( 'Dublin' ) FOR SHARE Now, suppose we want to update these rows, so we'll want to lock them for an update. Haskell Postgres Pg . lockingAllTablesFor_ Pg . PgSelectLockingStrengthUpdate Nothing $ filter_ ( \\ c -> fromMaybe_ \"\" ( addressCity ( customerAddress c )) ==. \"Dublin\" ) $ all_ ( customer chinookDb ) SELECT \"t0\" . \"CustomerId\" AS \"res0\" , \"t0\" . \"FirstName\" AS \"res1\" , \"t0\" . \"LastName\" AS \"res2\" , \"t0\" . \"Company\" AS \"res3\" , \"t0\" . \"Address\" AS \"res4\" , \"t0\" . \"City\" AS \"res5\" , \"t0\" . \"State\" AS \"res6\" , \"t0\" . \"Country\" AS \"res7\" , \"t0\" . \"PostalCode\" AS \"res8\" , \"t0\" . \"Phone\" AS \"res9\" , \"t0\" . \"Fax\" AS \"res10\" , \"t0\" . \"Email\" AS \"res11\" , \"t0\" . \"SupportRepId\" AS \"res12\" FROM \"Customer\" AS \"t0\" WHERE ( COALESCE ( \"t0\" . \"City\" , '' )) = ( 'Dublin' ) FOR UPDATE However, because there may be a lot of customers in Dublin that we'd like to update, this may block for a long time. Perhaps, we'd only like to lock rows that aren't already locked. This is inconsistent in general, but we do not always care. Postgres offers the SKIP LOCKED clause for this Haskell Postgres Pg . lockingAllTablesFor_ Pg . PgSelectLockingStrengthUpdate ( Just Pg . PgSelectLockingOptionsSkipLocked ) $ filter_ ( \\ c -> fromMaybe_ \"\" ( addressCity ( customerAddress c )) ==. \"Dublin\" ) $ all_ ( customer chinookDb ) SELECT \"t0\" . \"CustomerId\" AS \"res0\" , \"t0\" . \"FirstName\" AS \"res1\" , \"t0\" . \"LastName\" AS \"res2\" , \"t0\" . \"Company\" AS \"res3\" , \"t0\" . \"Address\" AS \"res4\" , \"t0\" . \"City\" AS \"res5\" , \"t0\" . \"State\" AS \"res6\" , \"t0\" . \"Country\" AS \"res7\" , \"t0\" . \"PostalCode\" AS \"res8\" , \"t0\" . \"Phone\" AS \"res9\" , \"t0\" . \"Fax\" AS \"res10\" , \"t0\" . \"Email\" AS \"res11\" , \"t0\" . \"SupportRepId\" AS \"res12\" FROM \"Customer\" AS \"t0\" WHERE ( COALESCE ( \"t0\" . \"City\" , '' )) = ( 'Dublin' ) FOR UPDATE SKIP LOCKED Or, if we do care, and don't want to wait anyway, we can ask Postgres to fail early instead of blocking, using NO WAIT Haskell Postgres Pg . lockingAllTablesFor_ Pg . PgSelectLockingStrengthUpdate ( Just Pg . PgSelectLockingOptionsNoWait ) $ filter_ ( \\ c -> fromMaybe_ \"\" ( addressCity ( customerAddress c )) ==. \"Dublin\" ) $ all_ ( customer chinookDb ) SELECT \"t0\" . \"CustomerId\" AS \"res0\" , \"t0\" . \"FirstName\" AS \"res1\" , \"t0\" . \"LastName\" AS \"res2\" , \"t0\" . \"Company\" AS \"res3\" , \"t0\" . \"Address\" AS \"res4\" , \"t0\" . \"City\" AS \"res5\" , \"t0\" . \"State\" AS \"res6\" , \"t0\" . \"Country\" AS \"res7\" , \"t0\" . \"PostalCode\" AS \"res8\" , \"t0\" . \"Phone\" AS \"res9\" , \"t0\" . \"Fax\" AS \"res10\" , \"t0\" . \"Email\" AS \"res11\" , \"t0\" . \"SupportRepId\" AS \"res12\" FROM \"Customer\" AS \"t0\" WHERE ( COALESCE ( \"t0\" . \"City\" , '' )) = ( 'Dublin' ) FOR UPDATE NOWAIT We can also specify the locking clauses when JOIN ing. Suppose we want to get all customers who live in London and have a support rep who lives in Paris, and skipping rows that we can't lock. Haskell Postgres Pg . lockingAllTablesFor_ Pg . PgSelectLockingStrengthShare ( Just Pg . PgSelectLockingOptionsSkipLocked ) $ do customer <- filter_ ( \\ c -> fromMaybe_ \"\" ( addressCity ( customerAddress c )) ==. \"London\" ) $ all_ ( customer chinookDb ) employee <- join_ ( employee chinookDb ) ( \\ e -> fromMaybe_ \"\" ( addressCity ( employeeAddress e )) ==. \"Paris\" &&. just_ ( pk e ) ==. customerSupportRep customer ) pure ( customerFirstName customer , customerLastName customer , pk employee ) SELECT \"t0\" . \"FirstName\" AS \"res0\" , \"t0\" . \"LastName\" AS \"res1\" , \"t1\" . \"EmployeeId\" AS \"res2\" FROM \"Customer\" AS \"t0\" INNER JOIN \"Employee\" AS \"t1\" ON (( COALESCE ( \"t1\" . \"City\" , '' )) = ( 'Paris' )) AND (( \"t1\" . \"EmployeeId\" ) IS NOT DISTINCT FROM ( \"t0\" . \"SupportRepId\" )) WHERE ( COALESCE ( \"t0\" . \"City\" , '' )) = ( 'London' ) FOR SHARE SKIP LOCKED You may notice that this query will lock rows in both the customers and employees table. This may not be what you want. You can also specify which tables to lock by using the lockingFor_ function. This requires you to specify which locks you want to hold by returning them from your query. For example, to lock only the customers table Haskell Postgres Pg . lockingFor_ Pg . PgSelectLockingStrengthShare ( Just Pg . PgSelectLockingOptionsSkipLocked ) $ do ( customerLock , customer ) <- Pg . locked_ ( customer chinookDb ) guard_ ( fromMaybe_ \"\" ( addressCity ( customerAddress customer )) ==. \"London\" ) employee <- filter_ ( \\ e -> fromMaybe_ \"\" ( addressCity ( employeeAddress e )) ==. \"Paris\" &&. just_ ( pk e ) ==. customerSupportRep customer ) $ all_ ( employee chinookDb ) pure (( customerFirstName customer , customerLastName customer , pk employee ) ` Pg . withLocks_ ` customerLock ) SELECT \"t0\" . \"FirstName\" AS \"res0\" , \"t0\" . \"LastName\" AS \"res1\" , \"t1\" . \"EmployeeId\" AS \"res2\" FROM \"Customer\" AS \"t0\" CROSS JOIN \"Employee\" AS \"t1\" WHERE (( COALESCE ( \"t0\" . \"City\" , '' )) = ( 'London' )) AND ((( COALESCE ( \"t1\" . \"City\" , '' )) = ( 'Paris' )) AND (( \"t1\" . \"EmployeeId\" ) IS NOT DISTINCT FROM ( \"t0\" . \"SupportRepId\" ))) FOR SHARE OF \"t0\" SKIP LOCKED In order to use the explicit locking clause, you need to use the locked_ function to get a reference to a lock for a particular table. This forces the locked table to be part of the join, which is a requirement for the Postgres locking clause. You can think of locked_ as exactly like all_ , except it returns a table lock as the first return value. Tip Locks can be combined monoidally, using mappend or (<>) . You can use this to lock multiple tables, by passing the result of mappend to withLocks_ . If you return mempty as the first argument, then this recovers the standard behavior of locking all tables. lockingFor_ is the most general locking combinator. You can recover the same behavior as lockingAllTablesFor_ by using the lockAll_ function. Haskell Postgres Pg . lockingFor_ Pg . PgSelectLockingStrengthShare ( Just Pg . PgSelectLockingOptionsSkipLocked ) $ do ( customerLock , customer ) <- Pg . locked_ ( customer chinookDb ) guard_ ( fromMaybe_ \"\" ( addressCity ( customerAddress customer )) ==. \"London\" ) employee <- filter_ ( \\ e -> fromMaybe_ \"\" ( addressCity ( employeeAddress e )) ==. \"Paris\" &&. just_ ( pk e ) ==. customerSupportRep customer ) $ all_ ( employee chinookDb ) pure ( Pg . lockAll_ ( customerFirstName customer , customerLastName customer , pk employee )) SELECT \"t0\" . \"FirstName\" AS \"res0\" , \"t0\" . \"LastName\" AS \"res1\" , \"t1\" . \"EmployeeId\" AS \"res2\" FROM \"Customer\" AS \"t0\" CROSS JOIN \"Employee\" AS \"t1\" WHERE (( COALESCE ( \"t0\" . \"City\" , '' )) = ( 'London' )) AND ((( COALESCE ( \"t1\" . \"City\" , '' )) = ( 'Paris' )) AND (( \"t1\" . \"EmployeeId\" ) IS NOT DISTINCT FROM ( \"t0\" . \"SupportRepId\" ))) FOR SHARE SKIP LOCKED Tip Table locks have the type PgLockedTables s , where s is the thread parameter, as described here DISTINCT ON support Postgres supports the DISTINCT ON clause with selects to return distinct results based on a particular key. The beam-postgres package provides the pgNubBy_ function to use this feature. For example, to get an arbitrary customer from each distinct area code Haskell Postgres Pg . pgNubBy_ ( addressPostalCode . customerAddress ) $ all_ ( customer chinookDb ) SELECT DISTINCT ON ( \"t0\" . \"PostalCode\" ) \"t0\" . \"CustomerId\" AS \"res0\" , \"t0\" . \"FirstName\" AS \"res1\" , \"t0\" . \"LastName\" AS \"res2\" , \"t0\" . \"Company\" AS \"res3\" , \"t0\" . \"Address\" AS \"res4\" , \"t0\" . \"City\" AS \"res5\" , \"t0\" . \"State\" AS \"res6\" , \"t0\" . \"Country\" AS \"res7\" , \"t0\" . \"PostalCode\" AS \"res8\" , \"t0\" . \"Phone\" AS \"res9\" , \"t0\" . \"Fax\" AS \"res10\" , \"t0\" . \"Email\" AS \"res11\" , \"t0\" . \"SupportRepId\" AS \"res12\" FROM \"Customer\" AS \"t0\" Aggregates string_agg The Postgres string_agg aggregate combines all column values in a group separated by a given separator. beam-postgres provides pgStringAgg and pgStringAggOver to use the unquantified and quantified versions of the string_agg aggregate appropriately. For example, to put together a list of all cities in all the postal codes we have for customers, Haskell Postgres aggregate_ ( \\ c -> ( group_ ( addressPostalCode ( customerAddress c )) , Pg . pgStringAgg ( coalesce_ [ addressCity ( customerAddress c )] \"\" ) \",\" ) ) $ all_ ( customer chinookDb ) SELECT \"t0\" . \"PostalCode\" AS \"res0\" , string_agg ( COALESCE ( \"t0\" . \"City\" , '' ), ',' ) AS \"res1\" FROM \"Customer\" AS \"t0\" GROUP BY \"t0\" . \"PostalCode\" The above will include one city multiple times if its shared by multiple customers. Haskell Postgres aggregate_ ( \\ c -> ( group_ ( addressPostalCode ( customerAddress c )) , Pg . pgStringAggOver distinctInGroup_ ( coalesce_ [ addressCity ( customerAddress c )] \"\" ) \",\" ) ) $ all_ ( customer chinookDb ) SELECT \"t0\" . \"PostalCode\" AS \"res0\" , string_agg ( DISTINCT COALESCE ( \"t0\" . \"City\" , '' ), ',' ) AS \"res1\" FROM \"Customer\" AS \"t0\" GROUP BY \"t0\" . \"PostalCode\" ON CONFLICT Postgres supports targeting a particular constraint as the target of an ON CONFLICT clause. You can use conflictingConstraint with the name of the constraint with the regular insertOnConflict function to use this functionality. For example, to update the row, only on conflicts relating to the \"PK_CUSTOMER\" constraint. Haskell Postgres --! import Database.Beam.Backend.SQL.BeamExtensions ( BeamHasInsertOnConflict ( .. )) --! import qualified Database.Beam.Postgres as Pg let newCustomer = Customer 42 \"John\" \"Doe\" Nothing ( Address ( Just \"Street\" ) ( Just \"City\" ) ( Just \"State\" ) Nothing Nothing ) Nothing Nothing \"john.doe@johndoe.com\" nothing_ runInsert $ insertOnConflict ( customer chinookDb ) ( insertValues [ newCustomer ]) ( Pg . conflictingConstraint \"PK_Customer\" ) ( onConflictUpdateSet ( \\ fields _ -> fields <-. val_ newCustomer )) INSERT INTO \"Customer\" ( \"CustomerId\" , \"FirstName\" , \"LastName\" , \"Company\" , \"Address\" , \"City\" , \"State\" , \"Country\" , \"PostalCode\" , \"Phone\" , \"Fax\" , \"Email\" , \"SupportRepId\" ) VALUES ( 42 , 'John' , 'Doe' , null , 'Street' , 'City' , 'State' , null , null , null , null , 'john.doe@johndoe.com' , null ) ON CONFLICT ON CONSTRAINT \"PK_Customer\" DO UPDATE SET \"CustomerId\" = ( 42 ), \"FirstName\" = ( 'John' ), \"LastName\" = ( 'Doe' ), \"Company\" = ( null ), \"Address\" = ( 'Street' ), \"City\" = ( 'City' ), \"State\" = ( 'State' ), \"Country\" = ( null ), \"PostalCode\" = ( null ), \"Phone\" = ( null ), \"Fax\" = ( null ), \"Email\" = ( 'john.doe@johndoe.com' ), \"SupportRepId\" = ( null ); Specifying actions Often times, you do not want to update every field on a conflict. For example, for upserts, you rarely want to update the primary key. The function onConflictUpdateInstead allows you to restrict which fields are updated in the case of a conflict. The required function argument is a projection of which fields ought to be updated. In the example below, we insert a new row, but if a row with the given primary key already exists, we update only the first and last name. Haskell Postgres -- import qualified Database.Beam.Postgres as Pg let newCustomer = Customer 42 \"John\" \"Doe\" Nothing ( Address ( Just \"Street\" ) ( Just \"City\" ) ( Just \"State\" ) Nothing Nothing ) Nothing Nothing \"john.doe@johndoe.com\" nothing_ runInsert $ Pg . insert ( customer chinookDb ) ( insertValues [ newCustomer ]) $ Pg . onConflict ( Pg . conflictingFields primaryKey ) ( Pg . onConflictUpdateInstead ( \\ c -> ( customerFirstName c , customerLastName c ))) INSERT INTO \"Customer\" ( \"CustomerId\" , \"FirstName\" , \"LastName\" , \"Company\" , \"Address\" , \"City\" , \"State\" , \"Country\" , \"PostalCode\" , \"Phone\" , \"Fax\" , \"Email\" , \"SupportRepId\" ) VALUES ( 42 , 'John' , 'Doe' , null , 'Street' , 'City' , 'State' , null , null , null , null , 'john.doe@johndoe.com' , null ) ON CONFLICT ( \"CustomerId\" ) DO UPDATE SET \"FirstName\" = ( \"excluded\" . \"FirstName\" ), \"LastName\" = ( \"excluded\" . \"LastName\" ); You can also specify a more specific update, using the onConflictUpdateSet function. This is the most general form of the postgres ON CONFLICT action. The excluded table is provided as the second argument. The syntax of the updates is similar to that of update . In the following example, we append the old first name to the new first name and replace the old last name. Haskell Postgres -- import qualified Database.Beam.Postgres as Pg let newCustomer = Customer 42 \"John\" \"Doe\" Nothing ( Address ( Just \"Street\" ) ( Just \"City\" ) ( Just \"State\" ) Nothing Nothing ) Nothing Nothing \"john.doe@johndoe.com\" nothing_ runInsert $ Pg . insert ( customer chinookDb ) ( insertValues [ newCustomer ]) $ Pg . onConflict ( Pg . conflictingFields primaryKey ) ( Pg . onConflictUpdateSet -- tbl is the old row, tblExcluded is the row proposed for insertion ( \\ tbl tblExcluded -> mconcat [ customerFirstName tbl <-. concat_ [ current_ ( customerFirstName tbl ), customerFirstName tblExcluded ] , customerLastName tbl <-. customerLastName tblExcluded ] ) ) INSERT INTO \"Customer\" ( \"CustomerId\" , \"FirstName\" , \"LastName\" , \"Company\" , \"Address\" , \"City\" , \"State\" , \"Country\" , \"PostalCode\" , \"Phone\" , \"Fax\" , \"Email\" , \"SupportRepId\" ) VALUES ( 42 , 'John' , 'Doe' , null , 'Street' , 'City' , 'State' , null , null , null , null , 'john.doe@johndoe.com' , null ) ON CONFLICT ( \"CustomerId\" ) DO UPDATE SET \"FirstName\" = ( CONCAT ( \"Customer\" . \"FirstName\" , \"excluded\" . \"FirstName\" )), \"LastName\" = ( \"excluded\" . \"LastName\" );","title":"beam-postgres"},{"location":"user-guide/backends/beam-postgres/#postgres-specific-data-types","text":"Postgres has several data types not available from beam-core . The beam-postgres library provides several types and functions to make working with these easier.","title":"Postgres-specific data types"},{"location":"user-guide/backends/beam-postgres/#the-tsvector-and-tsquery-types","text":"The tsvector and tsquery types form the basis of full-text search in Postgres. They correspond to the haskell types TsVector and TsQuery , which are just newtype-wrappers over ByteString . Haskell Postgres pure ( Pg . toTsVector ( Just Pg . english ) ( as_ @ String ( val_ \"The quick brown fox jumps over the lazy dog\" ))) SELECT to_tsvector ( 'english' , 'The quick brown fox jumps over the lazy dog' ) AS \"res0\"","title":"The tsvector and tsquery types"},{"location":"user-guide/backends/beam-postgres/#postgres-extensions","text":"","title":"Postgres extensions"},{"location":"user-guide/backends/beam-postgres/#select-locking-clause","text":"Postgres allows you to explicitly lock rows retrieved during a select using the locking clause . Beam supports most of the Postgres locking clause. However, there are some invariants that are currently not checked at compile time. For example, Postgres does not allow locking clauses with queries that use UNION , EXCEPT , or INTERSECT or those with aggregates. Since all these queries have the same type in Beam, we cannot catch these errors at compile-time. Current guidance is to only use the locking clause in top-level queries that you know to be safe. The following example finds all customers living in Dublin, and requests a ROW SHARE lock for each row. This prevents concurrent updates from updating these rows until the current transaction is complete. Haskell Postgres Pg . lockingAllTablesFor_ Pg . PgSelectLockingStrengthShare Nothing $ filter_ ( \\ c -> fromMaybe_ \"\" ( addressCity ( customerAddress c )) ==. \"Dublin\" ) $ all_ ( customer chinookDb ) SELECT \"t0\" . \"CustomerId\" AS \"res0\" , \"t0\" . \"FirstName\" AS \"res1\" , \"t0\" . \"LastName\" AS \"res2\" , \"t0\" . \"Company\" AS \"res3\" , \"t0\" . \"Address\" AS \"res4\" , \"t0\" . \"City\" AS \"res5\" , \"t0\" . \"State\" AS \"res6\" , \"t0\" . \"Country\" AS \"res7\" , \"t0\" . \"PostalCode\" AS \"res8\" , \"t0\" . \"Phone\" AS \"res9\" , \"t0\" . \"Fax\" AS \"res10\" , \"t0\" . \"Email\" AS \"res11\" , \"t0\" . \"SupportRepId\" AS \"res12\" FROM \"Customer\" AS \"t0\" WHERE ( COALESCE ( \"t0\" . \"City\" , '' )) = ( 'Dublin' ) FOR SHARE Now, suppose we want to update these rows, so we'll want to lock them for an update. Haskell Postgres Pg . lockingAllTablesFor_ Pg . PgSelectLockingStrengthUpdate Nothing $ filter_ ( \\ c -> fromMaybe_ \"\" ( addressCity ( customerAddress c )) ==. \"Dublin\" ) $ all_ ( customer chinookDb ) SELECT \"t0\" . \"CustomerId\" AS \"res0\" , \"t0\" . \"FirstName\" AS \"res1\" , \"t0\" . \"LastName\" AS \"res2\" , \"t0\" . \"Company\" AS \"res3\" , \"t0\" . \"Address\" AS \"res4\" , \"t0\" . \"City\" AS \"res5\" , \"t0\" . \"State\" AS \"res6\" , \"t0\" . \"Country\" AS \"res7\" , \"t0\" . \"PostalCode\" AS \"res8\" , \"t0\" . \"Phone\" AS \"res9\" , \"t0\" . \"Fax\" AS \"res10\" , \"t0\" . \"Email\" AS \"res11\" , \"t0\" . \"SupportRepId\" AS \"res12\" FROM \"Customer\" AS \"t0\" WHERE ( COALESCE ( \"t0\" . \"City\" , '' )) = ( 'Dublin' ) FOR UPDATE However, because there may be a lot of customers in Dublin that we'd like to update, this may block for a long time. Perhaps, we'd only like to lock rows that aren't already locked. This is inconsistent in general, but we do not always care. Postgres offers the SKIP LOCKED clause for this Haskell Postgres Pg . lockingAllTablesFor_ Pg . PgSelectLockingStrengthUpdate ( Just Pg . PgSelectLockingOptionsSkipLocked ) $ filter_ ( \\ c -> fromMaybe_ \"\" ( addressCity ( customerAddress c )) ==. \"Dublin\" ) $ all_ ( customer chinookDb ) SELECT \"t0\" . \"CustomerId\" AS \"res0\" , \"t0\" . \"FirstName\" AS \"res1\" , \"t0\" . \"LastName\" AS \"res2\" , \"t0\" . \"Company\" AS \"res3\" , \"t0\" . \"Address\" AS \"res4\" , \"t0\" . \"City\" AS \"res5\" , \"t0\" . \"State\" AS \"res6\" , \"t0\" . \"Country\" AS \"res7\" , \"t0\" . \"PostalCode\" AS \"res8\" , \"t0\" . \"Phone\" AS \"res9\" , \"t0\" . \"Fax\" AS \"res10\" , \"t0\" . \"Email\" AS \"res11\" , \"t0\" . \"SupportRepId\" AS \"res12\" FROM \"Customer\" AS \"t0\" WHERE ( COALESCE ( \"t0\" . \"City\" , '' )) = ( 'Dublin' ) FOR UPDATE SKIP LOCKED Or, if we do care, and don't want to wait anyway, we can ask Postgres to fail early instead of blocking, using NO WAIT Haskell Postgres Pg . lockingAllTablesFor_ Pg . PgSelectLockingStrengthUpdate ( Just Pg . PgSelectLockingOptionsNoWait ) $ filter_ ( \\ c -> fromMaybe_ \"\" ( addressCity ( customerAddress c )) ==. \"Dublin\" ) $ all_ ( customer chinookDb ) SELECT \"t0\" . \"CustomerId\" AS \"res0\" , \"t0\" . \"FirstName\" AS \"res1\" , \"t0\" . \"LastName\" AS \"res2\" , \"t0\" . \"Company\" AS \"res3\" , \"t0\" . \"Address\" AS \"res4\" , \"t0\" . \"City\" AS \"res5\" , \"t0\" . \"State\" AS \"res6\" , \"t0\" . \"Country\" AS \"res7\" , \"t0\" . \"PostalCode\" AS \"res8\" , \"t0\" . \"Phone\" AS \"res9\" , \"t0\" . \"Fax\" AS \"res10\" , \"t0\" . \"Email\" AS \"res11\" , \"t0\" . \"SupportRepId\" AS \"res12\" FROM \"Customer\" AS \"t0\" WHERE ( COALESCE ( \"t0\" . \"City\" , '' )) = ( 'Dublin' ) FOR UPDATE NOWAIT We can also specify the locking clauses when JOIN ing. Suppose we want to get all customers who live in London and have a support rep who lives in Paris, and skipping rows that we can't lock. Haskell Postgres Pg . lockingAllTablesFor_ Pg . PgSelectLockingStrengthShare ( Just Pg . PgSelectLockingOptionsSkipLocked ) $ do customer <- filter_ ( \\ c -> fromMaybe_ \"\" ( addressCity ( customerAddress c )) ==. \"London\" ) $ all_ ( customer chinookDb ) employee <- join_ ( employee chinookDb ) ( \\ e -> fromMaybe_ \"\" ( addressCity ( employeeAddress e )) ==. \"Paris\" &&. just_ ( pk e ) ==. customerSupportRep customer ) pure ( customerFirstName customer , customerLastName customer , pk employee ) SELECT \"t0\" . \"FirstName\" AS \"res0\" , \"t0\" . \"LastName\" AS \"res1\" , \"t1\" . \"EmployeeId\" AS \"res2\" FROM \"Customer\" AS \"t0\" INNER JOIN \"Employee\" AS \"t1\" ON (( COALESCE ( \"t1\" . \"City\" , '' )) = ( 'Paris' )) AND (( \"t1\" . \"EmployeeId\" ) IS NOT DISTINCT FROM ( \"t0\" . \"SupportRepId\" )) WHERE ( COALESCE ( \"t0\" . \"City\" , '' )) = ( 'London' ) FOR SHARE SKIP LOCKED You may notice that this query will lock rows in both the customers and employees table. This may not be what you want. You can also specify which tables to lock by using the lockingFor_ function. This requires you to specify which locks you want to hold by returning them from your query. For example, to lock only the customers table Haskell Postgres Pg . lockingFor_ Pg . PgSelectLockingStrengthShare ( Just Pg . PgSelectLockingOptionsSkipLocked ) $ do ( customerLock , customer ) <- Pg . locked_ ( customer chinookDb ) guard_ ( fromMaybe_ \"\" ( addressCity ( customerAddress customer )) ==. \"London\" ) employee <- filter_ ( \\ e -> fromMaybe_ \"\" ( addressCity ( employeeAddress e )) ==. \"Paris\" &&. just_ ( pk e ) ==. customerSupportRep customer ) $ all_ ( employee chinookDb ) pure (( customerFirstName customer , customerLastName customer , pk employee ) ` Pg . withLocks_ ` customerLock ) SELECT \"t0\" . \"FirstName\" AS \"res0\" , \"t0\" . \"LastName\" AS \"res1\" , \"t1\" . \"EmployeeId\" AS \"res2\" FROM \"Customer\" AS \"t0\" CROSS JOIN \"Employee\" AS \"t1\" WHERE (( COALESCE ( \"t0\" . \"City\" , '' )) = ( 'London' )) AND ((( COALESCE ( \"t1\" . \"City\" , '' )) = ( 'Paris' )) AND (( \"t1\" . \"EmployeeId\" ) IS NOT DISTINCT FROM ( \"t0\" . \"SupportRepId\" ))) FOR SHARE OF \"t0\" SKIP LOCKED In order to use the explicit locking clause, you need to use the locked_ function to get a reference to a lock for a particular table. This forces the locked table to be part of the join, which is a requirement for the Postgres locking clause. You can think of locked_ as exactly like all_ , except it returns a table lock as the first return value. Tip Locks can be combined monoidally, using mappend or (<>) . You can use this to lock multiple tables, by passing the result of mappend to withLocks_ . If you return mempty as the first argument, then this recovers the standard behavior of locking all tables. lockingFor_ is the most general locking combinator. You can recover the same behavior as lockingAllTablesFor_ by using the lockAll_ function. Haskell Postgres Pg . lockingFor_ Pg . PgSelectLockingStrengthShare ( Just Pg . PgSelectLockingOptionsSkipLocked ) $ do ( customerLock , customer ) <- Pg . locked_ ( customer chinookDb ) guard_ ( fromMaybe_ \"\" ( addressCity ( customerAddress customer )) ==. \"London\" ) employee <- filter_ ( \\ e -> fromMaybe_ \"\" ( addressCity ( employeeAddress e )) ==. \"Paris\" &&. just_ ( pk e ) ==. customerSupportRep customer ) $ all_ ( employee chinookDb ) pure ( Pg . lockAll_ ( customerFirstName customer , customerLastName customer , pk employee )) SELECT \"t0\" . \"FirstName\" AS \"res0\" , \"t0\" . \"LastName\" AS \"res1\" , \"t1\" . \"EmployeeId\" AS \"res2\" FROM \"Customer\" AS \"t0\" CROSS JOIN \"Employee\" AS \"t1\" WHERE (( COALESCE ( \"t0\" . \"City\" , '' )) = ( 'London' )) AND ((( COALESCE ( \"t1\" . \"City\" , '' )) = ( 'Paris' )) AND (( \"t1\" . \"EmployeeId\" ) IS NOT DISTINCT FROM ( \"t0\" . \"SupportRepId\" ))) FOR SHARE SKIP LOCKED Tip Table locks have the type PgLockedTables s , where s is the thread parameter, as described here","title":"SELECT locking clause"},{"location":"user-guide/backends/beam-postgres/#distinct-on-support","text":"Postgres supports the DISTINCT ON clause with selects to return distinct results based on a particular key. The beam-postgres package provides the pgNubBy_ function to use this feature. For example, to get an arbitrary customer from each distinct area code Haskell Postgres Pg . pgNubBy_ ( addressPostalCode . customerAddress ) $ all_ ( customer chinookDb ) SELECT DISTINCT ON ( \"t0\" . \"PostalCode\" ) \"t0\" . \"CustomerId\" AS \"res0\" , \"t0\" . \"FirstName\" AS \"res1\" , \"t0\" . \"LastName\" AS \"res2\" , \"t0\" . \"Company\" AS \"res3\" , \"t0\" . \"Address\" AS \"res4\" , \"t0\" . \"City\" AS \"res5\" , \"t0\" . \"State\" AS \"res6\" , \"t0\" . \"Country\" AS \"res7\" , \"t0\" . \"PostalCode\" AS \"res8\" , \"t0\" . \"Phone\" AS \"res9\" , \"t0\" . \"Fax\" AS \"res10\" , \"t0\" . \"Email\" AS \"res11\" , \"t0\" . \"SupportRepId\" AS \"res12\" FROM \"Customer\" AS \"t0\"","title":"DISTINCT ON support"},{"location":"user-guide/backends/beam-postgres/#aggregates","text":"","title":"Aggregates"},{"location":"user-guide/backends/beam-postgres/#string_agg","text":"The Postgres string_agg aggregate combines all column values in a group separated by a given separator. beam-postgres provides pgStringAgg and pgStringAggOver to use the unquantified and quantified versions of the string_agg aggregate appropriately. For example, to put together a list of all cities in all the postal codes we have for customers, Haskell Postgres aggregate_ ( \\ c -> ( group_ ( addressPostalCode ( customerAddress c )) , Pg . pgStringAgg ( coalesce_ [ addressCity ( customerAddress c )] \"\" ) \",\" ) ) $ all_ ( customer chinookDb ) SELECT \"t0\" . \"PostalCode\" AS \"res0\" , string_agg ( COALESCE ( \"t0\" . \"City\" , '' ), ',' ) AS \"res1\" FROM \"Customer\" AS \"t0\" GROUP BY \"t0\" . \"PostalCode\" The above will include one city multiple times if its shared by multiple customers. Haskell Postgres aggregate_ ( \\ c -> ( group_ ( addressPostalCode ( customerAddress c )) , Pg . pgStringAggOver distinctInGroup_ ( coalesce_ [ addressCity ( customerAddress c )] \"\" ) \",\" ) ) $ all_ ( customer chinookDb ) SELECT \"t0\" . \"PostalCode\" AS \"res0\" , string_agg ( DISTINCT COALESCE ( \"t0\" . \"City\" , '' ), ',' ) AS \"res1\" FROM \"Customer\" AS \"t0\" GROUP BY \"t0\" . \"PostalCode\"","title":"string_agg"},{"location":"user-guide/backends/beam-postgres/#on-conflict","text":"Postgres supports targeting a particular constraint as the target of an ON CONFLICT clause. You can use conflictingConstraint with the name of the constraint with the regular insertOnConflict function to use this functionality. For example, to update the row, only on conflicts relating to the \"PK_CUSTOMER\" constraint. Haskell Postgres --! import Database.Beam.Backend.SQL.BeamExtensions ( BeamHasInsertOnConflict ( .. )) --! import qualified Database.Beam.Postgres as Pg let newCustomer = Customer 42 \"John\" \"Doe\" Nothing ( Address ( Just \"Street\" ) ( Just \"City\" ) ( Just \"State\" ) Nothing Nothing ) Nothing Nothing \"john.doe@johndoe.com\" nothing_ runInsert $ insertOnConflict ( customer chinookDb ) ( insertValues [ newCustomer ]) ( Pg . conflictingConstraint \"PK_Customer\" ) ( onConflictUpdateSet ( \\ fields _ -> fields <-. val_ newCustomer )) INSERT INTO \"Customer\" ( \"CustomerId\" , \"FirstName\" , \"LastName\" , \"Company\" , \"Address\" , \"City\" , \"State\" , \"Country\" , \"PostalCode\" , \"Phone\" , \"Fax\" , \"Email\" , \"SupportRepId\" ) VALUES ( 42 , 'John' , 'Doe' , null , 'Street' , 'City' , 'State' , null , null , null , null , 'john.doe@johndoe.com' , null ) ON CONFLICT ON CONSTRAINT \"PK_Customer\" DO UPDATE SET \"CustomerId\" = ( 42 ), \"FirstName\" = ( 'John' ), \"LastName\" = ( 'Doe' ), \"Company\" = ( null ), \"Address\" = ( 'Street' ), \"City\" = ( 'City' ), \"State\" = ( 'State' ), \"Country\" = ( null ), \"PostalCode\" = ( null ), \"Phone\" = ( null ), \"Fax\" = ( null ), \"Email\" = ( 'john.doe@johndoe.com' ), \"SupportRepId\" = ( null );","title":"ON CONFLICT"},{"location":"user-guide/backends/beam-postgres/#specifying-actions","text":"Often times, you do not want to update every field on a conflict. For example, for upserts, you rarely want to update the primary key. The function onConflictUpdateInstead allows you to restrict which fields are updated in the case of a conflict. The required function argument is a projection of which fields ought to be updated. In the example below, we insert a new row, but if a row with the given primary key already exists, we update only the first and last name. Haskell Postgres -- import qualified Database.Beam.Postgres as Pg let newCustomer = Customer 42 \"John\" \"Doe\" Nothing ( Address ( Just \"Street\" ) ( Just \"City\" ) ( Just \"State\" ) Nothing Nothing ) Nothing Nothing \"john.doe@johndoe.com\" nothing_ runInsert $ Pg . insert ( customer chinookDb ) ( insertValues [ newCustomer ]) $ Pg . onConflict ( Pg . conflictingFields primaryKey ) ( Pg . onConflictUpdateInstead ( \\ c -> ( customerFirstName c , customerLastName c ))) INSERT INTO \"Customer\" ( \"CustomerId\" , \"FirstName\" , \"LastName\" , \"Company\" , \"Address\" , \"City\" , \"State\" , \"Country\" , \"PostalCode\" , \"Phone\" , \"Fax\" , \"Email\" , \"SupportRepId\" ) VALUES ( 42 , 'John' , 'Doe' , null , 'Street' , 'City' , 'State' , null , null , null , null , 'john.doe@johndoe.com' , null ) ON CONFLICT ( \"CustomerId\" ) DO UPDATE SET \"FirstName\" = ( \"excluded\" . \"FirstName\" ), \"LastName\" = ( \"excluded\" . \"LastName\" ); You can also specify a more specific update, using the onConflictUpdateSet function. This is the most general form of the postgres ON CONFLICT action. The excluded table is provided as the second argument. The syntax of the updates is similar to that of update . In the following example, we append the old first name to the new first name and replace the old last name. Haskell Postgres -- import qualified Database.Beam.Postgres as Pg let newCustomer = Customer 42 \"John\" \"Doe\" Nothing ( Address ( Just \"Street\" ) ( Just \"City\" ) ( Just \"State\" ) Nothing Nothing ) Nothing Nothing \"john.doe@johndoe.com\" nothing_ runInsert $ Pg . insert ( customer chinookDb ) ( insertValues [ newCustomer ]) $ Pg . onConflict ( Pg . conflictingFields primaryKey ) ( Pg . onConflictUpdateSet -- tbl is the old row, tblExcluded is the row proposed for insertion ( \\ tbl tblExcluded -> mconcat [ customerFirstName tbl <-. concat_ [ current_ ( customerFirstName tbl ), customerFirstName tblExcluded ] , customerLastName tbl <-. customerLastName tblExcluded ] ) ) INSERT INTO \"Customer\" ( \"CustomerId\" , \"FirstName\" , \"LastName\" , \"Company\" , \"Address\" , \"City\" , \"State\" , \"Country\" , \"PostalCode\" , \"Phone\" , \"Fax\" , \"Email\" , \"SupportRepId\" ) VALUES ( 42 , 'John' , 'Doe' , null , 'Street' , 'City' , 'State' , null , null , null , null , 'john.doe@johndoe.com' , null ) ON CONFLICT ( \"CustomerId\" ) DO UPDATE SET \"FirstName\" = ( CONCAT ( \"Customer\" . \"FirstName\" , \"excluded\" . \"FirstName\" )), \"LastName\" = ( \"excluded\" . \"LastName\" );","title":"Specifying actions"},{"location":"user-guide/backends/beam-sqlite/","text":"SQLite is a lightweight RDBMS meant for embedding in larger applications. Because it is not designed to be full-featured, not all Beam queries will work with SQLite. The module Database.Beam.SQLite.Checked provides many symbols usually imported from the Database.Beam module that enforce extra checks on queries to assure compliance with SQLite. Use this module in code that is SQLite specific for maximal compile-time safety. Note that this module should be imported instead of Database.Beam to avoid name clashes. Compatibility SQLite is compatible enough with Beam's query syntax, that adapting to its quirks is pretty straightforwards. The main special case for SQLite is its handling of nested set operations. On most backends, beam can output these directly, but SQLite requires us to generate subqueries.","title":"beam-sqlite"},{"location":"user-guide/backends/beam-sqlite/#compatibility","text":"SQLite is compatible enough with Beam's query syntax, that adapting to its quirks is pretty straightforwards. The main special case for SQLite is its handling of nested set operations. On most backends, beam can output these directly, but SQLite requires us to generate subqueries.","title":"Compatibility"},{"location":"user-guide/manipulation/delete/","text":"SQL DELETE expressions allow you to remove rows from a database. The delete function from Database.Beam.Query can be used to delete rows from a particular table. The function takes a table and a condition for the WHERE clause. The function returns a SqlDelete object that can be run in MonadBeam with runDelete . For example, to delete any customer, whose first name is Emilio. Haskell Postgres Sqlite runDelete $ delete ( customer chinookDb ) ( \\ c -> customerFirstName c ==. \"Emilio\" ) DELETE FROM \"Customer\" AS \"delete_target\" WHERE ( \"delete_target\" . \"FirstName\" ) = ( 'Emilio' ); DELETE FROM \"Customer\" WHERE ( \"FirstName\" ) = ( ? ); -- With values: [SQLText \"Emilio\"]; DELETE is fairly simple compared to the other manipulation commands, so that's really all there is to it. Expressions for the WHERE clause can be arbitrarily complex, assuming your backend supports it. For example, to delete any invoice with more than five invoice lines Haskell Sqlite runDelete $ delete ( invoice chinookDb ) ( \\ i -> 5 <. subquery_ ( aggregate_ ( \\ _ -> as_ @ Int32 countAll_ ) $ invoiceLines i )) DELETE FROM \"Invoice\" WHERE ( ? ) < ( ( SELECT COUNT ( * ) AS \"res0\" FROM \"InvoiceLine\" AS \"t0\" WHERE ( \"t0\" . \"InvoiceId\" ) = ( \"InvoiceId\" ))); -- With values: [SQLInteger 5]; Note The example above was only given for SQLite because it violates a foreign key constraint in the underlying database, and other backends are more pedantic","title":"DELETE"},{"location":"user-guide/manipulation/insert/","text":"SQL INSERT expressions allow you to insert rows in the database. There is a lot of variety in how you can provide new data, and Beam supports all standard ways. The insert function from Database.Beam.Query can be used to insert rows into a particular table. insert takes a table and a source of values, represented by SqlInsertValues , and returns a SqlInsert object that can be run in a MonadBeam with runInsert . The SqlInsertValues type takes two type parameters. The first is the underlying database syntax, and the second is the shape of the data it carries, specified as a beam table type. For example, a source of values in Postgres that can be inserted in the Chinook customers table would have the type SqlInsertValues PgInsertValuesSyntax CustomerT . This abstracts over where those values actually are. The values may be explicit haskell values, expressions returning customers, a query returning customers, or something else. Either way, they can all be used in the same way with the insert function. Inserting explicit new values If you have a record of explicit Haskell values, use the insertValues function. For example, to insert a new playlist into our chinook database Haskell Postgres Sqlite runInsert $ insert ( playlist chinookDb ) $ insertValues [ Playlist 700 ( Just \"My New Playlist\" ) , Playlist 701 ( Just \"Another Playlist\" ) , Playlist 702 ( Just \"Look... more playlists\" ) ] insertedPlaylists <- runSelectReturningList $ select $ filter_ ( \\ p -> playlistId p >=. 700 ) $ all_ ( playlist chinookDb ) putStrLn \"Inserted playlists:\" forM_ insertedPlaylists $ \\ p -> putStrLn ( show p ) INSERT INTO \"Playlist\" ( \"PlaylistId\" , \"Name\" ) VALUES ( 700 , 'My New Playlist' ), ( 701 , 'Another Playlist' ), ( 702 , 'Look... more playlists' ); SELECT \"t0\" . \"PlaylistId\" AS \"res0\" , \"t0\" . \"Name\" AS \"res1\" FROM \"Playlist\" AS \"t0\" WHERE ( \"t0\" . \"PlaylistId\" ) >= ( 700 ); -- Output: Inserted playlists: -- Output: Playlist {playlistId = 700, playlistName = Just \"My New Playlist\"} -- Output: Playlist {playlistId = 701, playlistName = Just \"Another Playlist\"} -- Output: Playlist {playlistId = 702, playlistName = Just \"Look... more playlists\"} INSERT INTO \"Playlist\" ( \"PlaylistId\" , \"Name\" ) VALUES ( ? , ? ), ( ? , ? ), ( ? , ? ); -- With values: [SQLInteger 700,SQLText \"My New Playlist\",SQLInteger 701,SQLText \"Another Playlist\",SQLInteger 702,SQLText \"Look... more playlists\"]; SELECT \"t0\" . \"PlaylistId\" AS \"res0\" , \"t0\" . \"Name\" AS \"res1\" FROM \"Playlist\" AS \"t0\" WHERE ( \"t0\" . \"PlaylistId\" ) >= ( ? ); -- With values: [SQLInteger 700]; -- Output: Inserted playlists: -- Output: Playlist {playlistId = 700, playlistName = Just \"My New Playlist\"} -- Output: Playlist {playlistId = 701, playlistName = Just \"Another Playlist\"} -- Output: Playlist {playlistId = 702, playlistName = Just \"Look... more playlists\"} Inserting calculated values Inserting explicit values is all well and good, but sometimes we want to defer some processing to the database. For example, perhaps we want to create a new invoice and use the current time as the invoice date. We could grab the current time using getCurrentTime and then use this to construct an explicit Haskell value, but this may cause synchronization issues for our application. To do this, beam allows us to specify arbitrary expressions as a source of values using the insertExpressions function. Haskell Postgres Sqlite runInsert $ insert ( invoice chinookDb ) $ insertExpressions [ Invoice ( val_ 800 ) ( CustomerId ( val_ 1 )) currentTimestamp_ ( val_ ( Address ( Just \"123 My Street\" ) ( Just \"Buenos Noches\" ) ( Just \"Rio\" ) ( Just \"Mozambique\" ) ( Just \"ABCDEF\" ))) ( val_ 1000 ) ] Just newInvoice <- runSelectReturningOne $ lookup_ ( invoice chinookDb ) ( InvoiceId 800 ) putStrLn ( \"Inserted invoice: \" ++ show newInvoice ) INSERT INTO \"Invoice\" ( \"InvoiceId\" , \"CustomerId\" , \"InvoiceDate\" , \"BillingAddress\" , \"BillingCity\" , \"BillingState\" , \"BillingCountry\" , \"BillingPostalCode\" , \"Total\" ) VALUES ( 800 , 1 , CURRENT_TIMESTAMP , '123 My Street' , 'Buenos Noches' , 'Rio' , 'Mozambique' , 'ABCDEF' , '1000.0' ); SELECT \"t0\" . \"InvoiceId\" AS \"res0\" , \"t0\" . \"CustomerId\" AS \"res1\" , \"t0\" . \"InvoiceDate\" AS \"res2\" , \"t0\" . \"BillingAddress\" AS \"res3\" , \"t0\" . \"BillingCity\" AS \"res4\" , \"t0\" . \"BillingState\" AS \"res5\" , \"t0\" . \"BillingCountry\" AS \"res6\" , \"t0\" . \"BillingPostalCode\" AS \"res7\" , \"t0\" . \"Total\" AS \"res8\" FROM \"Invoice\" AS \"t0\" WHERE ( \"t0\" . \"InvoiceId\" ) = ( 800 ); -- Output: Inserted invoice: Invoice {invoiceId = SqlSerial {unSerial = 800}, invoiceCustomer = CustomerId 1, invoiceDate = 2022-01-18 01:31:21.436807, invoiceBillingAddress = Address {address = Just \"123 My Street\", addressCity = Just \"Buenos Noches\", addressState = Just \"Rio\", addressCountry = Just \"Mozambique\", addressPostalCode = Just \"ABCDEF\"}, invoiceTotal = 1000.0} INSERT INTO \"Invoice\" ( \"InvoiceId\" , \"CustomerId\" , \"InvoiceDate\" , \"BillingAddress\" , \"BillingCity\" , \"BillingState\" , \"BillingCountry\" , \"BillingPostalCode\" , \"Total\" ) VALUES ( ? , ? , CURRENT_TIMESTAMP , ? , ? , ? , ? , ? , ? ); -- With values: [SQLInteger 800,SQLInteger 1,SQLText \"123 My Street\",SQLText \"Buenos Noches\",SQLText \"Rio\",SQLText \"Mozambique\",SQLText \"ABCDEF\",SQLText \"1000.0\"]; SELECT \"t0\" . \"InvoiceId\" AS \"res0\" , \"t0\" . \"CustomerId\" AS \"res1\" , \"t0\" . \"InvoiceDate\" AS \"res2\" , \"t0\" . \"BillingAddress\" AS \"res3\" , \"t0\" . \"BillingCity\" AS \"res4\" , \"t0\" . \"BillingState\" AS \"res5\" , \"t0\" . \"BillingCountry\" AS \"res6\" , \"t0\" . \"BillingPostalCode\" AS \"res7\" , \"t0\" . \"Total\" AS \"res8\" FROM \"Invoice\" AS \"t0\" WHERE ( \"t0\" . \"InvoiceId\" ) = ( ? ); -- With values: [SQLInteger 800]; -- Output: Inserted invoice: Invoice {invoiceId = SqlSerial {unSerial = 800}, invoiceCustomer = CustomerId 1, invoiceDate = 2022-01-18 01:31:24, invoiceBillingAddress = Address {address = Just \"123 My Street\", addressCity = Just \"Buenos Noches\", addressState = Just \"Rio\", addressCountry = Just \"Mozambique\", addressPostalCode = Just \"ABCDEF\"}, invoiceTotal = 1000.0} insertExpressions is strictly more general than insertValues . We can turn any insertValues to an insertExpressions by running every table value through the val_ function to convert a Haskell literal to an expression. For example, we can write the playlist example above as Haskell Postgres Sqlite runInsert $ insert ( playlist chinookDb ) $ insertExpressions [ val_ $ Playlist 700 ( Just \"My New Playlist\" ) , val_ $ Playlist 701 ( Just \"Another Playlist\" ) , val_ $ Playlist 702 ( Just \"Look... more playlists\" ) ] insertedPlaylists <- runSelectReturningList $ select $ filter_ ( \\ p -> playlistId p >=. 700 ) $ all_ ( playlist chinookDb ) putStrLn \"Inserted playlists:\" forM_ insertedPlaylists $ \\ p -> putStrLn ( show p ) INSERT INTO \"Playlist\" ( \"PlaylistId\" , \"Name\" ) VALUES ( 700 , 'My New Playlist' ), ( 701 , 'Another Playlist' ), ( 702 , 'Look... more playlists' ); SELECT \"t0\" . \"PlaylistId\" AS \"res0\" , \"t0\" . \"Name\" AS \"res1\" FROM \"Playlist\" AS \"t0\" WHERE ( \"t0\" . \"PlaylistId\" ) >= ( 700 ); -- Output: Inserted playlists: -- Output: Playlist {playlistId = 700, playlistName = Just \"My New Playlist\"} -- Output: Playlist {playlistId = 701, playlistName = Just \"Another Playlist\"} -- Output: Playlist {playlistId = 702, playlistName = Just \"Look... more playlists\"} INSERT INTO \"Playlist\" ( \"PlaylistId\" , \"Name\" ) VALUES ( ? , ? ), ( ? , ? ), ( ? , ? ); -- With values: [SQLInteger 700,SQLText \"My New Playlist\",SQLInteger 701,SQLText \"Another Playlist\",SQLInteger 702,SQLText \"Look... more playlists\"]; SELECT \"t0\" . \"PlaylistId\" AS \"res0\" , \"t0\" . \"Name\" AS \"res1\" FROM \"Playlist\" AS \"t0\" WHERE ( \"t0\" . \"PlaylistId\" ) >= ( ? ); -- With values: [SQLInteger 700]; -- Output: Inserted playlists: -- Output: Playlist {playlistId = 700, playlistName = Just \"My New Playlist\"} -- Output: Playlist {playlistId = 701, playlistName = Just \"Another Playlist\"} -- Output: Playlist {playlistId = 702, playlistName = Just \"Look... more playlists\"} One common use of insertExpressions_ is when adding new rows to tables where one field needs to be set to the default value. For example, auto-incrementing keys or random UUIDs are a common way to assign primary keys to rows. You can use insertExpressions_ using the default_ expression for each column that you want to use the default value for. For example, the query below adds a new invoice asking the database to assign a new id. Haskell Postgres runInsert $ insert ( invoice chinookDb ) $ insertExpressions [ Invoice default_ -- Ask the database to give us a default id ( val_ ( CustomerId 1 )) currentTimestamp_ ( val_ ( Address ( Just \"123 My Street\" ) ( Just \"Buenos Noches\" ) ( Just \"Rio\" ) ( Just \"Mozambique\" ) ( Just \"ABCDEF\" ))) ( val_ 1000 ) ] INSERT INTO \"Invoice\" ( \"InvoiceId\" , \"CustomerId\" , \"InvoiceDate\" , \"BillingAddress\" , \"BillingCity\" , \"BillingState\" , \"BillingCountry\" , \"BillingPostalCode\" , \"Total\" ) VALUES ( DEFAULT , 1 , CURRENT_TIMESTAMP , '123 My Street' , 'Buenos Noches' , 'Rio' , 'Mozambique' , 'ABCDEF' , '1000.0' ); Warning SQLite is a great little backend, but it doesn't support some standard SQL features, like the DEFAULT keyword in inserts. You can retrieve the same functionality by only inserting into a subset of columns. See the section on that below. Retrieving the rows inserted However, now we have no way of knowing what value the database assigned. Unfortunately, there is no database-agnostic solution to this problem. However, it's a common enough use case that beam provides a backend-agnostic way for some backends. Backends that provide this functionality provide an instance of MonadBeamInsertReturning . In order to use this class, you'll need to explicitly import Database.Beam.Backend.SQL.BeamExtensions . Below, we've imported this module qualified. Haskell Postgres Sqlite [ newInvoice ] <- BeamExtensions . runInsertReturningList $ insert ( invoice chinookDb ) $ insertExpressions [ Invoice default_ -- Ask the database to give us a default id ( val_ ( CustomerId 1 )) currentTimestamp_ ( val_ ( Address ( Just \"123 My Street\" ) ( Just \"Buenos Noches\" ) ( Just \"Rio\" ) ( Just \"Mozambique\" ) ( Just \"ABCDEF\" ))) ( val_ 1000 ) ] putStrLn ( \"We inserted a new invoice, and the result was \" ++ show newInvoice ) INSERT INTO \"Invoice\" ( \"InvoiceId\" , \"CustomerId\" , \"InvoiceDate\" , \"BillingAddress\" , \"BillingCity\" , \"BillingState\" , \"BillingCountry\" , \"BillingPostalCode\" , \"Total\" ) VALUES ( DEFAULT , 1 , CURRENT_TIMESTAMP , '123 My Street' , 'Buenos Noches' , 'Rio' , 'Mozambique' , 'ABCDEF' , '1000.0' ) RETURNING \"InvoiceId\" , \"CustomerId\" , \"InvoiceDate\" , \"BillingAddress\" , \"BillingCity\" , \"BillingState\" , \"BillingCountry\" , \"BillingPostalCode\" , \"Total\" ; -- Output: We inserted a new invoice, and the result was Invoice {invoiceId = SqlSerial {unSerial = 501}, invoiceCustomer = CustomerId 1, invoiceDate = 2022-01-18 01:31:34.451145, invoiceBillingAddress = Address {address = Just \"123 My Street\", addressCity = Just \"Buenos Noches\", addressState = Just \"Rio\", addressCountry = Just \"Mozambique\", addressPostalCode = Just \"ABCDEF\"}, invoiceTotal = 1000.0} INSERT INTO \"Invoice\" ( \"CustomerId\" , \"InvoiceDate\" , \"BillingAddress\" , \"BillingCity\" , \"BillingState\" , \"BillingCountry\" , \"BillingPostalCode\" , \"Total\" ) VALUES ( ? , CURRENT_TIMESTAMP , ? , ? , ? , ? , ? , ? ); -- With values: [SQLInteger 1,SQLText \"123 My Street\",SQLText \"Buenos Noches\",SQLText \"Rio\",SQLText \"Mozambique\",SQLText \"ABCDEF\",SQLText \"1000.0\"]; -- Output: We inserted a new invoice, and the result was Invoice {invoiceId = SqlSerial {unSerial = 413}, invoiceCustomer = CustomerId 1, invoiceDate = 2022-01-18 01:31:37, invoiceBillingAddress = Address {address = Just \"123 My Street\", addressCity = Just \"Buenos Noches\", addressState = Just \"Rio\", addressCountry = Just \"Mozambique\", addressPostalCode = Just \"ABCDEF\"}, invoiceTotal = 1000.0} The pattern match on the single newInvoice is safe, even though its partial. In general, you can expect the same amount of rows returned as specified in your SqlInsertValues . If you know what this is statically, then you can feel free to pattern match directly. Otherwise (if you used insertFrom , for example), you'll need to handle the possibility that nothing was inserted. Note Although SQLite has no support for the DEFAULT clause, MonadBeamInsertReturning in beam-sqlite inserts rows one at a time and will detect usage of the DEFAULT keyword. The beam authors consider this okay. While most beam statements are guaranteed to translate directly to the underlying DBMS system, runInsertReturningList is explicitly marked as emulated functionality. Inserting from the result of a SELECT statement Sometimes you want to use existing data to insert values. For example, perhaps we want to give every customer their own playlist, titled \" 's playlist\". We can use the insertFrom function to make a SqlInsertValues corresponding to the result of a query. Make sure to return a projection with the same 'shape' as your data. If not, you'll get a compile time error. For example, to create the playlists as above Haskell Postgres Sqlite runInsert $ insert ( playlist chinookDb ) $ insertFrom $ do c <- all_ ( customer chinookDb ) pure ( Playlist ( customerId c + 1000 ) ( just_ ( concat_ [ customerFirstName c , \"'s Playlist\" ]))) playlists <- runSelectReturningList $ select $ limit_ 10 $ orderBy_ ( \\ p -> asc_ ( playlistId p )) $ filter_ ( \\ p -> playlistId p >=. 1000 ) $ all_ ( playlist chinookDb ) putStrLn \"Inserted playlists\" forM_ playlists $ \\ playlist -> putStrLn ( \" - \" ++ show playlist ) INSERT INTO \"Playlist\" ( \"PlaylistId\" , \"Name\" ) SELECT ( \"t0\" . \"CustomerId\" ) + ( 1000 ) AS \"res0\" , CONCAT ( \"t0\" . \"FirstName\" , '''s Playlist' ) AS \"res1\" FROM \"Customer\" AS \"t0\" ; SELECT \"t0\" . \"PlaylistId\" AS \"res0\" , \"t0\" . \"Name\" AS \"res1\" FROM \"Playlist\" AS \"t0\" WHERE ( \"t0\" . \"PlaylistId\" ) >= ( 1000 ) ORDER BY \"t0\" . \"PlaylistId\" ASC LIMIT 10 ; -- Output: Inserted playlists -- Output: - Playlist {playlistId = 1001, playlistName = Just \"Lu\\237s's Playlist\"} -- Output: - Playlist {playlistId = 1002, playlistName = Just \"Leonie's Playlist\"} -- Output: - Playlist {playlistId = 1003, playlistName = Just \"Fran\\231ois's Playlist\"} -- Output: - Playlist {playlistId = 1004, playlistName = Just \"Bj\\345rn's Playlist\"} -- Output: - Playlist {playlistId = 1005, playlistName = Just \"Franti\\154ek's Playlist\"} -- Output: - Playlist {playlistId = 1006, playlistName = Just \"Helena's Playlist\"} -- Output: - Playlist {playlistId = 1007, playlistName = Just \"Astrid's Playlist\"} -- Output: - Playlist {playlistId = 1008, playlistName = Just \"Daan's Playlist\"} -- Output: - Playlist {playlistId = 1009, playlistName = Just \"Kara's Playlist\"} -- Output: - Playlist {playlistId = 1010, playlistName = Just \"Eduardo's Playlist\"} INSERT INTO \"Playlist\" ( \"PlaylistId\" , \"Name\" ) SELECT ( \"t0\" . \"CustomerId\" ) + ( ? ) AS \"res0\" , ( \"t0\" . \"FirstName\" || ( ? )) AS \"res1\" FROM \"Customer\" AS \"t0\" ; -- With values: [SQLInteger 1000,SQLText \"'s Playlist\"]; SELECT \"t0\" . \"PlaylistId\" AS \"res0\" , \"t0\" . \"Name\" AS \"res1\" FROM \"Playlist\" AS \"t0\" WHERE ( \"t0\" . \"PlaylistId\" ) >= ( ? ) ORDER BY \"t0\" . \"PlaylistId\" ASC LIMIT 10 ; -- With values: [SQLInteger 1000]; -- Output: Inserted playlists -- Output: - Playlist {playlistId = 1001, playlistName = Just \"Lu\\237s's Playlist\"} -- Output: - Playlist {playlistId = 1002, playlistName = Just \"Leonie's Playlist\"} -- Output: - Playlist {playlistId = 1003, playlistName = Just \"Fran\\231ois's Playlist\"} -- Output: - Playlist {playlistId = 1004, playlistName = Just \"Bj\\248rn's Playlist\"} -- Output: - Playlist {playlistId = 1005, playlistName = Just \"Franti\\353ek's Playlist\"} -- Output: - Playlist {playlistId = 1006, playlistName = Just \"Helena's Playlist\"} -- Output: - Playlist {playlistId = 1007, playlistName = Just \"Astrid's Playlist\"} -- Output: - Playlist {playlistId = 1008, playlistName = Just \"Daan's Playlist\"} -- Output: - Playlist {playlistId = 1009, playlistName = Just \"Kara's Playlist\"} -- Output: - Playlist {playlistId = 1010, playlistName = Just \"Eduardo's Playlist\"} Choosing a subset of columns Above, we used the default_ clause to set a column to a default value. Unfortunately, not all backends support default_ (SQLite being a notable exception). Moreover, some INSERT forms simply can't use default_ , such as insertFrom_ (you can't return default_ from a query). The standard SQL tool used in these cases is limiting the inserted data to specific columns. For example, suppose we want to insert new invoices for every customer with today's date. We can use the insertOnly function to project which field's are being inserted. Haskell Postgres Sqlite runInsert $ insertOnly ( invoice chinookDb ) ( \\ i -> ( invoiceCustomer i , invoiceDate i , invoiceBillingAddress i , invoiceTotal i ) ) $ insertFrom $ do c <- all_ ( customer chinookDb ) -- We'll just charge each customer $10 to be mean! pure ( primaryKey c , currentTimestamp_ , customerAddress c , as_ @ Scientific $ val_ 10 ) INSERT INTO \"Invoice\" ( \"CustomerId\" , \"InvoiceDate\" , \"BillingAddress\" , \"BillingCity\" , \"BillingState\" , \"BillingCountry\" , \"BillingPostalCode\" , \"Total\" ) SELECT \"t0\" . \"CustomerId\" AS \"res0\" , CURRENT_TIMESTAMP AS \"res1\" , \"t0\" . \"Address\" AS \"res2\" , \"t0\" . \"City\" AS \"res3\" , \"t0\" . \"State\" AS \"res4\" , \"t0\" . \"Country\" AS \"res5\" , \"t0\" . \"PostalCode\" AS \"res6\" , '10.0' AS \"res7\" FROM \"Customer\" AS \"t0\" ; INSERT INTO \"Invoice\" ( \"CustomerId\" , \"InvoiceDate\" , \"BillingAddress\" , \"BillingCity\" , \"BillingState\" , \"BillingCountry\" , \"BillingPostalCode\" , \"Total\" ) SELECT \"t0\" . \"CustomerId\" AS \"res0\" , CURRENT_TIMESTAMP AS \"res1\" , \"t0\" . \"Address\" AS \"res2\" , \"t0\" . \"City\" AS \"res3\" , \"t0\" . \"State\" AS \"res4\" , \"t0\" . \"Country\" AS \"res5\" , \"t0\" . \"PostalCode\" AS \"res6\" , ? AS \"res7\" FROM \"Customer\" AS \"t0\" ; -- With values: [SQLText \"10.0\"]; Inserting nothing Oftentimes, the values to be inserted are generated automatically by some Haskell function, and you just insert the resulting list. Sometimes, these lists may be empty. If you blindly translated this into SQL, you'd end up with INSERT s with empty VALUE clauses, which are illegal. Beam actually handles this gracefully. If a SqlInsertValues has no rows to insert, the SqlInsert returned by insert will know that it is empty. Running this SqlInsert results in nothing being sent to the database, which you can verify below. Haskell Postgres Sqlite let superComplicatedAction = pure [] -- Hopefully, you're more creative! valuesToInsert <- superComplicatedAction putStrLn \"The following runInsert will send no commands to the database\" runInsert $ insert ( playlist chinookDb ) $ insertValues valuesToInsert putStrLn \"See! I told you!\" -- Output: The following runInsert will send no commands to the database -- Output: See! I told you! -- Output: The following runInsert will send no commands to the database -- Output: See! I told you! ON CONFLICT Several backends (such as Postgres and SQLite) support ON CONFLICT subexpressions that specify what action to take when an INSERT statement conlicts with already present data. Beam support backend-agnostic ON CONFLICT statements via the BeamHasInsertOnConflict syntax. This class contains a new function to generate an SqlInsert . The insertOnConflict function can be used to attach ON CONFLICT actions to a SqlInsert . insertOnConflict :: Beamable table => DatabaseEntity be db ( TableEntity table ) -> SqlInsertValues be ( table ( QExpr be s )) -> SqlConflictTarget be table -> SqlConflictAction be table -> SqlInsert be table The SqlConflictTarget specifies on which kinds of conflicts the action should run. You have a few options anyConflict - run the action on any conflict conflictingFields - run the action only when certain fields conflict conflictingFieldsWhere - run the action only when certain fields conflict and a particular expression evaluates to true. The SqlConflictAction specifies what to do when a conflict happens. onConflictDoNothing - this cancels the insertion onConflictUpdateSet - sets fields to new values based on the current values onConflictUpdateSetWhere - sets fields to new values if a particular condition holds Acting on any conflict A common use case of ON CONFLICT is to upsert rows into a database. Upsertion refers to only inserting a row if another conflicting row does not already exist. For example, if you have a new customer with primary key 42, and you don't know if it's in the database or not, but you want to insert it if not, you can use the insertOnConflict function with the anyConflict target. let newCustomer = Customer 42 \"John\" \"Doe\" Nothing ( Address ( Just \"Street\" ) ( Just \"City\" ) ( Just \"State\" ) Nothing Nothing ) Nothing Nothing \"john.doe@johndoe.com\" nothing_ runInsert $ insertOnConflict ( customer chinookDb ) ( insertValues [ newCustomer ]) anyConflict onConflictDoNothing Acting only on certain conflicts Sometimes you only want to perform an action if a certain constraint is violated. If the conflicting index or constraint is on a field you can specify which fields with the function conflictingFields . Haskell Postgres Sqlite --! import Database.Beam.Backend.SQL.BeamExtensions ( BeamHasInsertOnConflict ( .. )) let newCustomer = Customer 42 \"John\" \"Doe\" Nothing ( Address ( Just \"Street\" ) ( Just \"City\" ) ( Just \"State\" ) Nothing Nothing ) Nothing Nothing \"john.doe@johndoe.com\" nothing_ runInsert $ insertOnConflict ( customer chinookDb ) ( insertValues [ newCustomer ]) ( conflictingFields ( \\ tbl -> primaryKey tbl )) ( onConflictUpdateSet ( \\ fields oldValues -> fields <-. val_ newCustomer )) INSERT INTO \"Customer\" ( \"CustomerId\" , \"FirstName\" , \"LastName\" , \"Company\" , \"Address\" , \"City\" , \"State\" , \"Country\" , \"PostalCode\" , \"Phone\" , \"Fax\" , \"Email\" , \"SupportRepId\" ) VALUES ( 42 , 'John' , 'Doe' , null , 'Street' , 'City' , 'State' , null , null , null , null , 'john.doe@johndoe.com' , null ) ON CONFLICT ( \"CustomerId\" ) DO UPDATE SET \"CustomerId\" = ( 42 ), \"FirstName\" = ( 'John' ), \"LastName\" = ( 'Doe' ), \"Company\" = ( null ), \"Address\" = ( 'Street' ), \"City\" = ( 'City' ), \"State\" = ( 'State' ), \"Country\" = ( null ), \"PostalCode\" = ( null ), \"Phone\" = ( null ), \"Fax\" = ( null ), \"Email\" = ( 'john.doe@johndoe.com' ), \"SupportRepId\" = ( null ); INSERT INTO \"Customer\" ( \"CustomerId\" , \"FirstName\" , \"LastName\" , \"Company\" , \"Address\" , \"City\" , \"State\" , \"Country\" , \"PostalCode\" , \"Phone\" , \"Fax\" , \"Email\" , \"SupportRepId\" ) VALUES ( ? , ? , ? , NULL , ? , ? , ? , NULL , NULL , NULL , NULL , ? , NULL ) ON CONFLICT ( \"CustomerId\" ) DO UPDATE SET \"CustomerId\" = ? , \"FirstName\" = ? , \"LastName\" = ? , \"Company\" = NULL , \"Address\" = ? , \"City\" = ? , \"State\" = ? , \"Country\" = NULL , \"PostalCode\" = NULL , \"Phone\" = NULL , \"Fax\" = NULL , \"Email\" = ? , \"SupportRepId\" = NULL ; -- With values: [SQLInteger 42,SQLText \"John\",SQLText \"Doe\",SQLText \"Street\",SQLText \"City\",SQLText \"State\",SQLText \"john.doe@johndoe.com\",SQLInteger 42,SQLText \"John\",SQLText \"Doe\",SQLText \"Street\",SQLText \"City\",SQLText \"State\",SQLText \"john.doe@johndoe.com\"]; Tip To specify a conflict on the primary keys, use conflictingFields primaryKey . You can also specify how to change the record should it not match. For example, to append the e-mail as an alternate when you insert an existing row, you can use the oldValues argument to get access to the old value. Haskell Postgres Sqlite --! import Database.Beam.Backend.SQL.BeamExtensions ( BeamHasInsertOnConflict ( .. )) let newCustomer = Customer 42 \"John\" \"Doe\" Nothing ( Address ( Just \"Street\" ) ( Just \"City\" ) ( Just \"State\" ) Nothing Nothing ) Nothing Nothing \"john.doe@johndoe.com\" nothing_ runInsert $ insertOnConflict ( customer chinookDb ) ( insertValues [ newCustomer ]) ( conflictingFields ( \\ tbl -> primaryKey tbl )) ( onConflictUpdateSet ( \\ fields oldValues -> customerEmail fields <-. concat_ [ customerEmail oldValues , \";\" , val_ ( customerEmail newCustomer )])) INSERT INTO \"Customer\" ( \"CustomerId\" , \"FirstName\" , \"LastName\" , \"Company\" , \"Address\" , \"City\" , \"State\" , \"Country\" , \"PostalCode\" , \"Phone\" , \"Fax\" , \"Email\" , \"SupportRepId\" ) VALUES ( 42 , 'John' , 'Doe' , null , 'Street' , 'City' , 'State' , null , null , null , null , 'john.doe@johndoe.com' , null ) ON CONFLICT ( \"CustomerId\" ) DO UPDATE SET \"Email\" = ( CONCAT ( \"excluded\" . \"Email\" , ';' , 'john.doe@johndoe.com' )); INSERT INTO \"Customer\" ( \"CustomerId\" , \"FirstName\" , \"LastName\" , \"Company\" , \"Address\" , \"City\" , \"State\" , \"Country\" , \"PostalCode\" , \"Phone\" , \"Fax\" , \"Email\" , \"SupportRepId\" ) VALUES ( ? , ? , ? , NULL , ? , ? , ? , NULL , NULL , NULL , NULL , ? , NULL ) ON CONFLICT ( \"CustomerId\" ) DO UPDATE SET \"Email\" = ( \"excluded\" . \"Email\" || ( ? ) || ( ? )); -- With values: [SQLInteger 42,SQLText \"John\",SQLText \"Doe\",SQLText \"Street\",SQLText \"City\",SQLText \"State\",SQLText \"john.doe@johndoe.com\",SQLText \";\",SQLText \"john.doe@johndoe.com\"]; If you want to be even more particular and only do this transformation on rows corresponding to customers from one state, use conflictingFieldsWhere . Haskell Postgres Sqlite --! import Database.Beam.Backend.SQL.BeamExtensions ( BeamHasInsertOnConflict ( .. )) let newCustomer = Customer 42 \"John\" \"Doe\" Nothing ( Address ( Just \"Street\" ) ( Just \"City\" ) ( Just \"State\" ) Nothing Nothing ) Nothing Nothing \"john.doe@johndoe.com\" nothing_ runInsert $ insertOnConflict ( customer chinookDb ) ( insertValues [ newCustomer ]) ( conflictingFieldsWhere ( \\ tbl -> primaryKey tbl ) ( \\ tbl -> addressState ( customerAddress tbl ) ==. val_ ( Just \"CA\" ))) ( onConflictUpdateSet ( \\ fields oldValues -> customerEmail fields <-. concat_ [ customerEmail oldValues , \";\" , val_ ( customerEmail newCustomer )])) INSERT INTO \"Customer\" ( \"CustomerId\" , \"FirstName\" , \"LastName\" , \"Company\" , \"Address\" , \"City\" , \"State\" , \"Country\" , \"PostalCode\" , \"Phone\" , \"Fax\" , \"Email\" , \"SupportRepId\" ) VALUES ( 42 , 'John' , 'Doe' , null , 'Street' , 'City' , 'State' , null , null , null , null , 'john.doe@johndoe.com' , null ) ON CONFLICT ( \"CustomerId\" ) WHERE (( \"State\" ) IS NOT DISTINCT FROM ( 'CA' )) DO UPDATE SET \"Email\" = ( CONCAT ( \"excluded\" . \"Email\" , ';' , 'john.doe@johndoe.com' )); INSERT INTO \"Customer\" ( \"CustomerId\" , \"FirstName\" , \"LastName\" , \"Company\" , \"Address\" , \"City\" , \"State\" , \"Country\" , \"PostalCode\" , \"Phone\" , \"Fax\" , \"Email\" , \"SupportRepId\" ) VALUES ( ? , ? , ? , NULL , ? , ? , ? , NULL , NULL , NULL , NULL , ? , NULL ) ON CONFLICT ( \"CustomerId\" ) WHERE CASE WHEN (( \"State\" ) IS NULL ) AND (( ? ) IS NULL ) THEN ? WHEN (( \"State\" ) IS NULL ) OR (( ? ) IS NULL ) THEN ? ELSE ( \"State\" ) = ( ? ) END DO UPDATE SET \"Email\" = ( \"excluded\" . \"Email\" || ( ? ) || ( ? )); -- With values: [SQLInteger 42,SQLText \"John\",SQLText \"Doe\",SQLText \"Street\",SQLText \"City\",SQLText \"State\",SQLText \"john.doe@johndoe.com\",SQLText \"CA\",SQLInteger 1,SQLText \"CA\",SQLInteger 0,SQLText \"CA\",SQLText \";\",SQLText \"john.doe@johndoe.com\"];","title":"INSERT"},{"location":"user-guide/manipulation/insert/#inserting-explicit-new-values","text":"If you have a record of explicit Haskell values, use the insertValues function. For example, to insert a new playlist into our chinook database Haskell Postgres Sqlite runInsert $ insert ( playlist chinookDb ) $ insertValues [ Playlist 700 ( Just \"My New Playlist\" ) , Playlist 701 ( Just \"Another Playlist\" ) , Playlist 702 ( Just \"Look... more playlists\" ) ] insertedPlaylists <- runSelectReturningList $ select $ filter_ ( \\ p -> playlistId p >=. 700 ) $ all_ ( playlist chinookDb ) putStrLn \"Inserted playlists:\" forM_ insertedPlaylists $ \\ p -> putStrLn ( show p ) INSERT INTO \"Playlist\" ( \"PlaylistId\" , \"Name\" ) VALUES ( 700 , 'My New Playlist' ), ( 701 , 'Another Playlist' ), ( 702 , 'Look... more playlists' ); SELECT \"t0\" . \"PlaylistId\" AS \"res0\" , \"t0\" . \"Name\" AS \"res1\" FROM \"Playlist\" AS \"t0\" WHERE ( \"t0\" . \"PlaylistId\" ) >= ( 700 ); -- Output: Inserted playlists: -- Output: Playlist {playlistId = 700, playlistName = Just \"My New Playlist\"} -- Output: Playlist {playlistId = 701, playlistName = Just \"Another Playlist\"} -- Output: Playlist {playlistId = 702, playlistName = Just \"Look... more playlists\"} INSERT INTO \"Playlist\" ( \"PlaylistId\" , \"Name\" ) VALUES ( ? , ? ), ( ? , ? ), ( ? , ? ); -- With values: [SQLInteger 700,SQLText \"My New Playlist\",SQLInteger 701,SQLText \"Another Playlist\",SQLInteger 702,SQLText \"Look... more playlists\"]; SELECT \"t0\" . \"PlaylistId\" AS \"res0\" , \"t0\" . \"Name\" AS \"res1\" FROM \"Playlist\" AS \"t0\" WHERE ( \"t0\" . \"PlaylistId\" ) >= ( ? ); -- With values: [SQLInteger 700]; -- Output: Inserted playlists: -- Output: Playlist {playlistId = 700, playlistName = Just \"My New Playlist\"} -- Output: Playlist {playlistId = 701, playlistName = Just \"Another Playlist\"} -- Output: Playlist {playlistId = 702, playlistName = Just \"Look... more playlists\"}","title":"Inserting explicit new values"},{"location":"user-guide/manipulation/insert/#inserting-calculated-values","text":"Inserting explicit values is all well and good, but sometimes we want to defer some processing to the database. For example, perhaps we want to create a new invoice and use the current time as the invoice date. We could grab the current time using getCurrentTime and then use this to construct an explicit Haskell value, but this may cause synchronization issues for our application. To do this, beam allows us to specify arbitrary expressions as a source of values using the insertExpressions function. Haskell Postgres Sqlite runInsert $ insert ( invoice chinookDb ) $ insertExpressions [ Invoice ( val_ 800 ) ( CustomerId ( val_ 1 )) currentTimestamp_ ( val_ ( Address ( Just \"123 My Street\" ) ( Just \"Buenos Noches\" ) ( Just \"Rio\" ) ( Just \"Mozambique\" ) ( Just \"ABCDEF\" ))) ( val_ 1000 ) ] Just newInvoice <- runSelectReturningOne $ lookup_ ( invoice chinookDb ) ( InvoiceId 800 ) putStrLn ( \"Inserted invoice: \" ++ show newInvoice ) INSERT INTO \"Invoice\" ( \"InvoiceId\" , \"CustomerId\" , \"InvoiceDate\" , \"BillingAddress\" , \"BillingCity\" , \"BillingState\" , \"BillingCountry\" , \"BillingPostalCode\" , \"Total\" ) VALUES ( 800 , 1 , CURRENT_TIMESTAMP , '123 My Street' , 'Buenos Noches' , 'Rio' , 'Mozambique' , 'ABCDEF' , '1000.0' ); SELECT \"t0\" . \"InvoiceId\" AS \"res0\" , \"t0\" . \"CustomerId\" AS \"res1\" , \"t0\" . \"InvoiceDate\" AS \"res2\" , \"t0\" . \"BillingAddress\" AS \"res3\" , \"t0\" . \"BillingCity\" AS \"res4\" , \"t0\" . \"BillingState\" AS \"res5\" , \"t0\" . \"BillingCountry\" AS \"res6\" , \"t0\" . \"BillingPostalCode\" AS \"res7\" , \"t0\" . \"Total\" AS \"res8\" FROM \"Invoice\" AS \"t0\" WHERE ( \"t0\" . \"InvoiceId\" ) = ( 800 ); -- Output: Inserted invoice: Invoice {invoiceId = SqlSerial {unSerial = 800}, invoiceCustomer = CustomerId 1, invoiceDate = 2022-01-18 01:31:21.436807, invoiceBillingAddress = Address {address = Just \"123 My Street\", addressCity = Just \"Buenos Noches\", addressState = Just \"Rio\", addressCountry = Just \"Mozambique\", addressPostalCode = Just \"ABCDEF\"}, invoiceTotal = 1000.0} INSERT INTO \"Invoice\" ( \"InvoiceId\" , \"CustomerId\" , \"InvoiceDate\" , \"BillingAddress\" , \"BillingCity\" , \"BillingState\" , \"BillingCountry\" , \"BillingPostalCode\" , \"Total\" ) VALUES ( ? , ? , CURRENT_TIMESTAMP , ? , ? , ? , ? , ? , ? ); -- With values: [SQLInteger 800,SQLInteger 1,SQLText \"123 My Street\",SQLText \"Buenos Noches\",SQLText \"Rio\",SQLText \"Mozambique\",SQLText \"ABCDEF\",SQLText \"1000.0\"]; SELECT \"t0\" . \"InvoiceId\" AS \"res0\" , \"t0\" . \"CustomerId\" AS \"res1\" , \"t0\" . \"InvoiceDate\" AS \"res2\" , \"t0\" . \"BillingAddress\" AS \"res3\" , \"t0\" . \"BillingCity\" AS \"res4\" , \"t0\" . \"BillingState\" AS \"res5\" , \"t0\" . \"BillingCountry\" AS \"res6\" , \"t0\" . \"BillingPostalCode\" AS \"res7\" , \"t0\" . \"Total\" AS \"res8\" FROM \"Invoice\" AS \"t0\" WHERE ( \"t0\" . \"InvoiceId\" ) = ( ? ); -- With values: [SQLInteger 800]; -- Output: Inserted invoice: Invoice {invoiceId = SqlSerial {unSerial = 800}, invoiceCustomer = CustomerId 1, invoiceDate = 2022-01-18 01:31:24, invoiceBillingAddress = Address {address = Just \"123 My Street\", addressCity = Just \"Buenos Noches\", addressState = Just \"Rio\", addressCountry = Just \"Mozambique\", addressPostalCode = Just \"ABCDEF\"}, invoiceTotal = 1000.0} insertExpressions is strictly more general than insertValues . We can turn any insertValues to an insertExpressions by running every table value through the val_ function to convert a Haskell literal to an expression. For example, we can write the playlist example above as Haskell Postgres Sqlite runInsert $ insert ( playlist chinookDb ) $ insertExpressions [ val_ $ Playlist 700 ( Just \"My New Playlist\" ) , val_ $ Playlist 701 ( Just \"Another Playlist\" ) , val_ $ Playlist 702 ( Just \"Look... more playlists\" ) ] insertedPlaylists <- runSelectReturningList $ select $ filter_ ( \\ p -> playlistId p >=. 700 ) $ all_ ( playlist chinookDb ) putStrLn \"Inserted playlists:\" forM_ insertedPlaylists $ \\ p -> putStrLn ( show p ) INSERT INTO \"Playlist\" ( \"PlaylistId\" , \"Name\" ) VALUES ( 700 , 'My New Playlist' ), ( 701 , 'Another Playlist' ), ( 702 , 'Look... more playlists' ); SELECT \"t0\" . \"PlaylistId\" AS \"res0\" , \"t0\" . \"Name\" AS \"res1\" FROM \"Playlist\" AS \"t0\" WHERE ( \"t0\" . \"PlaylistId\" ) >= ( 700 ); -- Output: Inserted playlists: -- Output: Playlist {playlistId = 700, playlistName = Just \"My New Playlist\"} -- Output: Playlist {playlistId = 701, playlistName = Just \"Another Playlist\"} -- Output: Playlist {playlistId = 702, playlistName = Just \"Look... more playlists\"} INSERT INTO \"Playlist\" ( \"PlaylistId\" , \"Name\" ) VALUES ( ? , ? ), ( ? , ? ), ( ? , ? ); -- With values: [SQLInteger 700,SQLText \"My New Playlist\",SQLInteger 701,SQLText \"Another Playlist\",SQLInteger 702,SQLText \"Look... more playlists\"]; SELECT \"t0\" . \"PlaylistId\" AS \"res0\" , \"t0\" . \"Name\" AS \"res1\" FROM \"Playlist\" AS \"t0\" WHERE ( \"t0\" . \"PlaylistId\" ) >= ( ? ); -- With values: [SQLInteger 700]; -- Output: Inserted playlists: -- Output: Playlist {playlistId = 700, playlistName = Just \"My New Playlist\"} -- Output: Playlist {playlistId = 701, playlistName = Just \"Another Playlist\"} -- Output: Playlist {playlistId = 702, playlistName = Just \"Look... more playlists\"} One common use of insertExpressions_ is when adding new rows to tables where one field needs to be set to the default value. For example, auto-incrementing keys or random UUIDs are a common way to assign primary keys to rows. You can use insertExpressions_ using the default_ expression for each column that you want to use the default value for. For example, the query below adds a new invoice asking the database to assign a new id. Haskell Postgres runInsert $ insert ( invoice chinookDb ) $ insertExpressions [ Invoice default_ -- Ask the database to give us a default id ( val_ ( CustomerId 1 )) currentTimestamp_ ( val_ ( Address ( Just \"123 My Street\" ) ( Just \"Buenos Noches\" ) ( Just \"Rio\" ) ( Just \"Mozambique\" ) ( Just \"ABCDEF\" ))) ( val_ 1000 ) ] INSERT INTO \"Invoice\" ( \"InvoiceId\" , \"CustomerId\" , \"InvoiceDate\" , \"BillingAddress\" , \"BillingCity\" , \"BillingState\" , \"BillingCountry\" , \"BillingPostalCode\" , \"Total\" ) VALUES ( DEFAULT , 1 , CURRENT_TIMESTAMP , '123 My Street' , 'Buenos Noches' , 'Rio' , 'Mozambique' , 'ABCDEF' , '1000.0' ); Warning SQLite is a great little backend, but it doesn't support some standard SQL features, like the DEFAULT keyword in inserts. You can retrieve the same functionality by only inserting into a subset of columns. See the section on that below.","title":"Inserting calculated values"},{"location":"user-guide/manipulation/insert/#retrieving-the-rows-inserted","text":"However, now we have no way of knowing what value the database assigned. Unfortunately, there is no database-agnostic solution to this problem. However, it's a common enough use case that beam provides a backend-agnostic way for some backends. Backends that provide this functionality provide an instance of MonadBeamInsertReturning . In order to use this class, you'll need to explicitly import Database.Beam.Backend.SQL.BeamExtensions . Below, we've imported this module qualified. Haskell Postgres Sqlite [ newInvoice ] <- BeamExtensions . runInsertReturningList $ insert ( invoice chinookDb ) $ insertExpressions [ Invoice default_ -- Ask the database to give us a default id ( val_ ( CustomerId 1 )) currentTimestamp_ ( val_ ( Address ( Just \"123 My Street\" ) ( Just \"Buenos Noches\" ) ( Just \"Rio\" ) ( Just \"Mozambique\" ) ( Just \"ABCDEF\" ))) ( val_ 1000 ) ] putStrLn ( \"We inserted a new invoice, and the result was \" ++ show newInvoice ) INSERT INTO \"Invoice\" ( \"InvoiceId\" , \"CustomerId\" , \"InvoiceDate\" , \"BillingAddress\" , \"BillingCity\" , \"BillingState\" , \"BillingCountry\" , \"BillingPostalCode\" , \"Total\" ) VALUES ( DEFAULT , 1 , CURRENT_TIMESTAMP , '123 My Street' , 'Buenos Noches' , 'Rio' , 'Mozambique' , 'ABCDEF' , '1000.0' ) RETURNING \"InvoiceId\" , \"CustomerId\" , \"InvoiceDate\" , \"BillingAddress\" , \"BillingCity\" , \"BillingState\" , \"BillingCountry\" , \"BillingPostalCode\" , \"Total\" ; -- Output: We inserted a new invoice, and the result was Invoice {invoiceId = SqlSerial {unSerial = 501}, invoiceCustomer = CustomerId 1, invoiceDate = 2022-01-18 01:31:34.451145, invoiceBillingAddress = Address {address = Just \"123 My Street\", addressCity = Just \"Buenos Noches\", addressState = Just \"Rio\", addressCountry = Just \"Mozambique\", addressPostalCode = Just \"ABCDEF\"}, invoiceTotal = 1000.0} INSERT INTO \"Invoice\" ( \"CustomerId\" , \"InvoiceDate\" , \"BillingAddress\" , \"BillingCity\" , \"BillingState\" , \"BillingCountry\" , \"BillingPostalCode\" , \"Total\" ) VALUES ( ? , CURRENT_TIMESTAMP , ? , ? , ? , ? , ? , ? ); -- With values: [SQLInteger 1,SQLText \"123 My Street\",SQLText \"Buenos Noches\",SQLText \"Rio\",SQLText \"Mozambique\",SQLText \"ABCDEF\",SQLText \"1000.0\"]; -- Output: We inserted a new invoice, and the result was Invoice {invoiceId = SqlSerial {unSerial = 413}, invoiceCustomer = CustomerId 1, invoiceDate = 2022-01-18 01:31:37, invoiceBillingAddress = Address {address = Just \"123 My Street\", addressCity = Just \"Buenos Noches\", addressState = Just \"Rio\", addressCountry = Just \"Mozambique\", addressPostalCode = Just \"ABCDEF\"}, invoiceTotal = 1000.0} The pattern match on the single newInvoice is safe, even though its partial. In general, you can expect the same amount of rows returned as specified in your SqlInsertValues . If you know what this is statically, then you can feel free to pattern match directly. Otherwise (if you used insertFrom , for example), you'll need to handle the possibility that nothing was inserted. Note Although SQLite has no support for the DEFAULT clause, MonadBeamInsertReturning in beam-sqlite inserts rows one at a time and will detect usage of the DEFAULT keyword. The beam authors consider this okay. While most beam statements are guaranteed to translate directly to the underlying DBMS system, runInsertReturningList is explicitly marked as emulated functionality.","title":"Retrieving the rows inserted"},{"location":"user-guide/manipulation/insert/#inserting-from-the-result-of-a-select-statement","text":"Sometimes you want to use existing data to insert values. For example, perhaps we want to give every customer their own playlist, titled \" 's playlist\". We can use the insertFrom function to make a SqlInsertValues corresponding to the result of a query. Make sure to return a projection with the same 'shape' as your data. If not, you'll get a compile time error. For example, to create the playlists as above Haskell Postgres Sqlite runInsert $ insert ( playlist chinookDb ) $ insertFrom $ do c <- all_ ( customer chinookDb ) pure ( Playlist ( customerId c + 1000 ) ( just_ ( concat_ [ customerFirstName c , \"'s Playlist\" ]))) playlists <- runSelectReturningList $ select $ limit_ 10 $ orderBy_ ( \\ p -> asc_ ( playlistId p )) $ filter_ ( \\ p -> playlistId p >=. 1000 ) $ all_ ( playlist chinookDb ) putStrLn \"Inserted playlists\" forM_ playlists $ \\ playlist -> putStrLn ( \" - \" ++ show playlist ) INSERT INTO \"Playlist\" ( \"PlaylistId\" , \"Name\" ) SELECT ( \"t0\" . \"CustomerId\" ) + ( 1000 ) AS \"res0\" , CONCAT ( \"t0\" . \"FirstName\" , '''s Playlist' ) AS \"res1\" FROM \"Customer\" AS \"t0\" ; SELECT \"t0\" . \"PlaylistId\" AS \"res0\" , \"t0\" . \"Name\" AS \"res1\" FROM \"Playlist\" AS \"t0\" WHERE ( \"t0\" . \"PlaylistId\" ) >= ( 1000 ) ORDER BY \"t0\" . \"PlaylistId\" ASC LIMIT 10 ; -- Output: Inserted playlists -- Output: - Playlist {playlistId = 1001, playlistName = Just \"Lu\\237s's Playlist\"} -- Output: - Playlist {playlistId = 1002, playlistName = Just \"Leonie's Playlist\"} -- Output: - Playlist {playlistId = 1003, playlistName = Just \"Fran\\231ois's Playlist\"} -- Output: - Playlist {playlistId = 1004, playlistName = Just \"Bj\\345rn's Playlist\"} -- Output: - Playlist {playlistId = 1005, playlistName = Just \"Franti\\154ek's Playlist\"} -- Output: - Playlist {playlistId = 1006, playlistName = Just \"Helena's Playlist\"} -- Output: - Playlist {playlistId = 1007, playlistName = Just \"Astrid's Playlist\"} -- Output: - Playlist {playlistId = 1008, playlistName = Just \"Daan's Playlist\"} -- Output: - Playlist {playlistId = 1009, playlistName = Just \"Kara's Playlist\"} -- Output: - Playlist {playlistId = 1010, playlistName = Just \"Eduardo's Playlist\"} INSERT INTO \"Playlist\" ( \"PlaylistId\" , \"Name\" ) SELECT ( \"t0\" . \"CustomerId\" ) + ( ? ) AS \"res0\" , ( \"t0\" . \"FirstName\" || ( ? )) AS \"res1\" FROM \"Customer\" AS \"t0\" ; -- With values: [SQLInteger 1000,SQLText \"'s Playlist\"]; SELECT \"t0\" . \"PlaylistId\" AS \"res0\" , \"t0\" . \"Name\" AS \"res1\" FROM \"Playlist\" AS \"t0\" WHERE ( \"t0\" . \"PlaylistId\" ) >= ( ? ) ORDER BY \"t0\" . \"PlaylistId\" ASC LIMIT 10 ; -- With values: [SQLInteger 1000]; -- Output: Inserted playlists -- Output: - Playlist {playlistId = 1001, playlistName = Just \"Lu\\237s's Playlist\"} -- Output: - Playlist {playlistId = 1002, playlistName = Just \"Leonie's Playlist\"} -- Output: - Playlist {playlistId = 1003, playlistName = Just \"Fran\\231ois's Playlist\"} -- Output: - Playlist {playlistId = 1004, playlistName = Just \"Bj\\248rn's Playlist\"} -- Output: - Playlist {playlistId = 1005, playlistName = Just \"Franti\\353ek's Playlist\"} -- Output: - Playlist {playlistId = 1006, playlistName = Just \"Helena's Playlist\"} -- Output: - Playlist {playlistId = 1007, playlistName = Just \"Astrid's Playlist\"} -- Output: - Playlist {playlistId = 1008, playlistName = Just \"Daan's Playlist\"} -- Output: - Playlist {playlistId = 1009, playlistName = Just \"Kara's Playlist\"} -- Output: - Playlist {playlistId = 1010, playlistName = Just \"Eduardo's Playlist\"}","title":"Inserting from the result of a SELECT statement"},{"location":"user-guide/manipulation/insert/#choosing-a-subset-of-columns","text":"Above, we used the default_ clause to set a column to a default value. Unfortunately, not all backends support default_ (SQLite being a notable exception). Moreover, some INSERT forms simply can't use default_ , such as insertFrom_ (you can't return default_ from a query). The standard SQL tool used in these cases is limiting the inserted data to specific columns. For example, suppose we want to insert new invoices for every customer with today's date. We can use the insertOnly function to project which field's are being inserted. Haskell Postgres Sqlite runInsert $ insertOnly ( invoice chinookDb ) ( \\ i -> ( invoiceCustomer i , invoiceDate i , invoiceBillingAddress i , invoiceTotal i ) ) $ insertFrom $ do c <- all_ ( customer chinookDb ) -- We'll just charge each customer $10 to be mean! pure ( primaryKey c , currentTimestamp_ , customerAddress c , as_ @ Scientific $ val_ 10 ) INSERT INTO \"Invoice\" ( \"CustomerId\" , \"InvoiceDate\" , \"BillingAddress\" , \"BillingCity\" , \"BillingState\" , \"BillingCountry\" , \"BillingPostalCode\" , \"Total\" ) SELECT \"t0\" . \"CustomerId\" AS \"res0\" , CURRENT_TIMESTAMP AS \"res1\" , \"t0\" . \"Address\" AS \"res2\" , \"t0\" . \"City\" AS \"res3\" , \"t0\" . \"State\" AS \"res4\" , \"t0\" . \"Country\" AS \"res5\" , \"t0\" . \"PostalCode\" AS \"res6\" , '10.0' AS \"res7\" FROM \"Customer\" AS \"t0\" ; INSERT INTO \"Invoice\" ( \"CustomerId\" , \"InvoiceDate\" , \"BillingAddress\" , \"BillingCity\" , \"BillingState\" , \"BillingCountry\" , \"BillingPostalCode\" , \"Total\" ) SELECT \"t0\" . \"CustomerId\" AS \"res0\" , CURRENT_TIMESTAMP AS \"res1\" , \"t0\" . \"Address\" AS \"res2\" , \"t0\" . \"City\" AS \"res3\" , \"t0\" . \"State\" AS \"res4\" , \"t0\" . \"Country\" AS \"res5\" , \"t0\" . \"PostalCode\" AS \"res6\" , ? AS \"res7\" FROM \"Customer\" AS \"t0\" ; -- With values: [SQLText \"10.0\"];","title":"Choosing a subset of columns"},{"location":"user-guide/manipulation/insert/#inserting-nothing","text":"Oftentimes, the values to be inserted are generated automatically by some Haskell function, and you just insert the resulting list. Sometimes, these lists may be empty. If you blindly translated this into SQL, you'd end up with INSERT s with empty VALUE clauses, which are illegal. Beam actually handles this gracefully. If a SqlInsertValues has no rows to insert, the SqlInsert returned by insert will know that it is empty. Running this SqlInsert results in nothing being sent to the database, which you can verify below. Haskell Postgres Sqlite let superComplicatedAction = pure [] -- Hopefully, you're more creative! valuesToInsert <- superComplicatedAction putStrLn \"The following runInsert will send no commands to the database\" runInsert $ insert ( playlist chinookDb ) $ insertValues valuesToInsert putStrLn \"See! I told you!\" -- Output: The following runInsert will send no commands to the database -- Output: See! I told you! -- Output: The following runInsert will send no commands to the database -- Output: See! I told you!","title":"Inserting nothing"},{"location":"user-guide/manipulation/insert/#on-conflict","text":"Several backends (such as Postgres and SQLite) support ON CONFLICT subexpressions that specify what action to take when an INSERT statement conlicts with already present data. Beam support backend-agnostic ON CONFLICT statements via the BeamHasInsertOnConflict syntax. This class contains a new function to generate an SqlInsert . The insertOnConflict function can be used to attach ON CONFLICT actions to a SqlInsert . insertOnConflict :: Beamable table => DatabaseEntity be db ( TableEntity table ) -> SqlInsertValues be ( table ( QExpr be s )) -> SqlConflictTarget be table -> SqlConflictAction be table -> SqlInsert be table The SqlConflictTarget specifies on which kinds of conflicts the action should run. You have a few options anyConflict - run the action on any conflict conflictingFields - run the action only when certain fields conflict conflictingFieldsWhere - run the action only when certain fields conflict and a particular expression evaluates to true. The SqlConflictAction specifies what to do when a conflict happens. onConflictDoNothing - this cancels the insertion onConflictUpdateSet - sets fields to new values based on the current values onConflictUpdateSetWhere - sets fields to new values if a particular condition holds","title":"ON CONFLICT"},{"location":"user-guide/manipulation/insert/#acting-on-any-conflict","text":"A common use case of ON CONFLICT is to upsert rows into a database. Upsertion refers to only inserting a row if another conflicting row does not already exist. For example, if you have a new customer with primary key 42, and you don't know if it's in the database or not, but you want to insert it if not, you can use the insertOnConflict function with the anyConflict target. let newCustomer = Customer 42 \"John\" \"Doe\" Nothing ( Address ( Just \"Street\" ) ( Just \"City\" ) ( Just \"State\" ) Nothing Nothing ) Nothing Nothing \"john.doe@johndoe.com\" nothing_ runInsert $ insertOnConflict ( customer chinookDb ) ( insertValues [ newCustomer ]) anyConflict onConflictDoNothing","title":"Acting on any conflict"},{"location":"user-guide/manipulation/insert/#acting-only-on-certain-conflicts","text":"Sometimes you only want to perform an action if a certain constraint is violated. If the conflicting index or constraint is on a field you can specify which fields with the function conflictingFields . Haskell Postgres Sqlite --! import Database.Beam.Backend.SQL.BeamExtensions ( BeamHasInsertOnConflict ( .. )) let newCustomer = Customer 42 \"John\" \"Doe\" Nothing ( Address ( Just \"Street\" ) ( Just \"City\" ) ( Just \"State\" ) Nothing Nothing ) Nothing Nothing \"john.doe@johndoe.com\" nothing_ runInsert $ insertOnConflict ( customer chinookDb ) ( insertValues [ newCustomer ]) ( conflictingFields ( \\ tbl -> primaryKey tbl )) ( onConflictUpdateSet ( \\ fields oldValues -> fields <-. val_ newCustomer )) INSERT INTO \"Customer\" ( \"CustomerId\" , \"FirstName\" , \"LastName\" , \"Company\" , \"Address\" , \"City\" , \"State\" , \"Country\" , \"PostalCode\" , \"Phone\" , \"Fax\" , \"Email\" , \"SupportRepId\" ) VALUES ( 42 , 'John' , 'Doe' , null , 'Street' , 'City' , 'State' , null , null , null , null , 'john.doe@johndoe.com' , null ) ON CONFLICT ( \"CustomerId\" ) DO UPDATE SET \"CustomerId\" = ( 42 ), \"FirstName\" = ( 'John' ), \"LastName\" = ( 'Doe' ), \"Company\" = ( null ), \"Address\" = ( 'Street' ), \"City\" = ( 'City' ), \"State\" = ( 'State' ), \"Country\" = ( null ), \"PostalCode\" = ( null ), \"Phone\" = ( null ), \"Fax\" = ( null ), \"Email\" = ( 'john.doe@johndoe.com' ), \"SupportRepId\" = ( null ); INSERT INTO \"Customer\" ( \"CustomerId\" , \"FirstName\" , \"LastName\" , \"Company\" , \"Address\" , \"City\" , \"State\" , \"Country\" , \"PostalCode\" , \"Phone\" , \"Fax\" , \"Email\" , \"SupportRepId\" ) VALUES ( ? , ? , ? , NULL , ? , ? , ? , NULL , NULL , NULL , NULL , ? , NULL ) ON CONFLICT ( \"CustomerId\" ) DO UPDATE SET \"CustomerId\" = ? , \"FirstName\" = ? , \"LastName\" = ? , \"Company\" = NULL , \"Address\" = ? , \"City\" = ? , \"State\" = ? , \"Country\" = NULL , \"PostalCode\" = NULL , \"Phone\" = NULL , \"Fax\" = NULL , \"Email\" = ? , \"SupportRepId\" = NULL ; -- With values: [SQLInteger 42,SQLText \"John\",SQLText \"Doe\",SQLText \"Street\",SQLText \"City\",SQLText \"State\",SQLText \"john.doe@johndoe.com\",SQLInteger 42,SQLText \"John\",SQLText \"Doe\",SQLText \"Street\",SQLText \"City\",SQLText \"State\",SQLText \"john.doe@johndoe.com\"]; Tip To specify a conflict on the primary keys, use conflictingFields primaryKey . You can also specify how to change the record should it not match. For example, to append the e-mail as an alternate when you insert an existing row, you can use the oldValues argument to get access to the old value. Haskell Postgres Sqlite --! import Database.Beam.Backend.SQL.BeamExtensions ( BeamHasInsertOnConflict ( .. )) let newCustomer = Customer 42 \"John\" \"Doe\" Nothing ( Address ( Just \"Street\" ) ( Just \"City\" ) ( Just \"State\" ) Nothing Nothing ) Nothing Nothing \"john.doe@johndoe.com\" nothing_ runInsert $ insertOnConflict ( customer chinookDb ) ( insertValues [ newCustomer ]) ( conflictingFields ( \\ tbl -> primaryKey tbl )) ( onConflictUpdateSet ( \\ fields oldValues -> customerEmail fields <-. concat_ [ customerEmail oldValues , \";\" , val_ ( customerEmail newCustomer )])) INSERT INTO \"Customer\" ( \"CustomerId\" , \"FirstName\" , \"LastName\" , \"Company\" , \"Address\" , \"City\" , \"State\" , \"Country\" , \"PostalCode\" , \"Phone\" , \"Fax\" , \"Email\" , \"SupportRepId\" ) VALUES ( 42 , 'John' , 'Doe' , null , 'Street' , 'City' , 'State' , null , null , null , null , 'john.doe@johndoe.com' , null ) ON CONFLICT ( \"CustomerId\" ) DO UPDATE SET \"Email\" = ( CONCAT ( \"excluded\" . \"Email\" , ';' , 'john.doe@johndoe.com' )); INSERT INTO \"Customer\" ( \"CustomerId\" , \"FirstName\" , \"LastName\" , \"Company\" , \"Address\" , \"City\" , \"State\" , \"Country\" , \"PostalCode\" , \"Phone\" , \"Fax\" , \"Email\" , \"SupportRepId\" ) VALUES ( ? , ? , ? , NULL , ? , ? , ? , NULL , NULL , NULL , NULL , ? , NULL ) ON CONFLICT ( \"CustomerId\" ) DO UPDATE SET \"Email\" = ( \"excluded\" . \"Email\" || ( ? ) || ( ? )); -- With values: [SQLInteger 42,SQLText \"John\",SQLText \"Doe\",SQLText \"Street\",SQLText \"City\",SQLText \"State\",SQLText \"john.doe@johndoe.com\",SQLText \";\",SQLText \"john.doe@johndoe.com\"]; If you want to be even more particular and only do this transformation on rows corresponding to customers from one state, use conflictingFieldsWhere . Haskell Postgres Sqlite --! import Database.Beam.Backend.SQL.BeamExtensions ( BeamHasInsertOnConflict ( .. )) let newCustomer = Customer 42 \"John\" \"Doe\" Nothing ( Address ( Just \"Street\" ) ( Just \"City\" ) ( Just \"State\" ) Nothing Nothing ) Nothing Nothing \"john.doe@johndoe.com\" nothing_ runInsert $ insertOnConflict ( customer chinookDb ) ( insertValues [ newCustomer ]) ( conflictingFieldsWhere ( \\ tbl -> primaryKey tbl ) ( \\ tbl -> addressState ( customerAddress tbl ) ==. val_ ( Just \"CA\" ))) ( onConflictUpdateSet ( \\ fields oldValues -> customerEmail fields <-. concat_ [ customerEmail oldValues , \";\" , val_ ( customerEmail newCustomer )])) INSERT INTO \"Customer\" ( \"CustomerId\" , \"FirstName\" , \"LastName\" , \"Company\" , \"Address\" , \"City\" , \"State\" , \"Country\" , \"PostalCode\" , \"Phone\" , \"Fax\" , \"Email\" , \"SupportRepId\" ) VALUES ( 42 , 'John' , 'Doe' , null , 'Street' , 'City' , 'State' , null , null , null , null , 'john.doe@johndoe.com' , null ) ON CONFLICT ( \"CustomerId\" ) WHERE (( \"State\" ) IS NOT DISTINCT FROM ( 'CA' )) DO UPDATE SET \"Email\" = ( CONCAT ( \"excluded\" . \"Email\" , ';' , 'john.doe@johndoe.com' )); INSERT INTO \"Customer\" ( \"CustomerId\" , \"FirstName\" , \"LastName\" , \"Company\" , \"Address\" , \"City\" , \"State\" , \"Country\" , \"PostalCode\" , \"Phone\" , \"Fax\" , \"Email\" , \"SupportRepId\" ) VALUES ( ? , ? , ? , NULL , ? , ? , ? , NULL , NULL , NULL , NULL , ? , NULL ) ON CONFLICT ( \"CustomerId\" ) WHERE CASE WHEN (( \"State\" ) IS NULL ) AND (( ? ) IS NULL ) THEN ? WHEN (( \"State\" ) IS NULL ) OR (( ? ) IS NULL ) THEN ? ELSE ( \"State\" ) = ( ? ) END DO UPDATE SET \"Email\" = ( \"excluded\" . \"Email\" || ( ? ) || ( ? )); -- With values: [SQLInteger 42,SQLText \"John\",SQLText \"Doe\",SQLText \"Street\",SQLText \"City\",SQLText \"State\",SQLText \"john.doe@johndoe.com\",SQLText \"CA\",SQLInteger 1,SQLText \"CA\",SQLInteger 0,SQLText \"CA\",SQLText \";\",SQLText \"john.doe@johndoe.com\"];","title":"Acting only on certain conflicts"},{"location":"user-guide/manipulation/update/","text":"SQL UPDATE expressions allow you to update rows in the database. Beam supplies two functions to update a row in a beam database. Saving entire rows The save function allows you to save entire rows to a database. It generates a SET clause that sets the value of every non-primary-key column and a WHERE clause that matches on the primary key. For example, suppose we have a customer object representing Mark Philips let c :: Customer c = Customer 14 \"Mark\" \"Philips\" ( Just \"Telus\" ) ( Address ( Just \"8210 111 ST NW\" ) ( Just \"Edmonton\" ) ( Just \"AB\" ) ( Just \"Canada\" ) ( Just \"T6G 2C7\" )) ( Just \"+1 (780) 434-4554\" ) ( Just \"+1 (780) 434-5565\" ) \"mphilips12@shaw.ca\" ( EmployeeKey ( Just 5 )) Mark's phone number recently changed and we'd like to update the database based on our new customer object. We can use Haskell record update syntax to easily save the entire row. Haskell Postgres Sqlite Just c <- runSelectReturningOne $ lookup_ ( customer chinookDb ) ( CustomerId 14 ) putStrLn ( \"Old phone number is \" ++ show ( customerPhone c )) runUpdate $ save ( customer chinookDb ) ( c { customerPhone = Just \"+1 (123) 456-7890\" }) Just c' <- runSelectReturningOne $ lookup_ ( customer chinookDb ) ( CustomerId 14 ) putStrLn ( \"New phone number is \" ++ show ( customerPhone c' )) SELECT \"t0\" . \"CustomerId\" AS \"res0\" , \"t0\" . \"FirstName\" AS \"res1\" , \"t0\" . \"LastName\" AS \"res2\" , \"t0\" . \"Company\" AS \"res3\" , \"t0\" . \"Address\" AS \"res4\" , \"t0\" . \"City\" AS \"res5\" , \"t0\" . \"State\" AS \"res6\" , \"t0\" . \"Country\" AS \"res7\" , \"t0\" . \"PostalCode\" AS \"res8\" , \"t0\" . \"Phone\" AS \"res9\" , \"t0\" . \"Fax\" AS \"res10\" , \"t0\" . \"Email\" AS \"res11\" , \"t0\" . \"SupportRepId\" AS \"res12\" FROM \"Customer\" AS \"t0\" WHERE ( \"t0\" . \"CustomerId\" ) = ( 14 ); -- Output: Old phone number is Just \"+1 (780) 434-4554\" UPDATE \"Customer\" SET \"FirstName\" = 'Mark' , \"LastName\" = 'Philips' , \"Company\" = 'Telus' , \"Address\" = '8210 111 ST NW' , \"City\" = 'Edmonton' , \"State\" = 'AB' , \"Country\" = 'Canada' , \"PostalCode\" = 'T6G 2C7' , \"Phone\" = '+1 (123) 456-7890' , \"Fax\" = '+1 (780) 434-5565' , \"Email\" = 'mphilips12@shaw.ca' , \"SupportRepId\" = 5 WHERE ( 14 ) = ( \"CustomerId\" ); SELECT \"t0\" . \"CustomerId\" AS \"res0\" , \"t0\" . \"FirstName\" AS \"res1\" , \"t0\" . \"LastName\" AS \"res2\" , \"t0\" . \"Company\" AS \"res3\" , \"t0\" . \"Address\" AS \"res4\" , \"t0\" . \"City\" AS \"res5\" , \"t0\" . \"State\" AS \"res6\" , \"t0\" . \"Country\" AS \"res7\" , \"t0\" . \"PostalCode\" AS \"res8\" , \"t0\" . \"Phone\" AS \"res9\" , \"t0\" . \"Fax\" AS \"res10\" , \"t0\" . \"Email\" AS \"res11\" , \"t0\" . \"SupportRepId\" AS \"res12\" FROM \"Customer\" AS \"t0\" WHERE ( \"t0\" . \"CustomerId\" ) = ( 14 ); -- Output: New phone number is Just \"+1 (123) 456-7890\" SELECT \"t0\" . \"CustomerId\" AS \"res0\" , \"t0\" . \"FirstName\" AS \"res1\" , \"t0\" . \"LastName\" AS \"res2\" , \"t0\" . \"Company\" AS \"res3\" , \"t0\" . \"Address\" AS \"res4\" , \"t0\" . \"City\" AS \"res5\" , \"t0\" . \"State\" AS \"res6\" , \"t0\" . \"Country\" AS \"res7\" , \"t0\" . \"PostalCode\" AS \"res8\" , \"t0\" . \"Phone\" AS \"res9\" , \"t0\" . \"Fax\" AS \"res10\" , \"t0\" . \"Email\" AS \"res11\" , \"t0\" . \"SupportRepId\" AS \"res12\" FROM \"Customer\" AS \"t0\" WHERE ( \"t0\" . \"CustomerId\" ) = ( ? ); -- With values: [SQLInteger 14]; -- Output: Old phone number is Just \"+1 (780) 434-4554\" UPDATE \"Customer\" SET \"FirstName\" =? , \"LastName\" =? , \"Company\" =? , \"Address\" =? , \"City\" =? , \"State\" =? , \"Country\" =? , \"PostalCode\" =? , \"Phone\" =? , \"Fax\" =? , \"Email\" =? , \"SupportRepId\" =? WHERE ( ? ) = ( \"CustomerId\" ); -- With values: [SQLText \"Mark\",SQLText \"Philips\",SQLText \"Telus\",SQLText \"8210 111 ST NW\",SQLText \"Edmonton\",SQLText \"AB\",SQLText \"Canada\",SQLText \"T6G 2C7\",SQLText \"+1 (123) 456-7890\",SQLText \"+1 (780) 434-5565\",SQLText \"mphilips12@shaw.ca\",SQLInteger 5,SQLInteger 14]; SELECT \"t0\" . \"CustomerId\" AS \"res0\" , \"t0\" . \"FirstName\" AS \"res1\" , \"t0\" . \"LastName\" AS \"res2\" , \"t0\" . \"Company\" AS \"res3\" , \"t0\" . \"Address\" AS \"res4\" , \"t0\" . \"City\" AS \"res5\" , \"t0\" . \"State\" AS \"res6\" , \"t0\" . \"Country\" AS \"res7\" , \"t0\" . \"PostalCode\" AS \"res8\" , \"t0\" . \"Phone\" AS \"res9\" , \"t0\" . \"Fax\" AS \"res10\" , \"t0\" . \"Email\" AS \"res11\" , \"t0\" . \"SupportRepId\" AS \"res12\" FROM \"Customer\" AS \"t0\" WHERE ( \"t0\" . \"CustomerId\" ) = ( ? ); -- With values: [SQLInteger 14]; -- Output: New phone number is Just \"+1 (123) 456-7890\" The save function generates a value of type SqlUpdate syntax CustomerT , where syntax is the type of the appropriate backend syntax ( SqliteCommandSyntax for beam-sqlite and PgCommandSyntax for beam-postgres ). Like select and the runSelect* functions, we use the runUpdate function to run the command against the database Fine-grained updates While save is useful when many fields may have changed, often times you only want to update a few columns. Moreover, if you have several large columns, using save may end up sending huge pieces of data to the database. The SQL UPDATE syntax allows you to set or modify each column individually and to even calculate a new value based off the result of an expression. The beam update function exposes this functionality. update takes a table, a set of assignments (which can be combined monoidally), and a boolean expression, and returns a SqlUpdate . For example, suppose Canada and the USA became one country and we needed to update all customer addresses to reflect that. Haskell Postgres Sqlite Just canadianCount <- runSelectReturningOne $ select $ aggregate_ ( \\ _ -> as_ @ Int32 countAll_ ) $ filter_ ( \\ c -> addressCountry ( customerAddress c ) ==. val_ ( Just \"Canada\" )) $ all_ ( customer chinookDb ) Just usaCount <- runSelectReturningOne $ select $ aggregate_ ( \\ _ -> as_ @ Int32 countAll_ ) $ filter_ ( \\ c -> addressCountry ( customerAddress c ) ==. val_ ( Just \"USA\" )) $ all_ ( customer chinookDb ) putStrLn ( \"Before, there were \" ++ show canadianCount ++ \" addresses in Canada and \" ++ show usaCount ++ \" in the USA.\" ) -- This is the important part! runUpdate $ update ( customer chinookDb ) ( \\ c -> addressCountry ( customerAddress c ) <-. val_ ( Just \"USA\" )) ( \\ c -> addressCountry ( customerAddress c ) ==. val_ ( Just \"Canada\" )) Just canadianCount' <- runSelectReturningOne $ select $ aggregate_ ( \\ _ -> as_ @ Int32 countAll_ ) $ filter_ ( \\ c -> addressCountry ( customerAddress c ) ==. val_ ( Just \"Canada\" )) $ all_ ( customer chinookDb ) Just usaCount' <- runSelectReturningOne $ select $ aggregate_ ( \\ _ -> as_ @ Int32 countAll_ ) $ filter_ ( \\ c -> addressCountry ( customerAddress c ) ==. val_ ( Just \"USA\" )) $ all_ ( customer chinookDb ) putStrLn ( \"Now, there are \" ++ show canadianCount' ++ \" addresses in Canada and \" ++ show usaCount' ++ \" in the USA.\" ) SELECT COUNT ( * ) AS \"res0\" FROM \"Customer\" AS \"t0\" WHERE ( \"t0\" . \"Country\" ) IS NOT DISTINCT FROM ( 'Canada' ); SELECT COUNT ( * ) AS \"res0\" FROM \"Customer\" AS \"t0\" WHERE ( \"t0\" . \"Country\" ) IS NOT DISTINCT FROM ( 'USA' ); -- Output: Before, there were 8 addresses in Canada and 13 in the USA. UPDATE \"Customer\" SET \"Country\" = 'USA' WHERE ( \"Country\" ) IS NOT DISTINCT FROM ( 'Canada' ); SELECT COUNT ( * ) AS \"res0\" FROM \"Customer\" AS \"t0\" WHERE ( \"t0\" . \"Country\" ) IS NOT DISTINCT FROM ( 'Canada' ); SELECT COUNT ( * ) AS \"res0\" FROM \"Customer\" AS \"t0\" WHERE ( \"t0\" . \"Country\" ) IS NOT DISTINCT FROM ( 'USA' ); -- Output: Now, there are 0 addresses in Canada and 21 in the USA. SELECT COUNT ( * ) AS \"res0\" FROM \"Customer\" AS \"t0\" WHERE CASE WHEN (( \"t0\" . \"Country\" ) IS NULL ) AND (( ? ) IS NULL ) THEN ? WHEN (( \"t0\" . \"Country\" ) IS NULL ) OR (( ? ) IS NULL ) THEN ? ELSE ( \"t0\" . \"Country\" ) = ( ? ) END ; -- With values: [SQLText \"Canada\",SQLInteger 1,SQLText \"Canada\",SQLInteger 0,SQLText \"Canada\"]; SELECT COUNT ( * ) AS \"res0\" FROM \"Customer\" AS \"t0\" WHERE CASE WHEN (( \"t0\" . \"Country\" ) IS NULL ) AND (( ? ) IS NULL ) THEN ? WHEN (( \"t0\" . \"Country\" ) IS NULL ) OR (( ? ) IS NULL ) THEN ? ELSE ( \"t0\" . \"Country\" ) = ( ? ) END ; -- With values: [SQLText \"USA\",SQLInteger 1,SQLText \"USA\",SQLInteger 0,SQLText \"USA\"]; -- Output: Before, there were 8 addresses in Canada and 13 in the USA. UPDATE \"Customer\" SET \"Country\" =? WHERE CASE WHEN (( \"Country\" ) IS NULL ) AND (( ? ) IS NULL ) THEN ? WHEN (( \"Country\" ) IS NULL ) OR (( ? ) IS NULL ) THEN ? ELSE ( \"Country\" ) = ( ? ) END ; -- With values: [SQLText \"USA\",SQLText \"Canada\",SQLInteger 1,SQLText \"Canada\",SQLInteger 0,SQLText \"Canada\"]; SELECT COUNT ( * ) AS \"res0\" FROM \"Customer\" AS \"t0\" WHERE CASE WHEN (( \"t0\" . \"Country\" ) IS NULL ) AND (( ? ) IS NULL ) THEN ? WHEN (( \"t0\" . \"Country\" ) IS NULL ) OR (( ? ) IS NULL ) THEN ? ELSE ( \"t0\" . \"Country\" ) = ( ? ) END ; -- With values: [SQLText \"Canada\",SQLInteger 1,SQLText \"Canada\",SQLInteger 0,SQLText \"Canada\"]; SELECT COUNT ( * ) AS \"res0\" FROM \"Customer\" AS \"t0\" WHERE CASE WHEN (( \"t0\" . \"Country\" ) IS NULL ) AND (( ? ) IS NULL ) THEN ? WHEN (( \"t0\" . \"Country\" ) IS NULL ) OR (( ? ) IS NULL ) THEN ? ELSE ( \"t0\" . \"Country\" ) = ( ? ) END ; -- With values: [SQLText \"USA\",SQLInteger 1,SQLText \"USA\",SQLInteger 0,SQLText \"USA\"]; -- Output: Now, there are 0 addresses in Canada and 21 in the USA. We can update columns based on their old value as well, using the current_ function. For example, suppose we wanted to fudge our sales data a bit and double quantity of every line item. Haskell Postgres Sqlite Just totalLineItems <- runSelectReturningOne $ select $ aggregate_ ( \\ ln -> sum_ ( invoiceLineQuantity ln )) $ all_ ( invoiceLine chinookDb ) putStrLn ( \"Before, we had \" ++ show totalLineItems ++ \" total products sold \\n \" ) runUpdate $ update ( invoiceLine chinookDb ) ( \\ ln -> invoiceLineQuantity ln <-. current_ ( invoiceLineQuantity ln ) * 2 ) ( \\ _ -> val_ True ) Just totalLineItems' <- runSelectReturningOne $ select $ aggregate_ ( \\ ln -> sum_ ( invoiceLineQuantity ln )) $ all_ ( invoiceLine chinookDb ) putStrLn ( \"With a few simple lines, we've double our sales figure to \" ++ show totalLineItems' ++ \" products sold!\" ) SELECT SUM ( \"t0\" . \"Quantity\" ) AS \"res0\" FROM \"InvoiceLine\" AS \"t0\" ; -- Output: Before, we had Just 2240 total products sold UPDATE \"InvoiceLine\" SET \"Quantity\" = ( \"Quantity\" ) * ( 2 ) WHERE true ; SELECT SUM ( \"t0\" . \"Quantity\" ) AS \"res0\" FROM \"InvoiceLine\" AS \"t0\" ; -- Output: With a few simple lines, we've double our sales figure to Just 4480 products sold! SELECT SUM ( \"t0\" . \"Quantity\" ) AS \"res0\" FROM \"InvoiceLine\" AS \"t0\" ; -- With values: []; -- Output: Before, we had Just 2240 total products sold UPDATE \"InvoiceLine\" SET \"Quantity\" = ( \"Quantity\" ) * ( ? ) WHERE ? ; -- With values: [SQLInteger 2,SQLInteger 1]; SELECT SUM ( \"t0\" . \"Quantity\" ) AS \"res0\" FROM \"InvoiceLine\" AS \"t0\" ; -- With values: []; -- Output: With a few simple lines, we've double our sales figure to Just 4480 products sold! Amazing! A few simple lines, and we've doubled our sales -- beam is awesome!","title":"UPDATE"},{"location":"user-guide/manipulation/update/#saving-entire-rows","text":"The save function allows you to save entire rows to a database. It generates a SET clause that sets the value of every non-primary-key column and a WHERE clause that matches on the primary key. For example, suppose we have a customer object representing Mark Philips let c :: Customer c = Customer 14 \"Mark\" \"Philips\" ( Just \"Telus\" ) ( Address ( Just \"8210 111 ST NW\" ) ( Just \"Edmonton\" ) ( Just \"AB\" ) ( Just \"Canada\" ) ( Just \"T6G 2C7\" )) ( Just \"+1 (780) 434-4554\" ) ( Just \"+1 (780) 434-5565\" ) \"mphilips12@shaw.ca\" ( EmployeeKey ( Just 5 )) Mark's phone number recently changed and we'd like to update the database based on our new customer object. We can use Haskell record update syntax to easily save the entire row. Haskell Postgres Sqlite Just c <- runSelectReturningOne $ lookup_ ( customer chinookDb ) ( CustomerId 14 ) putStrLn ( \"Old phone number is \" ++ show ( customerPhone c )) runUpdate $ save ( customer chinookDb ) ( c { customerPhone = Just \"+1 (123) 456-7890\" }) Just c' <- runSelectReturningOne $ lookup_ ( customer chinookDb ) ( CustomerId 14 ) putStrLn ( \"New phone number is \" ++ show ( customerPhone c' )) SELECT \"t0\" . \"CustomerId\" AS \"res0\" , \"t0\" . \"FirstName\" AS \"res1\" , \"t0\" . \"LastName\" AS \"res2\" , \"t0\" . \"Company\" AS \"res3\" , \"t0\" . \"Address\" AS \"res4\" , \"t0\" . \"City\" AS \"res5\" , \"t0\" . \"State\" AS \"res6\" , \"t0\" . \"Country\" AS \"res7\" , \"t0\" . \"PostalCode\" AS \"res8\" , \"t0\" . \"Phone\" AS \"res9\" , \"t0\" . \"Fax\" AS \"res10\" , \"t0\" . \"Email\" AS \"res11\" , \"t0\" . \"SupportRepId\" AS \"res12\" FROM \"Customer\" AS \"t0\" WHERE ( \"t0\" . \"CustomerId\" ) = ( 14 ); -- Output: Old phone number is Just \"+1 (780) 434-4554\" UPDATE \"Customer\" SET \"FirstName\" = 'Mark' , \"LastName\" = 'Philips' , \"Company\" = 'Telus' , \"Address\" = '8210 111 ST NW' , \"City\" = 'Edmonton' , \"State\" = 'AB' , \"Country\" = 'Canada' , \"PostalCode\" = 'T6G 2C7' , \"Phone\" = '+1 (123) 456-7890' , \"Fax\" = '+1 (780) 434-5565' , \"Email\" = 'mphilips12@shaw.ca' , \"SupportRepId\" = 5 WHERE ( 14 ) = ( \"CustomerId\" ); SELECT \"t0\" . \"CustomerId\" AS \"res0\" , \"t0\" . \"FirstName\" AS \"res1\" , \"t0\" . \"LastName\" AS \"res2\" , \"t0\" . \"Company\" AS \"res3\" , \"t0\" . \"Address\" AS \"res4\" , \"t0\" . \"City\" AS \"res5\" , \"t0\" . \"State\" AS \"res6\" , \"t0\" . \"Country\" AS \"res7\" , \"t0\" . \"PostalCode\" AS \"res8\" , \"t0\" . \"Phone\" AS \"res9\" , \"t0\" . \"Fax\" AS \"res10\" , \"t0\" . \"Email\" AS \"res11\" , \"t0\" . \"SupportRepId\" AS \"res12\" FROM \"Customer\" AS \"t0\" WHERE ( \"t0\" . \"CustomerId\" ) = ( 14 ); -- Output: New phone number is Just \"+1 (123) 456-7890\" SELECT \"t0\" . \"CustomerId\" AS \"res0\" , \"t0\" . \"FirstName\" AS \"res1\" , \"t0\" . \"LastName\" AS \"res2\" , \"t0\" . \"Company\" AS \"res3\" , \"t0\" . \"Address\" AS \"res4\" , \"t0\" . \"City\" AS \"res5\" , \"t0\" . \"State\" AS \"res6\" , \"t0\" . \"Country\" AS \"res7\" , \"t0\" . \"PostalCode\" AS \"res8\" , \"t0\" . \"Phone\" AS \"res9\" , \"t0\" . \"Fax\" AS \"res10\" , \"t0\" . \"Email\" AS \"res11\" , \"t0\" . \"SupportRepId\" AS \"res12\" FROM \"Customer\" AS \"t0\" WHERE ( \"t0\" . \"CustomerId\" ) = ( ? ); -- With values: [SQLInteger 14]; -- Output: Old phone number is Just \"+1 (780) 434-4554\" UPDATE \"Customer\" SET \"FirstName\" =? , \"LastName\" =? , \"Company\" =? , \"Address\" =? , \"City\" =? , \"State\" =? , \"Country\" =? , \"PostalCode\" =? , \"Phone\" =? , \"Fax\" =? , \"Email\" =? , \"SupportRepId\" =? WHERE ( ? ) = ( \"CustomerId\" ); -- With values: [SQLText \"Mark\",SQLText \"Philips\",SQLText \"Telus\",SQLText \"8210 111 ST NW\",SQLText \"Edmonton\",SQLText \"AB\",SQLText \"Canada\",SQLText \"T6G 2C7\",SQLText \"+1 (123) 456-7890\",SQLText \"+1 (780) 434-5565\",SQLText \"mphilips12@shaw.ca\",SQLInteger 5,SQLInteger 14]; SELECT \"t0\" . \"CustomerId\" AS \"res0\" , \"t0\" . \"FirstName\" AS \"res1\" , \"t0\" . \"LastName\" AS \"res2\" , \"t0\" . \"Company\" AS \"res3\" , \"t0\" . \"Address\" AS \"res4\" , \"t0\" . \"City\" AS \"res5\" , \"t0\" . \"State\" AS \"res6\" , \"t0\" . \"Country\" AS \"res7\" , \"t0\" . \"PostalCode\" AS \"res8\" , \"t0\" . \"Phone\" AS \"res9\" , \"t0\" . \"Fax\" AS \"res10\" , \"t0\" . \"Email\" AS \"res11\" , \"t0\" . \"SupportRepId\" AS \"res12\" FROM \"Customer\" AS \"t0\" WHERE ( \"t0\" . \"CustomerId\" ) = ( ? ); -- With values: [SQLInteger 14]; -- Output: New phone number is Just \"+1 (123) 456-7890\" The save function generates a value of type SqlUpdate syntax CustomerT , where syntax is the type of the appropriate backend syntax ( SqliteCommandSyntax for beam-sqlite and PgCommandSyntax for beam-postgres ). Like select and the runSelect* functions, we use the runUpdate function to run the command against the database","title":"Saving entire rows"},{"location":"user-guide/manipulation/update/#fine-grained-updates","text":"While save is useful when many fields may have changed, often times you only want to update a few columns. Moreover, if you have several large columns, using save may end up sending huge pieces of data to the database. The SQL UPDATE syntax allows you to set or modify each column individually and to even calculate a new value based off the result of an expression. The beam update function exposes this functionality. update takes a table, a set of assignments (which can be combined monoidally), and a boolean expression, and returns a SqlUpdate . For example, suppose Canada and the USA became one country and we needed to update all customer addresses to reflect that. Haskell Postgres Sqlite Just canadianCount <- runSelectReturningOne $ select $ aggregate_ ( \\ _ -> as_ @ Int32 countAll_ ) $ filter_ ( \\ c -> addressCountry ( customerAddress c ) ==. val_ ( Just \"Canada\" )) $ all_ ( customer chinookDb ) Just usaCount <- runSelectReturningOne $ select $ aggregate_ ( \\ _ -> as_ @ Int32 countAll_ ) $ filter_ ( \\ c -> addressCountry ( customerAddress c ) ==. val_ ( Just \"USA\" )) $ all_ ( customer chinookDb ) putStrLn ( \"Before, there were \" ++ show canadianCount ++ \" addresses in Canada and \" ++ show usaCount ++ \" in the USA.\" ) -- This is the important part! runUpdate $ update ( customer chinookDb ) ( \\ c -> addressCountry ( customerAddress c ) <-. val_ ( Just \"USA\" )) ( \\ c -> addressCountry ( customerAddress c ) ==. val_ ( Just \"Canada\" )) Just canadianCount' <- runSelectReturningOne $ select $ aggregate_ ( \\ _ -> as_ @ Int32 countAll_ ) $ filter_ ( \\ c -> addressCountry ( customerAddress c ) ==. val_ ( Just \"Canada\" )) $ all_ ( customer chinookDb ) Just usaCount' <- runSelectReturningOne $ select $ aggregate_ ( \\ _ -> as_ @ Int32 countAll_ ) $ filter_ ( \\ c -> addressCountry ( customerAddress c ) ==. val_ ( Just \"USA\" )) $ all_ ( customer chinookDb ) putStrLn ( \"Now, there are \" ++ show canadianCount' ++ \" addresses in Canada and \" ++ show usaCount' ++ \" in the USA.\" ) SELECT COUNT ( * ) AS \"res0\" FROM \"Customer\" AS \"t0\" WHERE ( \"t0\" . \"Country\" ) IS NOT DISTINCT FROM ( 'Canada' ); SELECT COUNT ( * ) AS \"res0\" FROM \"Customer\" AS \"t0\" WHERE ( \"t0\" . \"Country\" ) IS NOT DISTINCT FROM ( 'USA' ); -- Output: Before, there were 8 addresses in Canada and 13 in the USA. UPDATE \"Customer\" SET \"Country\" = 'USA' WHERE ( \"Country\" ) IS NOT DISTINCT FROM ( 'Canada' ); SELECT COUNT ( * ) AS \"res0\" FROM \"Customer\" AS \"t0\" WHERE ( \"t0\" . \"Country\" ) IS NOT DISTINCT FROM ( 'Canada' ); SELECT COUNT ( * ) AS \"res0\" FROM \"Customer\" AS \"t0\" WHERE ( \"t0\" . \"Country\" ) IS NOT DISTINCT FROM ( 'USA' ); -- Output: Now, there are 0 addresses in Canada and 21 in the USA. SELECT COUNT ( * ) AS \"res0\" FROM \"Customer\" AS \"t0\" WHERE CASE WHEN (( \"t0\" . \"Country\" ) IS NULL ) AND (( ? ) IS NULL ) THEN ? WHEN (( \"t0\" . \"Country\" ) IS NULL ) OR (( ? ) IS NULL ) THEN ? ELSE ( \"t0\" . \"Country\" ) = ( ? ) END ; -- With values: [SQLText \"Canada\",SQLInteger 1,SQLText \"Canada\",SQLInteger 0,SQLText \"Canada\"]; SELECT COUNT ( * ) AS \"res0\" FROM \"Customer\" AS \"t0\" WHERE CASE WHEN (( \"t0\" . \"Country\" ) IS NULL ) AND (( ? ) IS NULL ) THEN ? WHEN (( \"t0\" . \"Country\" ) IS NULL ) OR (( ? ) IS NULL ) THEN ? ELSE ( \"t0\" . \"Country\" ) = ( ? ) END ; -- With values: [SQLText \"USA\",SQLInteger 1,SQLText \"USA\",SQLInteger 0,SQLText \"USA\"]; -- Output: Before, there were 8 addresses in Canada and 13 in the USA. UPDATE \"Customer\" SET \"Country\" =? WHERE CASE WHEN (( \"Country\" ) IS NULL ) AND (( ? ) IS NULL ) THEN ? WHEN (( \"Country\" ) IS NULL ) OR (( ? ) IS NULL ) THEN ? ELSE ( \"Country\" ) = ( ? ) END ; -- With values: [SQLText \"USA\",SQLText \"Canada\",SQLInteger 1,SQLText \"Canada\",SQLInteger 0,SQLText \"Canada\"]; SELECT COUNT ( * ) AS \"res0\" FROM \"Customer\" AS \"t0\" WHERE CASE WHEN (( \"t0\" . \"Country\" ) IS NULL ) AND (( ? ) IS NULL ) THEN ? WHEN (( \"t0\" . \"Country\" ) IS NULL ) OR (( ? ) IS NULL ) THEN ? ELSE ( \"t0\" . \"Country\" ) = ( ? ) END ; -- With values: [SQLText \"Canada\",SQLInteger 1,SQLText \"Canada\",SQLInteger 0,SQLText \"Canada\"]; SELECT COUNT ( * ) AS \"res0\" FROM \"Customer\" AS \"t0\" WHERE CASE WHEN (( \"t0\" . \"Country\" ) IS NULL ) AND (( ? ) IS NULL ) THEN ? WHEN (( \"t0\" . \"Country\" ) IS NULL ) OR (( ? ) IS NULL ) THEN ? ELSE ( \"t0\" . \"Country\" ) = ( ? ) END ; -- With values: [SQLText \"USA\",SQLInteger 1,SQLText \"USA\",SQLInteger 0,SQLText \"USA\"]; -- Output: Now, there are 0 addresses in Canada and 21 in the USA. We can update columns based on their old value as well, using the current_ function. For example, suppose we wanted to fudge our sales data a bit and double quantity of every line item. Haskell Postgres Sqlite Just totalLineItems <- runSelectReturningOne $ select $ aggregate_ ( \\ ln -> sum_ ( invoiceLineQuantity ln )) $ all_ ( invoiceLine chinookDb ) putStrLn ( \"Before, we had \" ++ show totalLineItems ++ \" total products sold \\n \" ) runUpdate $ update ( invoiceLine chinookDb ) ( \\ ln -> invoiceLineQuantity ln <-. current_ ( invoiceLineQuantity ln ) * 2 ) ( \\ _ -> val_ True ) Just totalLineItems' <- runSelectReturningOne $ select $ aggregate_ ( \\ ln -> sum_ ( invoiceLineQuantity ln )) $ all_ ( invoiceLine chinookDb ) putStrLn ( \"With a few simple lines, we've double our sales figure to \" ++ show totalLineItems' ++ \" products sold!\" ) SELECT SUM ( \"t0\" . \"Quantity\" ) AS \"res0\" FROM \"InvoiceLine\" AS \"t0\" ; -- Output: Before, we had Just 2240 total products sold UPDATE \"InvoiceLine\" SET \"Quantity\" = ( \"Quantity\" ) * ( 2 ) WHERE true ; SELECT SUM ( \"t0\" . \"Quantity\" ) AS \"res0\" FROM \"InvoiceLine\" AS \"t0\" ; -- Output: With a few simple lines, we've double our sales figure to Just 4480 products sold! SELECT SUM ( \"t0\" . \"Quantity\" ) AS \"res0\" FROM \"InvoiceLine\" AS \"t0\" ; -- With values: []; -- Output: Before, we had Just 2240 total products sold UPDATE \"InvoiceLine\" SET \"Quantity\" = ( \"Quantity\" ) * ( ? ) WHERE ? ; -- With values: [SQLInteger 2,SQLInteger 1]; SELECT SUM ( \"t0\" . \"Quantity\" ) AS \"res0\" FROM \"InvoiceLine\" AS \"t0\" ; -- With values: []; -- Output: With a few simple lines, we've double our sales figure to Just 4480 products sold! Amazing! A few simple lines, and we've doubled our sales -- beam is awesome!","title":"Fine-grained updates"},{"location":"user-guide/queries/advanced-features/","text":"This page documents other advanced features that beam supports across backends that support them. SQL2003 T611: Elementary OLAP operations This optional SQL2003 feature allows attaching arbitrary FILTER (WHERE ..) clauses to aggregates. During querying only rows matching the given expression are included in computing the aggregate. This can often be simulated in other databases by an appropriate CASE expression, but beam will not do this translation. Haskell Postgres aggregate_ ( \\ i -> ( group_ ( invoiceCustomer i ), as_ @ Int32 $ countAll_ ` filterWhere_ ` ( invoiceTotal i >. 500 ), as_ @ Int32 $ countAll_ ` filterWhere_ ` ( invoiceTotal i <. 100 ))) $ all_ ( invoice chinookDb ) SELECT \"t0\" . \"CustomerId\" AS \"res0\" , COUNT ( * ) FILTER ( WHERE ( \"t0\" . \"Total\" ) > ( '500.0' )) AS \"res1\" , COUNT ( * ) FILTER ( WHERE ( \"t0\" . \"Total\" ) < ( '100.0' )) AS \"res2\" FROM \"Invoice\" AS \"t0\" GROUP BY \"t0\" . \"CustomerId\" These combine as you'd expect with window functions. For example, to return each invoice along with the average total of all invoices by the same customer where the invoice was billed to an address in Los Angeles, Haskell Postgres withWindow_ ( \\ i -> frame_ ( partitionBy_ ( invoiceCustomer i )) noOrder_ noBounds_ ) ( \\ i w -> ( i , avg_ ( invoiceTotal i ) ` filterWhere_ ` ( addressCity ( invoiceBillingAddress i ) ==. just_ \"Los Angeles\" ) ` over_ ` w )) ( all_ ( invoice chinookDb )) SELECT \"t0\" . \"InvoiceId\" AS \"res0\" , \"t0\" . \"CustomerId\" AS \"res1\" , \"t0\" . \"InvoiceDate\" AS \"res2\" , \"t0\" . \"BillingAddress\" AS \"res3\" , \"t0\" . \"BillingCity\" AS \"res4\" , \"t0\" . \"BillingState\" AS \"res5\" , \"t0\" . \"BillingCountry\" AS \"res6\" , \"t0\" . \"BillingPostalCode\" AS \"res7\" , \"t0\" . \"Total\" AS \"res8\" , AVG ( \"t0\" . \"Total\" ) FILTER ( WHERE ( \"t0\" . \"BillingCity\" ) IS NOT DISTINCT FROM ( 'Los Angeles' )) OVER ( PARTITION BY \"t0\" . \"CustomerId\" ) AS \"res9\" FROM \"Invoice\" AS \"t0\" Danger\" FILTER (WHERE ..) must be applied directly to a SQL aggregate function, but this isn't enforced at compile time. This may be fixed in a later version of beam. This extension also provides various window functions for SQL. The only one beam currently implements is RANK() via the rank_ function. Contributions are appreciated! Null Ordering This optional SQL2003 feature allows nulls to appear before or after non-null values in the sort ordering. Haskell Postgres limit_ 10 $ orderBy_ ( \\ e -> ( asc_ ( addressState ( employeeAddress e )), nullsLast_ ( desc_ ( addressCity ( employeeAddress e ))))) $ all_ ( employee chinookDb ) SELECT \"t0\" . \"EmployeeId\" AS \"res0\" , \"t0\" . \"LastName\" AS \"res1\" , \"t0\" . \"FirstName\" AS \"res2\" , \"t0\" . \"Title\" AS \"res3\" , \"t0\" . \"ReportsTo\" AS \"res4\" , \"t0\" . \"BirthDate\" AS \"res5\" , \"t0\" . \"HireDate\" AS \"res6\" , \"t0\" . \"Address\" AS \"res7\" , \"t0\" . \"City\" AS \"res8\" , \"t0\" . \"State\" AS \"res9\" , \"t0\" . \"Country\" AS \"res10\" , \"t0\" . \"PostalCode\" AS \"res11\" , \"t0\" . \"Phone\" AS \"res12\" , \"t0\" . \"Fax\" AS \"res13\" , \"t0\" . \"Email\" AS \"res14\" FROM \"Employee\" AS \"t0\" ORDER BY \"t0\" . \"State\" ASC , \"t0\" . \"City\" DESC NULLS LAST LIMIT 10 SQL2003 T612: Advanced OLAP operations This provides both the PERCENT_RANK() and CUME_DIST() functions as percentRank_ and cumeDist_ respectively.","title":"Advanced features"},{"location":"user-guide/queries/advanced-features/#sql2003-t611-elementary-olap-operations","text":"This optional SQL2003 feature allows attaching arbitrary FILTER (WHERE ..) clauses to aggregates. During querying only rows matching the given expression are included in computing the aggregate. This can often be simulated in other databases by an appropriate CASE expression, but beam will not do this translation. Haskell Postgres aggregate_ ( \\ i -> ( group_ ( invoiceCustomer i ), as_ @ Int32 $ countAll_ ` filterWhere_ ` ( invoiceTotal i >. 500 ), as_ @ Int32 $ countAll_ ` filterWhere_ ` ( invoiceTotal i <. 100 ))) $ all_ ( invoice chinookDb ) SELECT \"t0\" . \"CustomerId\" AS \"res0\" , COUNT ( * ) FILTER ( WHERE ( \"t0\" . \"Total\" ) > ( '500.0' )) AS \"res1\" , COUNT ( * ) FILTER ( WHERE ( \"t0\" . \"Total\" ) < ( '100.0' )) AS \"res2\" FROM \"Invoice\" AS \"t0\" GROUP BY \"t0\" . \"CustomerId\" These combine as you'd expect with window functions. For example, to return each invoice along with the average total of all invoices by the same customer where the invoice was billed to an address in Los Angeles, Haskell Postgres withWindow_ ( \\ i -> frame_ ( partitionBy_ ( invoiceCustomer i )) noOrder_ noBounds_ ) ( \\ i w -> ( i , avg_ ( invoiceTotal i ) ` filterWhere_ ` ( addressCity ( invoiceBillingAddress i ) ==. just_ \"Los Angeles\" ) ` over_ ` w )) ( all_ ( invoice chinookDb )) SELECT \"t0\" . \"InvoiceId\" AS \"res0\" , \"t0\" . \"CustomerId\" AS \"res1\" , \"t0\" . \"InvoiceDate\" AS \"res2\" , \"t0\" . \"BillingAddress\" AS \"res3\" , \"t0\" . \"BillingCity\" AS \"res4\" , \"t0\" . \"BillingState\" AS \"res5\" , \"t0\" . \"BillingCountry\" AS \"res6\" , \"t0\" . \"BillingPostalCode\" AS \"res7\" , \"t0\" . \"Total\" AS \"res8\" , AVG ( \"t0\" . \"Total\" ) FILTER ( WHERE ( \"t0\" . \"BillingCity\" ) IS NOT DISTINCT FROM ( 'Los Angeles' )) OVER ( PARTITION BY \"t0\" . \"CustomerId\" ) AS \"res9\" FROM \"Invoice\" AS \"t0\" Danger\" FILTER (WHERE ..) must be applied directly to a SQL aggregate function, but this isn't enforced at compile time. This may be fixed in a later version of beam. This extension also provides various window functions for SQL. The only one beam currently implements is RANK() via the rank_ function. Contributions are appreciated!","title":"SQL2003 T611: Elementary OLAP operations"},{"location":"user-guide/queries/advanced-features/#null-ordering","text":"This optional SQL2003 feature allows nulls to appear before or after non-null values in the sort ordering. Haskell Postgres limit_ 10 $ orderBy_ ( \\ e -> ( asc_ ( addressState ( employeeAddress e )), nullsLast_ ( desc_ ( addressCity ( employeeAddress e ))))) $ all_ ( employee chinookDb ) SELECT \"t0\" . \"EmployeeId\" AS \"res0\" , \"t0\" . \"LastName\" AS \"res1\" , \"t0\" . \"FirstName\" AS \"res2\" , \"t0\" . \"Title\" AS \"res3\" , \"t0\" . \"ReportsTo\" AS \"res4\" , \"t0\" . \"BirthDate\" AS \"res5\" , \"t0\" . \"HireDate\" AS \"res6\" , \"t0\" . \"Address\" AS \"res7\" , \"t0\" . \"City\" AS \"res8\" , \"t0\" . \"State\" AS \"res9\" , \"t0\" . \"Country\" AS \"res10\" , \"t0\" . \"PostalCode\" AS \"res11\" , \"t0\" . \"Phone\" AS \"res12\" , \"t0\" . \"Fax\" AS \"res13\" , \"t0\" . \"Email\" AS \"res14\" FROM \"Employee\" AS \"t0\" ORDER BY \"t0\" . \"State\" ASC , \"t0\" . \"City\" DESC NULLS LAST LIMIT 10","title":"Null Ordering"},{"location":"user-guide/queries/advanced-features/#sql2003-t612-advanced-olap-operations","text":"This provides both the PERCENT_RANK() and CUME_DIST() functions as percentRank_ and cumeDist_ respectively.","title":"SQL2003 T612: Advanced OLAP operations"},{"location":"user-guide/queries/aggregates/","text":"You can use the aggregate_ function to group your result set and compute aggregates within the group. You can think of aggregate_ as a souped up version of Haskell's groupBy . You use aggregate_ by specifying an underlying query to run and a function that produces an aggregation projection. An aggregation projection is either a value of type QAgg syntax s a , a value of type QGroupExpr syntax s a , or a tuple of such values. Any QGenExpr that uses an aggregate function is automatically assigned the QAgg syntax s a type. Any QGenExpr that contains the group_ combinator is given the type QGroupExpr . During query generation, the expressions of type QGroupExpr are added to the GROUP BY clause, and expressions of type QAgg are treated as aggregation to be computed. The result of the aggregate_ lifts all the QAgg s and QGroupExpr s to 'regular' value-level QExpr s, so the result of aggregate_ can be used in expressions as usual. Simple aggregate usage Suppose we wanted to count the number of genres in our database. Haskell Postgres Sqlite aggregate_ ( \\ _ -> as_ @ Int32 countAll_ ) ( all_ ( genre chinookDb )) SELECT COUNT ( * ) AS \"res0\" FROM \"Genre\" AS \"t0\" SELECT COUNT ( * ) AS \"res0\" FROM \"Genre\" AS \"t0\" ; -- With values: [] Adding a GROUP BY clause Above, SQL used the default grouping, which puts all rows in one group. We can also specify columns and expressions to group by. For example, if we wanted to count the number of tracks for each genre, we can use the group_ function to group by the genre. Haskell Postgres Sqlite aggregate_ ( \\ ( genre , track ) -> ( group_ genre , as_ @ Int32 $ count_ ( trackId track ))) $ do g <- all_ ( genre chinookDb ) t <- genreTracks g pure ( g , t ) SELECT \"t0\" . \"GenreId\" AS \"res0\" , \"t0\" . \"Name\" AS \"res1\" , COUNT ( \"t1\" . \"TrackId\" ) AS \"res2\" FROM \"Genre\" AS \"t0\" LEFT JOIN \"Track\" AS \"t1\" ON ( \"t1\" . \"GenreId\" ) IS NOT DISTINCT FROM ( \"t0\" . \"GenreId\" ) GROUP BY \"t0\" . \"GenreId\" , \"t0\" . \"Name\" SELECT \"t0\" . \"GenreId\" AS \"res0\" , \"t0\" . \"Name\" AS \"res1\" , COUNT ( \"t1\" . \"TrackId\" ) AS \"res2\" FROM \"Genre\" AS \"t0\" LEFT JOIN \"Track\" AS \"t1\" ON CASE WHEN (( \"t1\" . \"GenreId\" ) IS NULL ) AND (( \"t0\" . \"GenreId\" ) IS NULL ) THEN ? WHEN (( \"t1\" . \"GenreId\" ) IS NULL ) OR (( \"t0\" . \"GenreId\" ) IS NULL ) THEN ? ELSE ( \"t1\" . \"GenreId\" ) = ( \"t0\" . \"GenreId\" ) END GROUP BY \"t0\" . \"GenreId\" , \"t0\" . \"Name\" ; -- With values: [SQLInteger 1,SQLInteger 0] Tip count_ can return any Integral type. Adding the explicit as_ @Int32 above prevents an ambiguous type error. SQL compatibility Above, we demonstrated the use of count_ and countAll_ which map to the appropriate SQL aggregates. Beam supports all of the other standard SQL92 aggregates. In general, SQL aggregates are named similarly in beam and SQL. As usual, the aggregate function in beam is suffixed by an underscore. For example, sum_ corresponds to the SQL aggregate SUM . SQL also allows you to specify set quantifiers for each aggregate. Beam supports these as well. By convention, versions of aggregates that take in an optional set quantifier are suffixed by Over . For example SUM(DISTINCT x) can be written sumOver_ distinctInGroup_ x . The universally quantified version of each aggregate is obtained by using the allInGroup_ quantifier. Thus, sum_ == sumOver_ allInGroup_ . Because ALL is the default set quantifier, beam does not typically generate it in queries. If, for some reason, you would like beam to be explicit about it, you can use the allInGroupExplicitly_ quantifier. Haskell Postgres Sqlite aggregate_ ( \\ ( genre , track ) -> ( group_ genre , as_ @ Int32 $ countOver_ distinctInGroup_ ( trackUnitPrice track ) , fromMaybe_ 0 ( sumOver_ allInGroupExplicitly_ ( fromMaybe_ 0 ( trackMilliseconds track ))) ` div_ ` 1000 )) $ do g <- all_ ( genre chinookDb ) t <- genreTracks g pure ( g , t ) SELECT \"t0\" . \"GenreId\" AS \"res0\" , \"t0\" . \"Name\" AS \"res1\" , COUNT ( DISTINCT \"t1\" . \"UnitPrice\" ) AS \"res2\" , ( COALESCE ( SUM ( ALL COALESCE ( \"t1\" . \"Milliseconds\" , 0 )), 0 )) / ( 1000 ) AS \"res3\" FROM \"Genre\" AS \"t0\" LEFT JOIN \"Track\" AS \"t1\" ON ( \"t1\" . \"GenreId\" ) IS NOT DISTINCT FROM ( \"t0\" . \"GenreId\" ) GROUP BY \"t0\" . \"GenreId\" , \"t0\" . \"Name\" SELECT \"t0\" . \"GenreId\" AS \"res0\" , \"t0\" . \"Name\" AS \"res1\" , COUNT ( DISTINCT \"t1\" . \"UnitPrice\" ) AS \"res2\" , ( COALESCE ( SUM ( ALL COALESCE ( \"t1\" . \"Milliseconds\" , ? )), ? )) / ( ? ) AS \"res3\" FROM \"Genre\" AS \"t0\" LEFT JOIN \"Track\" AS \"t1\" ON CASE WHEN (( \"t1\" . \"GenreId\" ) IS NULL ) AND (( \"t0\" . \"GenreId\" ) IS NULL ) THEN ? WHEN (( \"t1\" . \"GenreId\" ) IS NULL ) OR (( \"t0\" . \"GenreId\" ) IS NULL ) THEN ? ELSE ( \"t1\" . \"GenreId\" ) = ( \"t0\" . \"GenreId\" ) END GROUP BY \"t0\" . \"GenreId\" , \"t0\" . \"Name\" ; -- With values: [SQLInteger 0,SQLInteger 0,SQLInteger 1000,SQLInteger 1,SQLInteger 0] Tip Most Beam aggregates ( count_ and countAll_ being an exception) return a Maybe value, because aggregating over no rows in SQL returns a NULL value. Use fromMaybe_ or coalesce_ to supply a default value in this case. The beam-core library supports the standard SQL aggregation functions. Individual backends are likely to support the full range of aggregates available on that backend (if not, please send a bug report). SQL Aggregate Relevant standard Unquantified beam function Quantified beam function SUM SQL92 sum_ sumOver_ MIN SQL92 min_ minOver_ MAX SQL92 max_ maxOver_ AVG SQL92 avg_ avgOver_ COUNT(x) SQL92 count_ countOver_ COUNT(*) SQL92 countAll_ N/A EVERY(x) SQL99 every_ everyOver_ ANY(x)/SOME(x) SQL99 any_ , some_ anyOver_ , someOver_ The HAVING clause SQL allows users to specify a HAVING condition to filter results based on the computed result of an aggregate. Beam fully supports HAVING clauses, but does not use any special syntax. Simply use filter_ or guard_ as usual, and beam will add a HAVING clause if it forms legal SQL. Otherwise, beam will create a subselect and add a WHERE clause. Either way, this is transparent to the user. Haskell Postgres Sqlite -- Only return results for genres whose total track length is over 5 minutes filter_ ( \\ ( genre , distinctPriceCount , totalTrackLength ) -> totalTrackLength >=. 300000 ) $ aggregate_ ( \\ ( genre , track ) -> ( group_ genre , as_ @ Int32 $ countOver_ distinctInGroup_ ( trackUnitPrice track ) , fromMaybe_ 0 ( sumOver_ allInGroupExplicitly_ ( trackMilliseconds track )) ` div_ ` 1000 )) $ ((,) <$> all_ ( genre chinookDb ) <*> all_ ( track chinookDb )) SELECT \"t0\" . \"GenreId\" AS \"res0\" , \"t0\" . \"Name\" AS \"res1\" , COUNT ( DISTINCT \"t1\" . \"UnitPrice\" ) AS \"res2\" , ( COALESCE ( SUM ( ALL \"t1\" . \"Milliseconds\" ), 0 )) / ( 1000 ) AS \"res3\" FROM \"Genre\" AS \"t0\" CROSS JOIN \"Track\" AS \"t1\" GROUP BY \"t0\" . \"GenreId\" , \"t0\" . \"Name\" HAVING (( COALESCE ( SUM ( ALL \"t1\" . \"Milliseconds\" ), 0 )) / ( 1000 )) >= ( 300000 ) SELECT \"t0\" . \"GenreId\" AS \"res0\" , \"t0\" . \"Name\" AS \"res1\" , COUNT ( DISTINCT \"t1\" . \"UnitPrice\" ) AS \"res2\" , ( COALESCE ( SUM ( ALL \"t1\" . \"Milliseconds\" ), ? )) / ( ? ) AS \"res3\" FROM \"Genre\" AS \"t0\" INNER JOIN \"Track\" AS \"t1\" GROUP BY \"t0\" . \"GenreId\" , \"t0\" . \"Name\" HAVING (( COALESCE ( SUM ( ALL \"t1\" . \"Milliseconds\" ), ? )) / ( ? )) >= ( ? ); -- With values: [SQLInteger 0,SQLInteger 1000,SQLInteger 0,SQLInteger 1000,SQLInteger 300000] Beam will also handle the filter_ correctly in the presence of more complicated queries. For example, we can now join our aggregate on genres back over tracks. Haskell Postgres Sqlite -- Only return results for genres whose total track length is over 5 minutes filter_ ( \\ ( genre , track , distinctPriceCount , totalTrackLength ) -> totalTrackLength >=. 300000 ) $ do ( genre , priceCnt , trackLength ) <- aggregate_ ( \\ ( genre , track ) -> ( group_ genre , as_ @ Int32 $ countOver_ distinctInGroup_ ( trackUnitPrice track ) , fromMaybe_ 0 ( sumOver_ allInGroupExplicitly_ ( trackMilliseconds track )) ` div_ ` 1000 )) $ ((,) <$> all_ ( genre chinookDb ) <*> all_ ( track chinookDb )) track <- genreTracks genre pure ( genre , track , priceCnt , trackLength ) SELECT \"t0\" . \"res0\" AS \"res0\" , \"t0\" . \"res1\" AS \"res1\" , \"t1\" . \"TrackId\" AS \"res2\" , \"t1\" . \"Name\" AS \"res3\" , \"t1\" . \"AlbumId\" AS \"res4\" , \"t1\" . \"MediaTypeId\" AS \"res5\" , \"t1\" . \"GenreId\" AS \"res6\" , \"t1\" . \"Composer\" AS \"res7\" , \"t1\" . \"Milliseconds\" AS \"res8\" , \"t1\" . \"Bytes\" AS \"res9\" , \"t1\" . \"UnitPrice\" AS \"res10\" , \"t0\" . \"res2\" AS \"res11\" , \"t0\" . \"res3\" AS \"res12\" FROM ( SELECT \"t0\" . \"GenreId\" AS \"res0\" , \"t0\" . \"Name\" AS \"res1\" , COUNT ( DISTINCT \"t1\" . \"UnitPrice\" ) AS \"res2\" , ( COALESCE ( SUM ( ALL \"t1\" . \"Milliseconds\" ), 0 )) / ( 1000 ) AS \"res3\" FROM \"Genre\" AS \"t0\" CROSS JOIN \"Track\" AS \"t1\" GROUP BY \"t0\" . \"GenreId\" , \"t0\" . \"Name\" ) AS \"t0\" LEFT JOIN \"Track\" AS \"t1\" ON ( \"t1\" . \"GenreId\" ) IS NOT DISTINCT FROM ( \"t0\" . \"res0\" ) WHERE ( \"t0\" . \"res3\" ) >= ( 300000 ) SELECT \"t0\" . \"res0\" AS \"res0\" , \"t0\" . \"res1\" AS \"res1\" , \"t1\" . \"TrackId\" AS \"res2\" , \"t1\" . \"Name\" AS \"res3\" , \"t1\" . \"AlbumId\" AS \"res4\" , \"t1\" . \"MediaTypeId\" AS \"res5\" , \"t1\" . \"GenreId\" AS \"res6\" , \"t1\" . \"Composer\" AS \"res7\" , \"t1\" . \"Milliseconds\" AS \"res8\" , \"t1\" . \"Bytes\" AS \"res9\" , \"t1\" . \"UnitPrice\" AS \"res10\" , \"t0\" . \"res2\" AS \"res11\" , \"t0\" . \"res3\" AS \"res12\" FROM ( SELECT \"t0\" . \"GenreId\" AS \"res0\" , \"t0\" . \"Name\" AS \"res1\" , COUNT ( DISTINCT \"t1\" . \"UnitPrice\" ) AS \"res2\" , ( COALESCE ( SUM ( ALL \"t1\" . \"Milliseconds\" ), ? )) / ( ? ) AS \"res3\" FROM \"Genre\" AS \"t0\" INNER JOIN \"Track\" AS \"t1\" GROUP BY \"t0\" . \"GenreId\" , \"t0\" . \"Name\" ) AS \"t0\" LEFT JOIN \"Track\" AS \"t1\" ON CASE WHEN (( \"t1\" . \"GenreId\" ) IS NULL ) AND (( \"t0\" . \"res0\" ) IS NULL ) THEN ? WHEN (( \"t1\" . \"GenreId\" ) IS NULL ) OR (( \"t0\" . \"res0\" ) IS NULL ) THEN ? ELSE ( \"t1\" . \"GenreId\" ) = ( \"t0\" . \"res0\" ) END WHERE ( \"t0\" . \"res3\" ) >= ( ? ); -- With values: [SQLInteger 0,SQLInteger 1000,SQLInteger 1,SQLInteger 0,SQLInteger 300000] The position of filter_ changes the code generated. Above, the filter_ produced a WHERE clause on the outermost SELECT . If instead, we put the filter_ clause right outside the aggregate_ , beam will produce a HAVING clause instead. Haskell Postgres Sqlite -- Only return results for genres whose total track length is over 5 minutes do ( genre , priceCnt , trackLength ) <- filter_ ( \\ ( genre , distinctPriceCount , totalTrackLength ) -> totalTrackLength >=. 300000 ) $ aggregate_ ( \\ ( genre , track ) -> ( group_ genre , as_ @ Int32 $ countOver_ distinctInGroup_ ( trackUnitPrice track ) , fromMaybe_ 0 ( sumOver_ allInGroupExplicitly_ ( trackMilliseconds track )) ` div_ ` 1000 )) $ ((,) <$> all_ ( genre chinookDb ) <*> all_ ( track chinookDb )) track <- genreTracks genre pure ( genre , track , priceCnt , trackLength ) SELECT \"t0\" . \"res0\" AS \"res0\" , \"t0\" . \"res1\" AS \"res1\" , \"t1\" . \"TrackId\" AS \"res2\" , \"t1\" . \"Name\" AS \"res3\" , \"t1\" . \"AlbumId\" AS \"res4\" , \"t1\" . \"MediaTypeId\" AS \"res5\" , \"t1\" . \"GenreId\" AS \"res6\" , \"t1\" . \"Composer\" AS \"res7\" , \"t1\" . \"Milliseconds\" AS \"res8\" , \"t1\" . \"Bytes\" AS \"res9\" , \"t1\" . \"UnitPrice\" AS \"res10\" , \"t0\" . \"res2\" AS \"res11\" , \"t0\" . \"res3\" AS \"res12\" FROM ( SELECT \"t0\" . \"GenreId\" AS \"res0\" , \"t0\" . \"Name\" AS \"res1\" , COUNT ( DISTINCT \"t1\" . \"UnitPrice\" ) AS \"res2\" , ( COALESCE ( SUM ( ALL \"t1\" . \"Milliseconds\" ), 0 )) / ( 1000 ) AS \"res3\" FROM \"Genre\" AS \"t0\" CROSS JOIN \"Track\" AS \"t1\" GROUP BY \"t0\" . \"GenreId\" , \"t0\" . \"Name\" HAVING (( COALESCE ( SUM ( ALL \"t1\" . \"Milliseconds\" ), 0 )) / ( 1000 )) >= ( 300000 )) AS \"t0\" LEFT JOIN \"Track\" AS \"t1\" ON ( \"t1\" . \"GenreId\" ) IS NOT DISTINCT FROM ( \"t0\" . \"res0\" ) SELECT \"t0\" . \"res0\" AS \"res0\" , \"t0\" . \"res1\" AS \"res1\" , \"t1\" . \"TrackId\" AS \"res2\" , \"t1\" . \"Name\" AS \"res3\" , \"t1\" . \"AlbumId\" AS \"res4\" , \"t1\" . \"MediaTypeId\" AS \"res5\" , \"t1\" . \"GenreId\" AS \"res6\" , \"t1\" . \"Composer\" AS \"res7\" , \"t1\" . \"Milliseconds\" AS \"res8\" , \"t1\" . \"Bytes\" AS \"res9\" , \"t1\" . \"UnitPrice\" AS \"res10\" , \"t0\" . \"res2\" AS \"res11\" , \"t0\" . \"res3\" AS \"res12\" FROM ( SELECT \"t0\" . \"GenreId\" AS \"res0\" , \"t0\" . \"Name\" AS \"res1\" , COUNT ( DISTINCT \"t1\" . \"UnitPrice\" ) AS \"res2\" , ( COALESCE ( SUM ( ALL \"t1\" . \"Milliseconds\" ), ? )) / ( ? ) AS \"res3\" FROM \"Genre\" AS \"t0\" INNER JOIN \"Track\" AS \"t1\" GROUP BY \"t0\" . \"GenreId\" , \"t0\" . \"Name\" HAVING (( COALESCE ( SUM ( ALL \"t1\" . \"Milliseconds\" ), ? )) / ( ? )) >= ( ? )) AS \"t0\" LEFT JOIN \"Track\" AS \"t1\" ON CASE WHEN (( \"t1\" . \"GenreId\" ) IS NULL ) AND (( \"t0\" . \"res0\" ) IS NULL ) THEN ? WHEN (( \"t1\" . \"GenreId\" ) IS NULL ) OR (( \"t0\" . \"res0\" ) IS NULL ) THEN ? ELSE ( \"t1\" . \"GenreId\" ) = ( \"t0\" . \"res0\" ) END ; -- With values: [SQLInteger 0,SQLInteger 1000,SQLInteger 0,SQLInteger 1000,SQLInteger 300000,SQLInteger 1,SQLInteger 0] Due to the monadic structure, putting the filtered aggregate as the second clause in the JOIN causes the HAVING to be floated out, because the compiler can't prove that the conditional expression only depends on the results of the aggregate. Haskell Postgres Sqlite -- Only return results for genres whose total track length is over 5 minutes do track_ <- all_ ( track chinookDb ) ( genre , priceCnt , trackLength ) <- filter_ ( \\ ( genre , distinctPriceCount , totalTrackLength ) -> totalTrackLength >=. 300000 ) $ aggregate_ ( \\ ( genre , track ) -> ( group_ genre , as_ @ Int32 $ countOver_ distinctInGroup_ ( trackUnitPrice track ) , fromMaybe_ 0 ( sumOver_ allInGroupExplicitly_ ( trackMilliseconds track )) ` div_ ` 1000 )) $ ((,) <$> all_ ( genre chinookDb ) <*> all_ ( track chinookDb )) guard_ ( trackGenreId track_ ==. just_ ( pk genre )) pure ( genre , track_ , priceCnt , trackLength ) SELECT \"t1\" . \"res0\" AS \"res0\" , \"t1\" . \"res1\" AS \"res1\" , \"t0\" . \"TrackId\" AS \"res2\" , \"t0\" . \"Name\" AS \"res3\" , \"t0\" . \"AlbumId\" AS \"res4\" , \"t0\" . \"MediaTypeId\" AS \"res5\" , \"t0\" . \"GenreId\" AS \"res6\" , \"t0\" . \"Composer\" AS \"res7\" , \"t0\" . \"Milliseconds\" AS \"res8\" , \"t0\" . \"Bytes\" AS \"res9\" , \"t0\" . \"UnitPrice\" AS \"res10\" , \"t1\" . \"res2\" AS \"res11\" , \"t1\" . \"res3\" AS \"res12\" FROM \"Track\" AS \"t0\" CROSS JOIN ( SELECT \"t0\" . \"GenreId\" AS \"res0\" , \"t0\" . \"Name\" AS \"res1\" , COUNT ( DISTINCT \"t1\" . \"UnitPrice\" ) AS \"res2\" , ( COALESCE ( SUM ( ALL \"t1\" . \"Milliseconds\" ), 0 )) / ( 1000 ) AS \"res3\" FROM \"Genre\" AS \"t0\" CROSS JOIN \"Track\" AS \"t1\" GROUP BY \"t0\" . \"GenreId\" , \"t0\" . \"Name\" ) AS \"t1\" WHERE (( \"t1\" . \"res3\" ) >= ( 300000 )) AND (( \"t0\" . \"GenreId\" ) IS NOT DISTINCT FROM ( \"t1\" . \"res0\" )) SELECT \"t1\" . \"res0\" AS \"res0\" , \"t1\" . \"res1\" AS \"res1\" , \"t0\" . \"TrackId\" AS \"res2\" , \"t0\" . \"Name\" AS \"res3\" , \"t0\" . \"AlbumId\" AS \"res4\" , \"t0\" . \"MediaTypeId\" AS \"res5\" , \"t0\" . \"GenreId\" AS \"res6\" , \"t0\" . \"Composer\" AS \"res7\" , \"t0\" . \"Milliseconds\" AS \"res8\" , \"t0\" . \"Bytes\" AS \"res9\" , \"t0\" . \"UnitPrice\" AS \"res10\" , \"t1\" . \"res2\" AS \"res11\" , \"t1\" . \"res3\" AS \"res12\" FROM \"Track\" AS \"t0\" INNER JOIN ( SELECT \"t0\" . \"GenreId\" AS \"res0\" , \"t0\" . \"Name\" AS \"res1\" , COUNT ( DISTINCT \"t1\" . \"UnitPrice\" ) AS \"res2\" , ( COALESCE ( SUM ( ALL \"t1\" . \"Milliseconds\" ), ? )) / ( ? ) AS \"res3\" FROM \"Genre\" AS \"t0\" INNER JOIN \"Track\" AS \"t1\" GROUP BY \"t0\" . \"GenreId\" , \"t0\" . \"Name\" ) AS \"t1\" WHERE (( \"t1\" . \"res3\" ) >= ( ? )) AND ( CASE WHEN (( \"t0\" . \"GenreId\" ) IS NULL ) AND (( \"t1\" . \"res0\" ) IS NULL ) THEN ? WHEN (( \"t0\" . \"GenreId\" ) IS NULL ) OR (( \"t1\" . \"res0\" ) IS NULL ) THEN ? ELSE ( \"t0\" . \"GenreId\" ) = ( \"t1\" . \"res0\" ) END ); -- With values: [SQLInteger 0,SQLInteger 1000,SQLInteger 300000,SQLInteger 1,SQLInteger 0] You can prove to the compiler that the filter_ should generate a having by using the subselect_ combinator. Haskell Postgres Sqlite -- Only return results for genres whose total track length is over 5 minutes do track_ <- all_ ( track chinookDb ) ( genre , priceCnt , trackLength ) <- subselect_ $ filter_ ( \\ ( genre , distinctPriceCount , totalTrackLength ) -> totalTrackLength >=. 300000 ) $ aggregate_ ( \\ ( genre , track ) -> ( group_ genre , as_ @ Int32 $ countOver_ distinctInGroup_ ( trackUnitPrice track ) , fromMaybe_ 0 ( sumOver_ allInGroupExplicitly_ ( trackMilliseconds track )) ` div_ ` 1000 )) $ ((,) <$> all_ ( genre chinookDb ) <*> all_ ( track chinookDb )) guard_ ( trackGenreId track_ ==. just_ ( pk genre )) pure ( genre , track_ , priceCnt , trackLength ) SELECT \"t1\" . \"res0\" AS \"res0\" , \"t1\" . \"res1\" AS \"res1\" , \"t0\" . \"TrackId\" AS \"res2\" , \"t0\" . \"Name\" AS \"res3\" , \"t0\" . \"AlbumId\" AS \"res4\" , \"t0\" . \"MediaTypeId\" AS \"res5\" , \"t0\" . \"GenreId\" AS \"res6\" , \"t0\" . \"Composer\" AS \"res7\" , \"t0\" . \"Milliseconds\" AS \"res8\" , \"t0\" . \"Bytes\" AS \"res9\" , \"t0\" . \"UnitPrice\" AS \"res10\" , \"t1\" . \"res2\" AS \"res11\" , \"t1\" . \"res3\" AS \"res12\" FROM \"Track\" AS \"t0\" CROSS JOIN ( SELECT \"t0\" . \"res0\" AS \"res0\" , \"t0\" . \"res1\" AS \"res1\" , \"t0\" . \"res2\" AS \"res2\" , \"t0\" . \"res3\" AS \"res3\" FROM ( SELECT \"t0\" . \"GenreId\" AS \"res0\" , \"t0\" . \"Name\" AS \"res1\" , COUNT ( DISTINCT \"t1\" . \"UnitPrice\" ) AS \"res2\" , ( COALESCE ( SUM ( ALL \"t1\" . \"Milliseconds\" ), 0 )) / ( 1000 ) AS \"res3\" FROM \"Genre\" AS \"t0\" CROSS JOIN \"Track\" AS \"t1\" GROUP BY \"t0\" . \"GenreId\" , \"t0\" . \"Name\" HAVING (( COALESCE ( SUM ( ALL \"t1\" . \"Milliseconds\" ), 0 )) / ( 1000 )) >= ( 300000 )) AS \"t0\" ) AS \"t1\" WHERE ( \"t0\" . \"GenreId\" ) IS NOT DISTINCT FROM ( \"t1\" . \"res0\" ) SELECT \"t1\" . \"res0\" AS \"res0\" , \"t1\" . \"res1\" AS \"res1\" , \"t0\" . \"TrackId\" AS \"res2\" , \"t0\" . \"Name\" AS \"res3\" , \"t0\" . \"AlbumId\" AS \"res4\" , \"t0\" . \"MediaTypeId\" AS \"res5\" , \"t0\" . \"GenreId\" AS \"res6\" , \"t0\" . \"Composer\" AS \"res7\" , \"t0\" . \"Milliseconds\" AS \"res8\" , \"t0\" . \"Bytes\" AS \"res9\" , \"t0\" . \"UnitPrice\" AS \"res10\" , \"t1\" . \"res2\" AS \"res11\" , \"t1\" . \"res3\" AS \"res12\" FROM \"Track\" AS \"t0\" INNER JOIN ( SELECT \"t0\" . \"res0\" AS \"res0\" , \"t0\" . \"res1\" AS \"res1\" , \"t0\" . \"res2\" AS \"res2\" , \"t0\" . \"res3\" AS \"res3\" FROM ( SELECT \"t0\" . \"GenreId\" AS \"res0\" , \"t0\" . \"Name\" AS \"res1\" , COUNT ( DISTINCT \"t1\" . \"UnitPrice\" ) AS \"res2\" , ( COALESCE ( SUM ( ALL \"t1\" . \"Milliseconds\" ), ? )) / ( ? ) AS \"res3\" FROM \"Genre\" AS \"t0\" INNER JOIN \"Track\" AS \"t1\" GROUP BY \"t0\" . \"GenreId\" , \"t0\" . \"Name\" HAVING (( COALESCE ( SUM ( ALL \"t1\" . \"Milliseconds\" ), ? )) / ( ? )) >= ( ? )) AS \"t0\" ) AS \"t1\" WHERE CASE WHEN (( \"t0\" . \"GenreId\" ) IS NULL ) AND (( \"t1\" . \"res0\" ) IS NULL ) THEN ? WHEN (( \"t0\" . \"GenreId\" ) IS NULL ) OR (( \"t1\" . \"res0\" ) IS NULL ) THEN ? ELSE ( \"t0\" . \"GenreId\" ) = ( \"t1\" . \"res0\" ) END ; -- With values: [SQLInteger 0,SQLInteger 1000,SQLInteger 0,SQLInteger 1000,SQLInteger 300000,SQLInteger 1,SQLInteger 0]","title":"Aggregates"},{"location":"user-guide/queries/aggregates/#simple-aggregate-usage","text":"Suppose we wanted to count the number of genres in our database. Haskell Postgres Sqlite aggregate_ ( \\ _ -> as_ @ Int32 countAll_ ) ( all_ ( genre chinookDb )) SELECT COUNT ( * ) AS \"res0\" FROM \"Genre\" AS \"t0\" SELECT COUNT ( * ) AS \"res0\" FROM \"Genre\" AS \"t0\" ; -- With values: []","title":"Simple aggregate usage"},{"location":"user-guide/queries/aggregates/#adding-a-group-by-clause","text":"Above, SQL used the default grouping, which puts all rows in one group. We can also specify columns and expressions to group by. For example, if we wanted to count the number of tracks for each genre, we can use the group_ function to group by the genre. Haskell Postgres Sqlite aggregate_ ( \\ ( genre , track ) -> ( group_ genre , as_ @ Int32 $ count_ ( trackId track ))) $ do g <- all_ ( genre chinookDb ) t <- genreTracks g pure ( g , t ) SELECT \"t0\" . \"GenreId\" AS \"res0\" , \"t0\" . \"Name\" AS \"res1\" , COUNT ( \"t1\" . \"TrackId\" ) AS \"res2\" FROM \"Genre\" AS \"t0\" LEFT JOIN \"Track\" AS \"t1\" ON ( \"t1\" . \"GenreId\" ) IS NOT DISTINCT FROM ( \"t0\" . \"GenreId\" ) GROUP BY \"t0\" . \"GenreId\" , \"t0\" . \"Name\" SELECT \"t0\" . \"GenreId\" AS \"res0\" , \"t0\" . \"Name\" AS \"res1\" , COUNT ( \"t1\" . \"TrackId\" ) AS \"res2\" FROM \"Genre\" AS \"t0\" LEFT JOIN \"Track\" AS \"t1\" ON CASE WHEN (( \"t1\" . \"GenreId\" ) IS NULL ) AND (( \"t0\" . \"GenreId\" ) IS NULL ) THEN ? WHEN (( \"t1\" . \"GenreId\" ) IS NULL ) OR (( \"t0\" . \"GenreId\" ) IS NULL ) THEN ? ELSE ( \"t1\" . \"GenreId\" ) = ( \"t0\" . \"GenreId\" ) END GROUP BY \"t0\" . \"GenreId\" , \"t0\" . \"Name\" ; -- With values: [SQLInteger 1,SQLInteger 0] Tip count_ can return any Integral type. Adding the explicit as_ @Int32 above prevents an ambiguous type error.","title":"Adding a GROUP BY clause"},{"location":"user-guide/queries/aggregates/#sql-compatibility","text":"Above, we demonstrated the use of count_ and countAll_ which map to the appropriate SQL aggregates. Beam supports all of the other standard SQL92 aggregates. In general, SQL aggregates are named similarly in beam and SQL. As usual, the aggregate function in beam is suffixed by an underscore. For example, sum_ corresponds to the SQL aggregate SUM . SQL also allows you to specify set quantifiers for each aggregate. Beam supports these as well. By convention, versions of aggregates that take in an optional set quantifier are suffixed by Over . For example SUM(DISTINCT x) can be written sumOver_ distinctInGroup_ x . The universally quantified version of each aggregate is obtained by using the allInGroup_ quantifier. Thus, sum_ == sumOver_ allInGroup_ . Because ALL is the default set quantifier, beam does not typically generate it in queries. If, for some reason, you would like beam to be explicit about it, you can use the allInGroupExplicitly_ quantifier. Haskell Postgres Sqlite aggregate_ ( \\ ( genre , track ) -> ( group_ genre , as_ @ Int32 $ countOver_ distinctInGroup_ ( trackUnitPrice track ) , fromMaybe_ 0 ( sumOver_ allInGroupExplicitly_ ( fromMaybe_ 0 ( trackMilliseconds track ))) ` div_ ` 1000 )) $ do g <- all_ ( genre chinookDb ) t <- genreTracks g pure ( g , t ) SELECT \"t0\" . \"GenreId\" AS \"res0\" , \"t0\" . \"Name\" AS \"res1\" , COUNT ( DISTINCT \"t1\" . \"UnitPrice\" ) AS \"res2\" , ( COALESCE ( SUM ( ALL COALESCE ( \"t1\" . \"Milliseconds\" , 0 )), 0 )) / ( 1000 ) AS \"res3\" FROM \"Genre\" AS \"t0\" LEFT JOIN \"Track\" AS \"t1\" ON ( \"t1\" . \"GenreId\" ) IS NOT DISTINCT FROM ( \"t0\" . \"GenreId\" ) GROUP BY \"t0\" . \"GenreId\" , \"t0\" . \"Name\" SELECT \"t0\" . \"GenreId\" AS \"res0\" , \"t0\" . \"Name\" AS \"res1\" , COUNT ( DISTINCT \"t1\" . \"UnitPrice\" ) AS \"res2\" , ( COALESCE ( SUM ( ALL COALESCE ( \"t1\" . \"Milliseconds\" , ? )), ? )) / ( ? ) AS \"res3\" FROM \"Genre\" AS \"t0\" LEFT JOIN \"Track\" AS \"t1\" ON CASE WHEN (( \"t1\" . \"GenreId\" ) IS NULL ) AND (( \"t0\" . \"GenreId\" ) IS NULL ) THEN ? WHEN (( \"t1\" . \"GenreId\" ) IS NULL ) OR (( \"t0\" . \"GenreId\" ) IS NULL ) THEN ? ELSE ( \"t1\" . \"GenreId\" ) = ( \"t0\" . \"GenreId\" ) END GROUP BY \"t0\" . \"GenreId\" , \"t0\" . \"Name\" ; -- With values: [SQLInteger 0,SQLInteger 0,SQLInteger 1000,SQLInteger 1,SQLInteger 0] Tip Most Beam aggregates ( count_ and countAll_ being an exception) return a Maybe value, because aggregating over no rows in SQL returns a NULL value. Use fromMaybe_ or coalesce_ to supply a default value in this case. The beam-core library supports the standard SQL aggregation functions. Individual backends are likely to support the full range of aggregates available on that backend (if not, please send a bug report). SQL Aggregate Relevant standard Unquantified beam function Quantified beam function SUM SQL92 sum_ sumOver_ MIN SQL92 min_ minOver_ MAX SQL92 max_ maxOver_ AVG SQL92 avg_ avgOver_ COUNT(x) SQL92 count_ countOver_ COUNT(*) SQL92 countAll_ N/A EVERY(x) SQL99 every_ everyOver_ ANY(x)/SOME(x) SQL99 any_ , some_ anyOver_ , someOver_","title":"SQL compatibility"},{"location":"user-guide/queries/aggregates/#the-having-clause","text":"SQL allows users to specify a HAVING condition to filter results based on the computed result of an aggregate. Beam fully supports HAVING clauses, but does not use any special syntax. Simply use filter_ or guard_ as usual, and beam will add a HAVING clause if it forms legal SQL. Otherwise, beam will create a subselect and add a WHERE clause. Either way, this is transparent to the user. Haskell Postgres Sqlite -- Only return results for genres whose total track length is over 5 minutes filter_ ( \\ ( genre , distinctPriceCount , totalTrackLength ) -> totalTrackLength >=. 300000 ) $ aggregate_ ( \\ ( genre , track ) -> ( group_ genre , as_ @ Int32 $ countOver_ distinctInGroup_ ( trackUnitPrice track ) , fromMaybe_ 0 ( sumOver_ allInGroupExplicitly_ ( trackMilliseconds track )) ` div_ ` 1000 )) $ ((,) <$> all_ ( genre chinookDb ) <*> all_ ( track chinookDb )) SELECT \"t0\" . \"GenreId\" AS \"res0\" , \"t0\" . \"Name\" AS \"res1\" , COUNT ( DISTINCT \"t1\" . \"UnitPrice\" ) AS \"res2\" , ( COALESCE ( SUM ( ALL \"t1\" . \"Milliseconds\" ), 0 )) / ( 1000 ) AS \"res3\" FROM \"Genre\" AS \"t0\" CROSS JOIN \"Track\" AS \"t1\" GROUP BY \"t0\" . \"GenreId\" , \"t0\" . \"Name\" HAVING (( COALESCE ( SUM ( ALL \"t1\" . \"Milliseconds\" ), 0 )) / ( 1000 )) >= ( 300000 ) SELECT \"t0\" . \"GenreId\" AS \"res0\" , \"t0\" . \"Name\" AS \"res1\" , COUNT ( DISTINCT \"t1\" . \"UnitPrice\" ) AS \"res2\" , ( COALESCE ( SUM ( ALL \"t1\" . \"Milliseconds\" ), ? )) / ( ? ) AS \"res3\" FROM \"Genre\" AS \"t0\" INNER JOIN \"Track\" AS \"t1\" GROUP BY \"t0\" . \"GenreId\" , \"t0\" . \"Name\" HAVING (( COALESCE ( SUM ( ALL \"t1\" . \"Milliseconds\" ), ? )) / ( ? )) >= ( ? ); -- With values: [SQLInteger 0,SQLInteger 1000,SQLInteger 0,SQLInteger 1000,SQLInteger 300000] Beam will also handle the filter_ correctly in the presence of more complicated queries. For example, we can now join our aggregate on genres back over tracks. Haskell Postgres Sqlite -- Only return results for genres whose total track length is over 5 minutes filter_ ( \\ ( genre , track , distinctPriceCount , totalTrackLength ) -> totalTrackLength >=. 300000 ) $ do ( genre , priceCnt , trackLength ) <- aggregate_ ( \\ ( genre , track ) -> ( group_ genre , as_ @ Int32 $ countOver_ distinctInGroup_ ( trackUnitPrice track ) , fromMaybe_ 0 ( sumOver_ allInGroupExplicitly_ ( trackMilliseconds track )) ` div_ ` 1000 )) $ ((,) <$> all_ ( genre chinookDb ) <*> all_ ( track chinookDb )) track <- genreTracks genre pure ( genre , track , priceCnt , trackLength ) SELECT \"t0\" . \"res0\" AS \"res0\" , \"t0\" . \"res1\" AS \"res1\" , \"t1\" . \"TrackId\" AS \"res2\" , \"t1\" . \"Name\" AS \"res3\" , \"t1\" . \"AlbumId\" AS \"res4\" , \"t1\" . \"MediaTypeId\" AS \"res5\" , \"t1\" . \"GenreId\" AS \"res6\" , \"t1\" . \"Composer\" AS \"res7\" , \"t1\" . \"Milliseconds\" AS \"res8\" , \"t1\" . \"Bytes\" AS \"res9\" , \"t1\" . \"UnitPrice\" AS \"res10\" , \"t0\" . \"res2\" AS \"res11\" , \"t0\" . \"res3\" AS \"res12\" FROM ( SELECT \"t0\" . \"GenreId\" AS \"res0\" , \"t0\" . \"Name\" AS \"res1\" , COUNT ( DISTINCT \"t1\" . \"UnitPrice\" ) AS \"res2\" , ( COALESCE ( SUM ( ALL \"t1\" . \"Milliseconds\" ), 0 )) / ( 1000 ) AS \"res3\" FROM \"Genre\" AS \"t0\" CROSS JOIN \"Track\" AS \"t1\" GROUP BY \"t0\" . \"GenreId\" , \"t0\" . \"Name\" ) AS \"t0\" LEFT JOIN \"Track\" AS \"t1\" ON ( \"t1\" . \"GenreId\" ) IS NOT DISTINCT FROM ( \"t0\" . \"res0\" ) WHERE ( \"t0\" . \"res3\" ) >= ( 300000 ) SELECT \"t0\" . \"res0\" AS \"res0\" , \"t0\" . \"res1\" AS \"res1\" , \"t1\" . \"TrackId\" AS \"res2\" , \"t1\" . \"Name\" AS \"res3\" , \"t1\" . \"AlbumId\" AS \"res4\" , \"t1\" . \"MediaTypeId\" AS \"res5\" , \"t1\" . \"GenreId\" AS \"res6\" , \"t1\" . \"Composer\" AS \"res7\" , \"t1\" . \"Milliseconds\" AS \"res8\" , \"t1\" . \"Bytes\" AS \"res9\" , \"t1\" . \"UnitPrice\" AS \"res10\" , \"t0\" . \"res2\" AS \"res11\" , \"t0\" . \"res3\" AS \"res12\" FROM ( SELECT \"t0\" . \"GenreId\" AS \"res0\" , \"t0\" . \"Name\" AS \"res1\" , COUNT ( DISTINCT \"t1\" . \"UnitPrice\" ) AS \"res2\" , ( COALESCE ( SUM ( ALL \"t1\" . \"Milliseconds\" ), ? )) / ( ? ) AS \"res3\" FROM \"Genre\" AS \"t0\" INNER JOIN \"Track\" AS \"t1\" GROUP BY \"t0\" . \"GenreId\" , \"t0\" . \"Name\" ) AS \"t0\" LEFT JOIN \"Track\" AS \"t1\" ON CASE WHEN (( \"t1\" . \"GenreId\" ) IS NULL ) AND (( \"t0\" . \"res0\" ) IS NULL ) THEN ? WHEN (( \"t1\" . \"GenreId\" ) IS NULL ) OR (( \"t0\" . \"res0\" ) IS NULL ) THEN ? ELSE ( \"t1\" . \"GenreId\" ) = ( \"t0\" . \"res0\" ) END WHERE ( \"t0\" . \"res3\" ) >= ( ? ); -- With values: [SQLInteger 0,SQLInteger 1000,SQLInteger 1,SQLInteger 0,SQLInteger 300000] The position of filter_ changes the code generated. Above, the filter_ produced a WHERE clause on the outermost SELECT . If instead, we put the filter_ clause right outside the aggregate_ , beam will produce a HAVING clause instead. Haskell Postgres Sqlite -- Only return results for genres whose total track length is over 5 minutes do ( genre , priceCnt , trackLength ) <- filter_ ( \\ ( genre , distinctPriceCount , totalTrackLength ) -> totalTrackLength >=. 300000 ) $ aggregate_ ( \\ ( genre , track ) -> ( group_ genre , as_ @ Int32 $ countOver_ distinctInGroup_ ( trackUnitPrice track ) , fromMaybe_ 0 ( sumOver_ allInGroupExplicitly_ ( trackMilliseconds track )) ` div_ ` 1000 )) $ ((,) <$> all_ ( genre chinookDb ) <*> all_ ( track chinookDb )) track <- genreTracks genre pure ( genre , track , priceCnt , trackLength ) SELECT \"t0\" . \"res0\" AS \"res0\" , \"t0\" . \"res1\" AS \"res1\" , \"t1\" . \"TrackId\" AS \"res2\" , \"t1\" . \"Name\" AS \"res3\" , \"t1\" . \"AlbumId\" AS \"res4\" , \"t1\" . \"MediaTypeId\" AS \"res5\" , \"t1\" . \"GenreId\" AS \"res6\" , \"t1\" . \"Composer\" AS \"res7\" , \"t1\" . \"Milliseconds\" AS \"res8\" , \"t1\" . \"Bytes\" AS \"res9\" , \"t1\" . \"UnitPrice\" AS \"res10\" , \"t0\" . \"res2\" AS \"res11\" , \"t0\" . \"res3\" AS \"res12\" FROM ( SELECT \"t0\" . \"GenreId\" AS \"res0\" , \"t0\" . \"Name\" AS \"res1\" , COUNT ( DISTINCT \"t1\" . \"UnitPrice\" ) AS \"res2\" , ( COALESCE ( SUM ( ALL \"t1\" . \"Milliseconds\" ), 0 )) / ( 1000 ) AS \"res3\" FROM \"Genre\" AS \"t0\" CROSS JOIN \"Track\" AS \"t1\" GROUP BY \"t0\" . \"GenreId\" , \"t0\" . \"Name\" HAVING (( COALESCE ( SUM ( ALL \"t1\" . \"Milliseconds\" ), 0 )) / ( 1000 )) >= ( 300000 )) AS \"t0\" LEFT JOIN \"Track\" AS \"t1\" ON ( \"t1\" . \"GenreId\" ) IS NOT DISTINCT FROM ( \"t0\" . \"res0\" ) SELECT \"t0\" . \"res0\" AS \"res0\" , \"t0\" . \"res1\" AS \"res1\" , \"t1\" . \"TrackId\" AS \"res2\" , \"t1\" . \"Name\" AS \"res3\" , \"t1\" . \"AlbumId\" AS \"res4\" , \"t1\" . \"MediaTypeId\" AS \"res5\" , \"t1\" . \"GenreId\" AS \"res6\" , \"t1\" . \"Composer\" AS \"res7\" , \"t1\" . \"Milliseconds\" AS \"res8\" , \"t1\" . \"Bytes\" AS \"res9\" , \"t1\" . \"UnitPrice\" AS \"res10\" , \"t0\" . \"res2\" AS \"res11\" , \"t0\" . \"res3\" AS \"res12\" FROM ( SELECT \"t0\" . \"GenreId\" AS \"res0\" , \"t0\" . \"Name\" AS \"res1\" , COUNT ( DISTINCT \"t1\" . \"UnitPrice\" ) AS \"res2\" , ( COALESCE ( SUM ( ALL \"t1\" . \"Milliseconds\" ), ? )) / ( ? ) AS \"res3\" FROM \"Genre\" AS \"t0\" INNER JOIN \"Track\" AS \"t1\" GROUP BY \"t0\" . \"GenreId\" , \"t0\" . \"Name\" HAVING (( COALESCE ( SUM ( ALL \"t1\" . \"Milliseconds\" ), ? )) / ( ? )) >= ( ? )) AS \"t0\" LEFT JOIN \"Track\" AS \"t1\" ON CASE WHEN (( \"t1\" . \"GenreId\" ) IS NULL ) AND (( \"t0\" . \"res0\" ) IS NULL ) THEN ? WHEN (( \"t1\" . \"GenreId\" ) IS NULL ) OR (( \"t0\" . \"res0\" ) IS NULL ) THEN ? ELSE ( \"t1\" . \"GenreId\" ) = ( \"t0\" . \"res0\" ) END ; -- With values: [SQLInteger 0,SQLInteger 1000,SQLInteger 0,SQLInteger 1000,SQLInteger 300000,SQLInteger 1,SQLInteger 0] Due to the monadic structure, putting the filtered aggregate as the second clause in the JOIN causes the HAVING to be floated out, because the compiler can't prove that the conditional expression only depends on the results of the aggregate. Haskell Postgres Sqlite -- Only return results for genres whose total track length is over 5 minutes do track_ <- all_ ( track chinookDb ) ( genre , priceCnt , trackLength ) <- filter_ ( \\ ( genre , distinctPriceCount , totalTrackLength ) -> totalTrackLength >=. 300000 ) $ aggregate_ ( \\ ( genre , track ) -> ( group_ genre , as_ @ Int32 $ countOver_ distinctInGroup_ ( trackUnitPrice track ) , fromMaybe_ 0 ( sumOver_ allInGroupExplicitly_ ( trackMilliseconds track )) ` div_ ` 1000 )) $ ((,) <$> all_ ( genre chinookDb ) <*> all_ ( track chinookDb )) guard_ ( trackGenreId track_ ==. just_ ( pk genre )) pure ( genre , track_ , priceCnt , trackLength ) SELECT \"t1\" . \"res0\" AS \"res0\" , \"t1\" . \"res1\" AS \"res1\" , \"t0\" . \"TrackId\" AS \"res2\" , \"t0\" . \"Name\" AS \"res3\" , \"t0\" . \"AlbumId\" AS \"res4\" , \"t0\" . \"MediaTypeId\" AS \"res5\" , \"t0\" . \"GenreId\" AS \"res6\" , \"t0\" . \"Composer\" AS \"res7\" , \"t0\" . \"Milliseconds\" AS \"res8\" , \"t0\" . \"Bytes\" AS \"res9\" , \"t0\" . \"UnitPrice\" AS \"res10\" , \"t1\" . \"res2\" AS \"res11\" , \"t1\" . \"res3\" AS \"res12\" FROM \"Track\" AS \"t0\" CROSS JOIN ( SELECT \"t0\" . \"GenreId\" AS \"res0\" , \"t0\" . \"Name\" AS \"res1\" , COUNT ( DISTINCT \"t1\" . \"UnitPrice\" ) AS \"res2\" , ( COALESCE ( SUM ( ALL \"t1\" . \"Milliseconds\" ), 0 )) / ( 1000 ) AS \"res3\" FROM \"Genre\" AS \"t0\" CROSS JOIN \"Track\" AS \"t1\" GROUP BY \"t0\" . \"GenreId\" , \"t0\" . \"Name\" ) AS \"t1\" WHERE (( \"t1\" . \"res3\" ) >= ( 300000 )) AND (( \"t0\" . \"GenreId\" ) IS NOT DISTINCT FROM ( \"t1\" . \"res0\" )) SELECT \"t1\" . \"res0\" AS \"res0\" , \"t1\" . \"res1\" AS \"res1\" , \"t0\" . \"TrackId\" AS \"res2\" , \"t0\" . \"Name\" AS \"res3\" , \"t0\" . \"AlbumId\" AS \"res4\" , \"t0\" . \"MediaTypeId\" AS \"res5\" , \"t0\" . \"GenreId\" AS \"res6\" , \"t0\" . \"Composer\" AS \"res7\" , \"t0\" . \"Milliseconds\" AS \"res8\" , \"t0\" . \"Bytes\" AS \"res9\" , \"t0\" . \"UnitPrice\" AS \"res10\" , \"t1\" . \"res2\" AS \"res11\" , \"t1\" . \"res3\" AS \"res12\" FROM \"Track\" AS \"t0\" INNER JOIN ( SELECT \"t0\" . \"GenreId\" AS \"res0\" , \"t0\" . \"Name\" AS \"res1\" , COUNT ( DISTINCT \"t1\" . \"UnitPrice\" ) AS \"res2\" , ( COALESCE ( SUM ( ALL \"t1\" . \"Milliseconds\" ), ? )) / ( ? ) AS \"res3\" FROM \"Genre\" AS \"t0\" INNER JOIN \"Track\" AS \"t1\" GROUP BY \"t0\" . \"GenreId\" , \"t0\" . \"Name\" ) AS \"t1\" WHERE (( \"t1\" . \"res3\" ) >= ( ? )) AND ( CASE WHEN (( \"t0\" . \"GenreId\" ) IS NULL ) AND (( \"t1\" . \"res0\" ) IS NULL ) THEN ? WHEN (( \"t0\" . \"GenreId\" ) IS NULL ) OR (( \"t1\" . \"res0\" ) IS NULL ) THEN ? ELSE ( \"t0\" . \"GenreId\" ) = ( \"t1\" . \"res0\" ) END ); -- With values: [SQLInteger 0,SQLInteger 1000,SQLInteger 300000,SQLInteger 1,SQLInteger 0] You can prove to the compiler that the filter_ should generate a having by using the subselect_ combinator. Haskell Postgres Sqlite -- Only return results for genres whose total track length is over 5 minutes do track_ <- all_ ( track chinookDb ) ( genre , priceCnt , trackLength ) <- subselect_ $ filter_ ( \\ ( genre , distinctPriceCount , totalTrackLength ) -> totalTrackLength >=. 300000 ) $ aggregate_ ( \\ ( genre , track ) -> ( group_ genre , as_ @ Int32 $ countOver_ distinctInGroup_ ( trackUnitPrice track ) , fromMaybe_ 0 ( sumOver_ allInGroupExplicitly_ ( trackMilliseconds track )) ` div_ ` 1000 )) $ ((,) <$> all_ ( genre chinookDb ) <*> all_ ( track chinookDb )) guard_ ( trackGenreId track_ ==. just_ ( pk genre )) pure ( genre , track_ , priceCnt , trackLength ) SELECT \"t1\" . \"res0\" AS \"res0\" , \"t1\" . \"res1\" AS \"res1\" , \"t0\" . \"TrackId\" AS \"res2\" , \"t0\" . \"Name\" AS \"res3\" , \"t0\" . \"AlbumId\" AS \"res4\" , \"t0\" . \"MediaTypeId\" AS \"res5\" , \"t0\" . \"GenreId\" AS \"res6\" , \"t0\" . \"Composer\" AS \"res7\" , \"t0\" . \"Milliseconds\" AS \"res8\" , \"t0\" . \"Bytes\" AS \"res9\" , \"t0\" . \"UnitPrice\" AS \"res10\" , \"t1\" . \"res2\" AS \"res11\" , \"t1\" . \"res3\" AS \"res12\" FROM \"Track\" AS \"t0\" CROSS JOIN ( SELECT \"t0\" . \"res0\" AS \"res0\" , \"t0\" . \"res1\" AS \"res1\" , \"t0\" . \"res2\" AS \"res2\" , \"t0\" . \"res3\" AS \"res3\" FROM ( SELECT \"t0\" . \"GenreId\" AS \"res0\" , \"t0\" . \"Name\" AS \"res1\" , COUNT ( DISTINCT \"t1\" . \"UnitPrice\" ) AS \"res2\" , ( COALESCE ( SUM ( ALL \"t1\" . \"Milliseconds\" ), 0 )) / ( 1000 ) AS \"res3\" FROM \"Genre\" AS \"t0\" CROSS JOIN \"Track\" AS \"t1\" GROUP BY \"t0\" . \"GenreId\" , \"t0\" . \"Name\" HAVING (( COALESCE ( SUM ( ALL \"t1\" . \"Milliseconds\" ), 0 )) / ( 1000 )) >= ( 300000 )) AS \"t0\" ) AS \"t1\" WHERE ( \"t0\" . \"GenreId\" ) IS NOT DISTINCT FROM ( \"t1\" . \"res0\" ) SELECT \"t1\" . \"res0\" AS \"res0\" , \"t1\" . \"res1\" AS \"res1\" , \"t0\" . \"TrackId\" AS \"res2\" , \"t0\" . \"Name\" AS \"res3\" , \"t0\" . \"AlbumId\" AS \"res4\" , \"t0\" . \"MediaTypeId\" AS \"res5\" , \"t0\" . \"GenreId\" AS \"res6\" , \"t0\" . \"Composer\" AS \"res7\" , \"t0\" . \"Milliseconds\" AS \"res8\" , \"t0\" . \"Bytes\" AS \"res9\" , \"t0\" . \"UnitPrice\" AS \"res10\" , \"t1\" . \"res2\" AS \"res11\" , \"t1\" . \"res3\" AS \"res12\" FROM \"Track\" AS \"t0\" INNER JOIN ( SELECT \"t0\" . \"res0\" AS \"res0\" , \"t0\" . \"res1\" AS \"res1\" , \"t0\" . \"res2\" AS \"res2\" , \"t0\" . \"res3\" AS \"res3\" FROM ( SELECT \"t0\" . \"GenreId\" AS \"res0\" , \"t0\" . \"Name\" AS \"res1\" , COUNT ( DISTINCT \"t1\" . \"UnitPrice\" ) AS \"res2\" , ( COALESCE ( SUM ( ALL \"t1\" . \"Milliseconds\" ), ? )) / ( ? ) AS \"res3\" FROM \"Genre\" AS \"t0\" INNER JOIN \"Track\" AS \"t1\" GROUP BY \"t0\" . \"GenreId\" , \"t0\" . \"Name\" HAVING (( COALESCE ( SUM ( ALL \"t1\" . \"Milliseconds\" ), ? )) / ( ? )) >= ( ? )) AS \"t0\" ) AS \"t1\" WHERE CASE WHEN (( \"t0\" . \"GenreId\" ) IS NULL ) AND (( \"t1\" . \"res0\" ) IS NULL ) THEN ? WHEN (( \"t0\" . \"GenreId\" ) IS NULL ) OR (( \"t1\" . \"res0\" ) IS NULL ) THEN ? ELSE ( \"t0\" . \"GenreId\" ) = ( \"t1\" . \"res0\" ) END ; -- With values: [SQLInteger 0,SQLInteger 1000,SQLInteger 0,SQLInteger 1000,SQLInteger 300000,SQLInteger 1,SQLInteger 0]","title":"The HAVING clause"},{"location":"user-guide/queries/basic/","text":"Given our database definition and database descriptor, we can query database entities and retrieve data. Before we discuss writing queries, we will take a look at some of the important query types. Data types The Q data type Beam queries are built using the Q data type. Q 's signature is as follows data Q be db s a In this definition be is the particular Beam backend this Q monad is written for. Each beam backend defines a custom tag type. For example, beam-sqlite provides the Sqlite tag, and beam-postgres provides the Postgres tag. You can see what SQL backends are available in GHCi by asking for info on the BeamSqlBackend class. Prelude Database.Beam Database.Beam.Sqlite Data.Text Database.SQLite.Simple Lens.Micro Data.Time Database.Beam.Backend.SQL T> :info BeamSqlBackend ass (Database.Beam.Backend.Types.BeamBackend be, IsSql92Syntax (BeamSqlBackendSyntax be), Sql92SanityCheck (BeamSqlBackendSyntax be), HasSqlValueSyntax (BeamSqlBackendValueSyntax be) Bool, HasSqlValueSyntax (BeamSqlBackendValueSyntax be) SqlNull, Eq (BeamSqlBackendExpressionSyntax be)) => BeamSqlBackend be -- Defined at /Users/travis/Projects/beam/beam-core/Database/Beam/Backend/SQL.hs:212:1 stance BeamSqlBackend Sqlite -- Defined at /Users/travis/Projects/beam/beam-sqlite/Database/Beam/Sqlite/Connection.hs:157:10 stance (IsSql92Syntax syntax, Sql92SanityCheck syntax, HasSqlValueSyntax (Sql92ValueSyntax syntax) Bool, HasSqlValueSyntax (Sql92ValueSyntax syntax) SqlNull, Eq (Sql92ExpressionSyntax syntax)) => BeamSqlBackend (MockSqlBackend syntax) -- Defined at /Users/travis/Projects/beam/beam-core/Database/Beam/Backend/SQL.hs:238:10 db is the type of the database (as we defined above). This is used to ensure you only query database entities that are in scope in this database. s is the scope parameter. For the most part, you'll write your queries so that they work over all s . Beam manipulates this parameter internally to ensure that the fields in your expressions are always in scope at run-time. a is the type of the result of the query. Q is a monad, which means you can use your favorite Monad , Applicative , and Functor functions. The Functor instance can be used to create projections as explained in the next section . The Monad and Applicative instances can be used to create JOINs . The QGenExpr type While Q represents the result of whole queries (entire SELECT s for example), QGenExpr represents the type of SQL expressions. QGenExpr also takes some type parameters: data QGenExpr context be s a context is the particular way in which this expression is being used. For example, expressions containing aggregates have context ~ QAggregateContext . Expressions returning scalar values have context ~ QValueContext . be is the backend for which this expression is written. For example, expressions destined for execution in PostgreSQL, will substitute this for Postgres (from Database.Beam.Postgres in the beam-postgres package). You can also leave this polymorphic if you want your expression to be useable across multiple backends. Note In previous versions of beam be indicated the 'syntax' rather than the backend. This was confusing because the syntax and backend types were not obviously related. Beam >=0.8.0.0 uses the backend type consistently to indicate where an expression is being used. s is a scoping parameter, which will match the s in Q . a is the type of this expression. For example, expressions returning SQL int values, will have Haskell type Int32 . This ensures that your SQL query won't fail at run-time with a type error. Beam defines some specializations of QGenExpr for common uses. type QExpr = QGenExpr QValueContext type QAgg = QGenExpr QAggregateContext type QOrd = QGenExpr QOrderingContext type QWindowExpr = QGenExpr QWindowingContext type QWindowFrame = QGenExpr QWindowFrameContext type QGroupExpr = QGenExpr QGroupingContext Thus, value expressions can be given the simpler type of QExpr be s a . Expressions containing aggregates are typed as QAgg be s a . A note on type inference These types may seem incredibly complicated. Indeed, the safety that beam tries to provide requires these scary-looking types. But alas, do not fear! Beam is also designed to assist type inference. For the most part, you will rarely need to annotate these types in your code. Occassionally you will need to provide a type for the result of an expression. For example, SELECT ing just the literal 1 may cause an ambiguity, because the compiler won't know which Integral type to use. Beam provides an easy utility function as_ for this. With -XTypeApplications enabled, as_ @ Int32 ( ambiguous expression ) ensures that ambiguous expression has the type QGenExpr ctxt be s Int32 with the ctxt , be , and s types appropriately inferred. Simple queries The easiest query is simply getting all the rows in a specific table. If you have a database object (something with type DatabaseSettings be db ) with some table or view entities, you can use the all_ function to retrieve all rows in a specific table or view. For example, to retrieve all PersonT entries in the exampleDb we defined in the last section, we can say all_ ( persons exampleDb ) :: Q be ExampleDb s ( PersonT ( QExpr s )) Note We give the full type of the query here for illustrative purposes only. There is no need to do so in your own code Two things to note. Firstly, here PersonT is parameterized over the QExpr s higher-kinded type. This means that each field in PersonT now contains a SQL expression instead of a Haskell value. This is the magic that our parameterized types allow. Thus, personFirstName ( all_ ( persons exampleDb )) :: QExpr be s Text and personFirstName ( Person \"John\" \"Smith\" 23 \"john.smith@example.com\" \"8888888888\" :: Person ) :: Text Secondly, the field type has the same scope variable as the entire query. This means, it can only be used in the scope of this query. You will never be able to inspect the type of s from outside Q . Once we have a query in terms of Q , we can use the select function from Database.Beam.Query to turn it into a select statement that can be run against the backend. select takes an expression of type Q , and converts it into a SQL statement, ready to be executed against the database. The output of the query passed to select must follow some conventions, so that beam knows how to serialize, deserialize, and project the appropriate values from the query. In particular, the return type of your query must be either a plain expression (i.e., type QExpr ), a Beamable type (i.e., a table or primary key, defined as above), or any combination of tuples of the above (Beam supports up to 8-tuples by default). Higher-order tuples can be formed by nested tuples. For example, for 16 return values, you can return a 2-tuple of 8-tuples or an 8-tuple of 2-tuples or a 4-tuple of 4-tuples, etc. With this in mind, we can use select to get a query statement against our database. The return type of all_ is just the table we ask for. In this case, we're interested in the persons table. The persons table has the Beamable type PersonT . As expected, the SqlSelect will return us concrete Person values (recall that Person is equivalent to PersonT Identity ). select ( all_ ( persons exampleDb )) :: HasQBuilder be => SqlSelect be Person Normally, you'd ship this select statement off to a backend to run, but for the purposes of this tutorial, we can also ask beam to dump what the standard SQL expression this query encodes. dumpSqlSelect ( all_ ( persons exampleDb )) SELECT ` t0 ` . ` email ` AS \"res0\" , ` t0 ` . ` first_name ` AS \"res1\" , ` t0 ` . ` last_name ` AS \"res2\" , ` t0 ` . ` password ` AS \"res3\" FROM \"cart_users\" AS \"t0\" Internally, dumpSqlSelect uses a beam-core provided syntax to generate standard ANSI SQL expressions. Note that these expressions should not be shipped to a backend directly, as they may not be escaped properly. Still, it is useful to see what would run. Tip all_ only works for TableEntity s. Use allFromView_ for ViewEntity s. A note on composability All beam queries are composable . This means that you can freely mix values of type Q in whichever way typechecks and expect a reasonable SQL query. This differs from the behavior of SQL, where the syntax for composing queries depends on the structure of that query. For example, suppose you wanted to fetch all rows of a table, filter them by a condition, limit the amount of rows returned and then join these rows with another table. In SQL, you'd have to write explicit subselects, take care of handling projections, etc. This is because this query doesn't fit into the 'standard' SQL query structure. However, in beam, you can simply write this query. Beam will take care of generating explicit subselects and handling projections. Scoping rules enforced by the Haskell type system ensure that the query is constructed correctly. For example, we can write the following (meaningless) query, and things will work as expected. Haskell Postgres Sqlite do tbl1 <- limit_ 10 $ filter_ ( \\ customer -> (( customerFirstName customer ` like_ ` \"Jo%\" ) &&. ( customerLastName customer ` like_ ` \"S%\" )) &&. ( addressState ( customerAddress customer ) ==. just_ \"CA\" ||. addressState ( customerAddress customer ) ==. just_ \"WA\" )) $ all_ ( customer chinookDb ) tbl2 <- all_ ( track chinookDb ) pure ( tbl1 , tbl2 ) SELECT \"t0\" . \"res0\" AS \"res0\" , \"t0\" . \"res1\" AS \"res1\" , \"t0\" . \"res2\" AS \"res2\" , \"t0\" . \"res3\" AS \"res3\" , \"t0\" . \"res4\" AS \"res4\" , \"t0\" . \"res5\" AS \"res5\" , \"t0\" . \"res6\" AS \"res6\" , \"t0\" . \"res7\" AS \"res7\" , \"t0\" . \"res8\" AS \"res8\" , \"t0\" . \"res9\" AS \"res9\" , \"t0\" . \"res10\" AS \"res10\" , \"t0\" . \"res11\" AS \"res11\" , \"t0\" . \"res12\" AS \"res12\" , \"t1\" . \"TrackId\" AS \"res13\" , \"t1\" . \"Name\" AS \"res14\" , \"t1\" . \"AlbumId\" AS \"res15\" , \"t1\" . \"MediaTypeId\" AS \"res16\" , \"t1\" . \"GenreId\" AS \"res17\" , \"t1\" . \"Composer\" AS \"res18\" , \"t1\" . \"Milliseconds\" AS \"res19\" , \"t1\" . \"Bytes\" AS \"res20\" , \"t1\" . \"UnitPrice\" AS \"res21\" FROM ( SELECT \"t0\" . \"CustomerId\" AS \"res0\" , \"t0\" . \"FirstName\" AS \"res1\" , \"t0\" . \"LastName\" AS \"res2\" , \"t0\" . \"Company\" AS \"res3\" , \"t0\" . \"Address\" AS \"res4\" , \"t0\" . \"City\" AS \"res5\" , \"t0\" . \"State\" AS \"res6\" , \"t0\" . \"Country\" AS \"res7\" , \"t0\" . \"PostalCode\" AS \"res8\" , \"t0\" . \"Phone\" AS \"res9\" , \"t0\" . \"Fax\" AS \"res10\" , \"t0\" . \"Email\" AS \"res11\" , \"t0\" . \"SupportRepId\" AS \"res12\" FROM \"Customer\" AS \"t0\" WHERE ((( \"t0\" . \"FirstName\" ) LIKE ( 'Jo%' )) AND (( \"t0\" . \"LastName\" ) LIKE ( 'S%' ))) AND ((( \"t0\" . \"State\" ) IS NOT DISTINCT FROM ( 'CA' )) OR (( \"t0\" . \"State\" ) IS NOT DISTINCT FROM ( 'WA' ))) LIMIT 10 ) AS \"t0\" CROSS JOIN \"Track\" AS \"t1\" SELECT \"t0\" . \"res0\" AS \"res0\" , \"t0\" . \"res1\" AS \"res1\" , \"t0\" . \"res2\" AS \"res2\" , \"t0\" . \"res3\" AS \"res3\" , \"t0\" . \"res4\" AS \"res4\" , \"t0\" . \"res5\" AS \"res5\" , \"t0\" . \"res6\" AS \"res6\" , \"t0\" . \"res7\" AS \"res7\" , \"t0\" . \"res8\" AS \"res8\" , \"t0\" . \"res9\" AS \"res9\" , \"t0\" . \"res10\" AS \"res10\" , \"t0\" . \"res11\" AS \"res11\" , \"t0\" . \"res12\" AS \"res12\" , \"t1\" . \"TrackId\" AS \"res13\" , \"t1\" . \"Name\" AS \"res14\" , \"t1\" . \"AlbumId\" AS \"res15\" , \"t1\" . \"MediaTypeId\" AS \"res16\" , \"t1\" . \"GenreId\" AS \"res17\" , \"t1\" . \"Composer\" AS \"res18\" , \"t1\" . \"Milliseconds\" AS \"res19\" , \"t1\" . \"Bytes\" AS \"res20\" , \"t1\" . \"UnitPrice\" AS \"res21\" FROM ( SELECT \"t0\" . \"CustomerId\" AS \"res0\" , \"t0\" . \"FirstName\" AS \"res1\" , \"t0\" . \"LastName\" AS \"res2\" , \"t0\" . \"Company\" AS \"res3\" , \"t0\" . \"Address\" AS \"res4\" , \"t0\" . \"City\" AS \"res5\" , \"t0\" . \"State\" AS \"res6\" , \"t0\" . \"Country\" AS \"res7\" , \"t0\" . \"PostalCode\" AS \"res8\" , \"t0\" . \"Phone\" AS \"res9\" , \"t0\" . \"Fax\" AS \"res10\" , \"t0\" . \"Email\" AS \"res11\" , \"t0\" . \"SupportRepId\" AS \"res12\" FROM \"Customer\" AS \"t0\" WHERE ((( \"t0\" . \"FirstName\" ) LIKE ( ? )) AND (( \"t0\" . \"LastName\" ) LIKE ( ? ))) AND (( CASE WHEN (( \"t0\" . \"State\" ) IS NULL ) AND (( ? ) IS NULL ) THEN ? WHEN (( \"t0\" . \"State\" ) IS NULL ) OR (( ? ) IS NULL ) THEN ? ELSE ( \"t0\" . \"State\" ) = ( ? ) END ) OR ( CASE WHEN (( \"t0\" . \"State\" ) IS NULL ) AND (( ? ) IS NULL ) THEN ? WHEN (( \"t0\" . \"State\" ) IS NULL ) OR (( ? ) IS NULL ) THEN ? ELSE ( \"t0\" . \"State\" ) = ( ? ) END )) LIMIT 10 ) AS \"t0\" INNER JOIN \"Track\" AS \"t1\" ; -- With values: [SQLText \"Jo%\",SQLText \"S%\",SQLText \"CA\",SQLInteger 1,SQLText \"CA\",SQLInteger 0,SQLText \"CA\",SQLText \"WA\",SQLInteger 1,SQLText \"WA\",SQLInteger 0,SQLText \"WA\"] This allows you to easily factor out queries. This means you can build a query library in your application and then freely mix and match these queries as necessary. This allows you to offload as much processing to the database as possible, rather than shipping data to your application pre-processing. Haskell Postgres Sqlite -- 'complicatedQuery' could be declared and imported from an external module here. The generated query is the same regardless let complicatedQuery = filter_ ( \\ customer -> (( customerFirstName customer ` like_ ` \"Jo%\" ) &&. ( customerLastName customer ` like_ ` \"S%\" )) &&. ( addressState ( customerAddress customer ) ==. just_ \"CA\" ||. addressState ( customerAddress customer ) ==. just_ \"WA\" )) $ all_ ( customer chinookDb ) in do tbl1 <- limit_ 10 $ complicatedQuery tbl2 <- all_ ( track chinookDb ) pure ( tbl1 , tbl2 ) SELECT \"t0\" . \"res0\" AS \"res0\" , \"t0\" . \"res1\" AS \"res1\" , \"t0\" . \"res2\" AS \"res2\" , \"t0\" . \"res3\" AS \"res3\" , \"t0\" . \"res4\" AS \"res4\" , \"t0\" . \"res5\" AS \"res5\" , \"t0\" . \"res6\" AS \"res6\" , \"t0\" . \"res7\" AS \"res7\" , \"t0\" . \"res8\" AS \"res8\" , \"t0\" . \"res9\" AS \"res9\" , \"t0\" . \"res10\" AS \"res10\" , \"t0\" . \"res11\" AS \"res11\" , \"t0\" . \"res12\" AS \"res12\" , \"t1\" . \"TrackId\" AS \"res13\" , \"t1\" . \"Name\" AS \"res14\" , \"t1\" . \"AlbumId\" AS \"res15\" , \"t1\" . \"MediaTypeId\" AS \"res16\" , \"t1\" . \"GenreId\" AS \"res17\" , \"t1\" . \"Composer\" AS \"res18\" , \"t1\" . \"Milliseconds\" AS \"res19\" , \"t1\" . \"Bytes\" AS \"res20\" , \"t1\" . \"UnitPrice\" AS \"res21\" FROM ( SELECT \"t0\" . \"CustomerId\" AS \"res0\" , \"t0\" . \"FirstName\" AS \"res1\" , \"t0\" . \"LastName\" AS \"res2\" , \"t0\" . \"Company\" AS \"res3\" , \"t0\" . \"Address\" AS \"res4\" , \"t0\" . \"City\" AS \"res5\" , \"t0\" . \"State\" AS \"res6\" , \"t0\" . \"Country\" AS \"res7\" , \"t0\" . \"PostalCode\" AS \"res8\" , \"t0\" . \"Phone\" AS \"res9\" , \"t0\" . \"Fax\" AS \"res10\" , \"t0\" . \"Email\" AS \"res11\" , \"t0\" . \"SupportRepId\" AS \"res12\" FROM \"Customer\" AS \"t0\" WHERE ((( \"t0\" . \"FirstName\" ) LIKE ( 'Jo%' )) AND (( \"t0\" . \"LastName\" ) LIKE ( 'S%' ))) AND ((( \"t0\" . \"State\" ) IS NOT DISTINCT FROM ( 'CA' )) OR (( \"t0\" . \"State\" ) IS NOT DISTINCT FROM ( 'WA' ))) LIMIT 10 ) AS \"t0\" CROSS JOIN \"Track\" AS \"t1\" SELECT \"t0\" . \"res0\" AS \"res0\" , \"t0\" . \"res1\" AS \"res1\" , \"t0\" . \"res2\" AS \"res2\" , \"t0\" . \"res3\" AS \"res3\" , \"t0\" . \"res4\" AS \"res4\" , \"t0\" . \"res5\" AS \"res5\" , \"t0\" . \"res6\" AS \"res6\" , \"t0\" . \"res7\" AS \"res7\" , \"t0\" . \"res8\" AS \"res8\" , \"t0\" . \"res9\" AS \"res9\" , \"t0\" . \"res10\" AS \"res10\" , \"t0\" . \"res11\" AS \"res11\" , \"t0\" . \"res12\" AS \"res12\" , \"t1\" . \"TrackId\" AS \"res13\" , \"t1\" . \"Name\" AS \"res14\" , \"t1\" . \"AlbumId\" AS \"res15\" , \"t1\" . \"MediaTypeId\" AS \"res16\" , \"t1\" . \"GenreId\" AS \"res17\" , \"t1\" . \"Composer\" AS \"res18\" , \"t1\" . \"Milliseconds\" AS \"res19\" , \"t1\" . \"Bytes\" AS \"res20\" , \"t1\" . \"UnitPrice\" AS \"res21\" FROM ( SELECT \"t0\" . \"CustomerId\" AS \"res0\" , \"t0\" . \"FirstName\" AS \"res1\" , \"t0\" . \"LastName\" AS \"res2\" , \"t0\" . \"Company\" AS \"res3\" , \"t0\" . \"Address\" AS \"res4\" , \"t0\" . \"City\" AS \"res5\" , \"t0\" . \"State\" AS \"res6\" , \"t0\" . \"Country\" AS \"res7\" , \"t0\" . \"PostalCode\" AS \"res8\" , \"t0\" . \"Phone\" AS \"res9\" , \"t0\" . \"Fax\" AS \"res10\" , \"t0\" . \"Email\" AS \"res11\" , \"t0\" . \"SupportRepId\" AS \"res12\" FROM \"Customer\" AS \"t0\" WHERE ((( \"t0\" . \"FirstName\" ) LIKE ( ? )) AND (( \"t0\" . \"LastName\" ) LIKE ( ? ))) AND (( CASE WHEN (( \"t0\" . \"State\" ) IS NULL ) AND (( ? ) IS NULL ) THEN ? WHEN (( \"t0\" . \"State\" ) IS NULL ) OR (( ? ) IS NULL ) THEN ? ELSE ( \"t0\" . \"State\" ) = ( ? ) END ) OR ( CASE WHEN (( \"t0\" . \"State\" ) IS NULL ) AND (( ? ) IS NULL ) THEN ? WHEN (( \"t0\" . \"State\" ) IS NULL ) OR (( ? ) IS NULL ) THEN ? ELSE ( \"t0\" . \"State\" ) = ( ? ) END )) LIMIT 10 ) AS \"t0\" INNER JOIN \"Track\" AS \"t1\" ; -- With values: [SQLText \"Jo%\",SQLText \"S%\",SQLText \"CA\",SQLInteger 1,SQLText \"CA\",SQLInteger 0,SQLText \"CA\",SQLText \"WA\",SQLInteger 1,SQLText \"WA\",SQLInteger 0,SQLText \"WA\"]","title":"Basic Queries"},{"location":"user-guide/queries/basic/#data-types","text":"","title":"Data types"},{"location":"user-guide/queries/basic/#the-q-data-type","text":"Beam queries are built using the Q data type. Q 's signature is as follows data Q be db s a In this definition be is the particular Beam backend this Q monad is written for. Each beam backend defines a custom tag type. For example, beam-sqlite provides the Sqlite tag, and beam-postgres provides the Postgres tag. You can see what SQL backends are available in GHCi by asking for info on the BeamSqlBackend class. Prelude Database.Beam Database.Beam.Sqlite Data.Text Database.SQLite.Simple Lens.Micro Data.Time Database.Beam.Backend.SQL T> :info BeamSqlBackend ass (Database.Beam.Backend.Types.BeamBackend be, IsSql92Syntax (BeamSqlBackendSyntax be), Sql92SanityCheck (BeamSqlBackendSyntax be), HasSqlValueSyntax (BeamSqlBackendValueSyntax be) Bool, HasSqlValueSyntax (BeamSqlBackendValueSyntax be) SqlNull, Eq (BeamSqlBackendExpressionSyntax be)) => BeamSqlBackend be -- Defined at /Users/travis/Projects/beam/beam-core/Database/Beam/Backend/SQL.hs:212:1 stance BeamSqlBackend Sqlite -- Defined at /Users/travis/Projects/beam/beam-sqlite/Database/Beam/Sqlite/Connection.hs:157:10 stance (IsSql92Syntax syntax, Sql92SanityCheck syntax, HasSqlValueSyntax (Sql92ValueSyntax syntax) Bool, HasSqlValueSyntax (Sql92ValueSyntax syntax) SqlNull, Eq (Sql92ExpressionSyntax syntax)) => BeamSqlBackend (MockSqlBackend syntax) -- Defined at /Users/travis/Projects/beam/beam-core/Database/Beam/Backend/SQL.hs:238:10 db is the type of the database (as we defined above). This is used to ensure you only query database entities that are in scope in this database. s is the scope parameter. For the most part, you'll write your queries so that they work over all s . Beam manipulates this parameter internally to ensure that the fields in your expressions are always in scope at run-time. a is the type of the result of the query. Q is a monad, which means you can use your favorite Monad , Applicative , and Functor functions. The Functor instance can be used to create projections as explained in the next section . The Monad and Applicative instances can be used to create JOINs .","title":"The Q data type"},{"location":"user-guide/queries/basic/#the-qgenexpr-type","text":"While Q represents the result of whole queries (entire SELECT s for example), QGenExpr represents the type of SQL expressions. QGenExpr also takes some type parameters: data QGenExpr context be s a context is the particular way in which this expression is being used. For example, expressions containing aggregates have context ~ QAggregateContext . Expressions returning scalar values have context ~ QValueContext . be is the backend for which this expression is written. For example, expressions destined for execution in PostgreSQL, will substitute this for Postgres (from Database.Beam.Postgres in the beam-postgres package). You can also leave this polymorphic if you want your expression to be useable across multiple backends. Note In previous versions of beam be indicated the 'syntax' rather than the backend. This was confusing because the syntax and backend types were not obviously related. Beam >=0.8.0.0 uses the backend type consistently to indicate where an expression is being used. s is a scoping parameter, which will match the s in Q . a is the type of this expression. For example, expressions returning SQL int values, will have Haskell type Int32 . This ensures that your SQL query won't fail at run-time with a type error. Beam defines some specializations of QGenExpr for common uses. type QExpr = QGenExpr QValueContext type QAgg = QGenExpr QAggregateContext type QOrd = QGenExpr QOrderingContext type QWindowExpr = QGenExpr QWindowingContext type QWindowFrame = QGenExpr QWindowFrameContext type QGroupExpr = QGenExpr QGroupingContext Thus, value expressions can be given the simpler type of QExpr be s a . Expressions containing aggregates are typed as QAgg be s a .","title":"The QGenExpr type"},{"location":"user-guide/queries/basic/#a-note-on-type-inference","text":"These types may seem incredibly complicated. Indeed, the safety that beam tries to provide requires these scary-looking types. But alas, do not fear! Beam is also designed to assist type inference. For the most part, you will rarely need to annotate these types in your code. Occassionally you will need to provide a type for the result of an expression. For example, SELECT ing just the literal 1 may cause an ambiguity, because the compiler won't know which Integral type to use. Beam provides an easy utility function as_ for this. With -XTypeApplications enabled, as_ @ Int32 ( ambiguous expression ) ensures that ambiguous expression has the type QGenExpr ctxt be s Int32 with the ctxt , be , and s types appropriately inferred.","title":"A note on type inference"},{"location":"user-guide/queries/basic/#simple-queries","text":"The easiest query is simply getting all the rows in a specific table. If you have a database object (something with type DatabaseSettings be db ) with some table or view entities, you can use the all_ function to retrieve all rows in a specific table or view. For example, to retrieve all PersonT entries in the exampleDb we defined in the last section, we can say all_ ( persons exampleDb ) :: Q be ExampleDb s ( PersonT ( QExpr s )) Note We give the full type of the query here for illustrative purposes only. There is no need to do so in your own code Two things to note. Firstly, here PersonT is parameterized over the QExpr s higher-kinded type. This means that each field in PersonT now contains a SQL expression instead of a Haskell value. This is the magic that our parameterized types allow. Thus, personFirstName ( all_ ( persons exampleDb )) :: QExpr be s Text and personFirstName ( Person \"John\" \"Smith\" 23 \"john.smith@example.com\" \"8888888888\" :: Person ) :: Text Secondly, the field type has the same scope variable as the entire query. This means, it can only be used in the scope of this query. You will never be able to inspect the type of s from outside Q . Once we have a query in terms of Q , we can use the select function from Database.Beam.Query to turn it into a select statement that can be run against the backend. select takes an expression of type Q , and converts it into a SQL statement, ready to be executed against the database. The output of the query passed to select must follow some conventions, so that beam knows how to serialize, deserialize, and project the appropriate values from the query. In particular, the return type of your query must be either a plain expression (i.e., type QExpr ), a Beamable type (i.e., a table or primary key, defined as above), or any combination of tuples of the above (Beam supports up to 8-tuples by default). Higher-order tuples can be formed by nested tuples. For example, for 16 return values, you can return a 2-tuple of 8-tuples or an 8-tuple of 2-tuples or a 4-tuple of 4-tuples, etc. With this in mind, we can use select to get a query statement against our database. The return type of all_ is just the table we ask for. In this case, we're interested in the persons table. The persons table has the Beamable type PersonT . As expected, the SqlSelect will return us concrete Person values (recall that Person is equivalent to PersonT Identity ). select ( all_ ( persons exampleDb )) :: HasQBuilder be => SqlSelect be Person Normally, you'd ship this select statement off to a backend to run, but for the purposes of this tutorial, we can also ask beam to dump what the standard SQL expression this query encodes. dumpSqlSelect ( all_ ( persons exampleDb )) SELECT ` t0 ` . ` email ` AS \"res0\" , ` t0 ` . ` first_name ` AS \"res1\" , ` t0 ` . ` last_name ` AS \"res2\" , ` t0 ` . ` password ` AS \"res3\" FROM \"cart_users\" AS \"t0\" Internally, dumpSqlSelect uses a beam-core provided syntax to generate standard ANSI SQL expressions. Note that these expressions should not be shipped to a backend directly, as they may not be escaped properly. Still, it is useful to see what would run. Tip all_ only works for TableEntity s. Use allFromView_ for ViewEntity s.","title":"Simple queries"},{"location":"user-guide/queries/basic/#a-note-on-composability","text":"All beam queries are composable . This means that you can freely mix values of type Q in whichever way typechecks and expect a reasonable SQL query. This differs from the behavior of SQL, where the syntax for composing queries depends on the structure of that query. For example, suppose you wanted to fetch all rows of a table, filter them by a condition, limit the amount of rows returned and then join these rows with another table. In SQL, you'd have to write explicit subselects, take care of handling projections, etc. This is because this query doesn't fit into the 'standard' SQL query structure. However, in beam, you can simply write this query. Beam will take care of generating explicit subselects and handling projections. Scoping rules enforced by the Haskell type system ensure that the query is constructed correctly. For example, we can write the following (meaningless) query, and things will work as expected. Haskell Postgres Sqlite do tbl1 <- limit_ 10 $ filter_ ( \\ customer -> (( customerFirstName customer ` like_ ` \"Jo%\" ) &&. ( customerLastName customer ` like_ ` \"S%\" )) &&. ( addressState ( customerAddress customer ) ==. just_ \"CA\" ||. addressState ( customerAddress customer ) ==. just_ \"WA\" )) $ all_ ( customer chinookDb ) tbl2 <- all_ ( track chinookDb ) pure ( tbl1 , tbl2 ) SELECT \"t0\" . \"res0\" AS \"res0\" , \"t0\" . \"res1\" AS \"res1\" , \"t0\" . \"res2\" AS \"res2\" , \"t0\" . \"res3\" AS \"res3\" , \"t0\" . \"res4\" AS \"res4\" , \"t0\" . \"res5\" AS \"res5\" , \"t0\" . \"res6\" AS \"res6\" , \"t0\" . \"res7\" AS \"res7\" , \"t0\" . \"res8\" AS \"res8\" , \"t0\" . \"res9\" AS \"res9\" , \"t0\" . \"res10\" AS \"res10\" , \"t0\" . \"res11\" AS \"res11\" , \"t0\" . \"res12\" AS \"res12\" , \"t1\" . \"TrackId\" AS \"res13\" , \"t1\" . \"Name\" AS \"res14\" , \"t1\" . \"AlbumId\" AS \"res15\" , \"t1\" . \"MediaTypeId\" AS \"res16\" , \"t1\" . \"GenreId\" AS \"res17\" , \"t1\" . \"Composer\" AS \"res18\" , \"t1\" . \"Milliseconds\" AS \"res19\" , \"t1\" . \"Bytes\" AS \"res20\" , \"t1\" . \"UnitPrice\" AS \"res21\" FROM ( SELECT \"t0\" . \"CustomerId\" AS \"res0\" , \"t0\" . \"FirstName\" AS \"res1\" , \"t0\" . \"LastName\" AS \"res2\" , \"t0\" . \"Company\" AS \"res3\" , \"t0\" . \"Address\" AS \"res4\" , \"t0\" . \"City\" AS \"res5\" , \"t0\" . \"State\" AS \"res6\" , \"t0\" . \"Country\" AS \"res7\" , \"t0\" . \"PostalCode\" AS \"res8\" , \"t0\" . \"Phone\" AS \"res9\" , \"t0\" . \"Fax\" AS \"res10\" , \"t0\" . \"Email\" AS \"res11\" , \"t0\" . \"SupportRepId\" AS \"res12\" FROM \"Customer\" AS \"t0\" WHERE ((( \"t0\" . \"FirstName\" ) LIKE ( 'Jo%' )) AND (( \"t0\" . \"LastName\" ) LIKE ( 'S%' ))) AND ((( \"t0\" . \"State\" ) IS NOT DISTINCT FROM ( 'CA' )) OR (( \"t0\" . \"State\" ) IS NOT DISTINCT FROM ( 'WA' ))) LIMIT 10 ) AS \"t0\" CROSS JOIN \"Track\" AS \"t1\" SELECT \"t0\" . \"res0\" AS \"res0\" , \"t0\" . \"res1\" AS \"res1\" , \"t0\" . \"res2\" AS \"res2\" , \"t0\" . \"res3\" AS \"res3\" , \"t0\" . \"res4\" AS \"res4\" , \"t0\" . \"res5\" AS \"res5\" , \"t0\" . \"res6\" AS \"res6\" , \"t0\" . \"res7\" AS \"res7\" , \"t0\" . \"res8\" AS \"res8\" , \"t0\" . \"res9\" AS \"res9\" , \"t0\" . \"res10\" AS \"res10\" , \"t0\" . \"res11\" AS \"res11\" , \"t0\" . \"res12\" AS \"res12\" , \"t1\" . \"TrackId\" AS \"res13\" , \"t1\" . \"Name\" AS \"res14\" , \"t1\" . \"AlbumId\" AS \"res15\" , \"t1\" . \"MediaTypeId\" AS \"res16\" , \"t1\" . \"GenreId\" AS \"res17\" , \"t1\" . \"Composer\" AS \"res18\" , \"t1\" . \"Milliseconds\" AS \"res19\" , \"t1\" . \"Bytes\" AS \"res20\" , \"t1\" . \"UnitPrice\" AS \"res21\" FROM ( SELECT \"t0\" . \"CustomerId\" AS \"res0\" , \"t0\" . \"FirstName\" AS \"res1\" , \"t0\" . \"LastName\" AS \"res2\" , \"t0\" . \"Company\" AS \"res3\" , \"t0\" . \"Address\" AS \"res4\" , \"t0\" . \"City\" AS \"res5\" , \"t0\" . \"State\" AS \"res6\" , \"t0\" . \"Country\" AS \"res7\" , \"t0\" . \"PostalCode\" AS \"res8\" , \"t0\" . \"Phone\" AS \"res9\" , \"t0\" . \"Fax\" AS \"res10\" , \"t0\" . \"Email\" AS \"res11\" , \"t0\" . \"SupportRepId\" AS \"res12\" FROM \"Customer\" AS \"t0\" WHERE ((( \"t0\" . \"FirstName\" ) LIKE ( ? )) AND (( \"t0\" . \"LastName\" ) LIKE ( ? ))) AND (( CASE WHEN (( \"t0\" . \"State\" ) IS NULL ) AND (( ? ) IS NULL ) THEN ? WHEN (( \"t0\" . \"State\" ) IS NULL ) OR (( ? ) IS NULL ) THEN ? ELSE ( \"t0\" . \"State\" ) = ( ? ) END ) OR ( CASE WHEN (( \"t0\" . \"State\" ) IS NULL ) AND (( ? ) IS NULL ) THEN ? WHEN (( \"t0\" . \"State\" ) IS NULL ) OR (( ? ) IS NULL ) THEN ? ELSE ( \"t0\" . \"State\" ) = ( ? ) END )) LIMIT 10 ) AS \"t0\" INNER JOIN \"Track\" AS \"t1\" ; -- With values: [SQLText \"Jo%\",SQLText \"S%\",SQLText \"CA\",SQLInteger 1,SQLText \"CA\",SQLInteger 0,SQLText \"CA\",SQLText \"WA\",SQLInteger 1,SQLText \"WA\",SQLInteger 0,SQLText \"WA\"] This allows you to easily factor out queries. This means you can build a query library in your application and then freely mix and match these queries as necessary. This allows you to offload as much processing to the database as possible, rather than shipping data to your application pre-processing. Haskell Postgres Sqlite -- 'complicatedQuery' could be declared and imported from an external module here. The generated query is the same regardless let complicatedQuery = filter_ ( \\ customer -> (( customerFirstName customer ` like_ ` \"Jo%\" ) &&. ( customerLastName customer ` like_ ` \"S%\" )) &&. ( addressState ( customerAddress customer ) ==. just_ \"CA\" ||. addressState ( customerAddress customer ) ==. just_ \"WA\" )) $ all_ ( customer chinookDb ) in do tbl1 <- limit_ 10 $ complicatedQuery tbl2 <- all_ ( track chinookDb ) pure ( tbl1 , tbl2 ) SELECT \"t0\" . \"res0\" AS \"res0\" , \"t0\" . \"res1\" AS \"res1\" , \"t0\" . \"res2\" AS \"res2\" , \"t0\" . \"res3\" AS \"res3\" , \"t0\" . \"res4\" AS \"res4\" , \"t0\" . \"res5\" AS \"res5\" , \"t0\" . \"res6\" AS \"res6\" , \"t0\" . \"res7\" AS \"res7\" , \"t0\" . \"res8\" AS \"res8\" , \"t0\" . \"res9\" AS \"res9\" , \"t0\" . \"res10\" AS \"res10\" , \"t0\" . \"res11\" AS \"res11\" , \"t0\" . \"res12\" AS \"res12\" , \"t1\" . \"TrackId\" AS \"res13\" , \"t1\" . \"Name\" AS \"res14\" , \"t1\" . \"AlbumId\" AS \"res15\" , \"t1\" . \"MediaTypeId\" AS \"res16\" , \"t1\" . \"GenreId\" AS \"res17\" , \"t1\" . \"Composer\" AS \"res18\" , \"t1\" . \"Milliseconds\" AS \"res19\" , \"t1\" . \"Bytes\" AS \"res20\" , \"t1\" . \"UnitPrice\" AS \"res21\" FROM ( SELECT \"t0\" . \"CustomerId\" AS \"res0\" , \"t0\" . \"FirstName\" AS \"res1\" , \"t0\" . \"LastName\" AS \"res2\" , \"t0\" . \"Company\" AS \"res3\" , \"t0\" . \"Address\" AS \"res4\" , \"t0\" . \"City\" AS \"res5\" , \"t0\" . \"State\" AS \"res6\" , \"t0\" . \"Country\" AS \"res7\" , \"t0\" . \"PostalCode\" AS \"res8\" , \"t0\" . \"Phone\" AS \"res9\" , \"t0\" . \"Fax\" AS \"res10\" , \"t0\" . \"Email\" AS \"res11\" , \"t0\" . \"SupportRepId\" AS \"res12\" FROM \"Customer\" AS \"t0\" WHERE ((( \"t0\" . \"FirstName\" ) LIKE ( 'Jo%' )) AND (( \"t0\" . \"LastName\" ) LIKE ( 'S%' ))) AND ((( \"t0\" . \"State\" ) IS NOT DISTINCT FROM ( 'CA' )) OR (( \"t0\" . \"State\" ) IS NOT DISTINCT FROM ( 'WA' ))) LIMIT 10 ) AS \"t0\" CROSS JOIN \"Track\" AS \"t1\" SELECT \"t0\" . \"res0\" AS \"res0\" , \"t0\" . \"res1\" AS \"res1\" , \"t0\" . \"res2\" AS \"res2\" , \"t0\" . \"res3\" AS \"res3\" , \"t0\" . \"res4\" AS \"res4\" , \"t0\" . \"res5\" AS \"res5\" , \"t0\" . \"res6\" AS \"res6\" , \"t0\" . \"res7\" AS \"res7\" , \"t0\" . \"res8\" AS \"res8\" , \"t0\" . \"res9\" AS \"res9\" , \"t0\" . \"res10\" AS \"res10\" , \"t0\" . \"res11\" AS \"res11\" , \"t0\" . \"res12\" AS \"res12\" , \"t1\" . \"TrackId\" AS \"res13\" , \"t1\" . \"Name\" AS \"res14\" , \"t1\" . \"AlbumId\" AS \"res15\" , \"t1\" . \"MediaTypeId\" AS \"res16\" , \"t1\" . \"GenreId\" AS \"res17\" , \"t1\" . \"Composer\" AS \"res18\" , \"t1\" . \"Milliseconds\" AS \"res19\" , \"t1\" . \"Bytes\" AS \"res20\" , \"t1\" . \"UnitPrice\" AS \"res21\" FROM ( SELECT \"t0\" . \"CustomerId\" AS \"res0\" , \"t0\" . \"FirstName\" AS \"res1\" , \"t0\" . \"LastName\" AS \"res2\" , \"t0\" . \"Company\" AS \"res3\" , \"t0\" . \"Address\" AS \"res4\" , \"t0\" . \"City\" AS \"res5\" , \"t0\" . \"State\" AS \"res6\" , \"t0\" . \"Country\" AS \"res7\" , \"t0\" . \"PostalCode\" AS \"res8\" , \"t0\" . \"Phone\" AS \"res9\" , \"t0\" . \"Fax\" AS \"res10\" , \"t0\" . \"Email\" AS \"res11\" , \"t0\" . \"SupportRepId\" AS \"res12\" FROM \"Customer\" AS \"t0\" WHERE ((( \"t0\" . \"FirstName\" ) LIKE ( ? )) AND (( \"t0\" . \"LastName\" ) LIKE ( ? ))) AND (( CASE WHEN (( \"t0\" . \"State\" ) IS NULL ) AND (( ? ) IS NULL ) THEN ? WHEN (( \"t0\" . \"State\" ) IS NULL ) OR (( ? ) IS NULL ) THEN ? ELSE ( \"t0\" . \"State\" ) = ( ? ) END ) OR ( CASE WHEN (( \"t0\" . \"State\" ) IS NULL ) AND (( ? ) IS NULL ) THEN ? WHEN (( \"t0\" . \"State\" ) IS NULL ) OR (( ? ) IS NULL ) THEN ? ELSE ( \"t0\" . \"State\" ) = ( ? ) END )) LIMIT 10 ) AS \"t0\" INNER JOIN \"Track\" AS \"t1\" ; -- With values: [SQLText \"Jo%\",SQLText \"S%\",SQLText \"CA\",SQLInteger 1,SQLText \"CA\",SQLInteger 0,SQLText \"CA\",SQLText \"WA\",SQLInteger 1,SQLText \"WA\",SQLInteger 0,SQLText \"WA\"]","title":"A note on composability"},{"location":"user-guide/queries/combining-queries/","text":"SQL lets you combine the results of multiple SELECT statements using the UNION , INTERSECT , and EXCEPT clauses. SQL Set operations The SQL Set operations are provided as the union_ , intersect_ , and except_ functions. SQL also allows an optional ALL clause to be specified with each of these. Beam implements these as unionAll_ , intersectAll_ , and exceptAll_ respectively. Each combinator takes two queries as arguments. The results of both queries will be combined accordingly. The returned type is the same as the type of both query arguments, which must be the same. For example, suppose we wanted the first and last names of both customers and employees. Haskell Postgres Sqlite let customerNames = fmap ( \\ c -> ( customerFirstName c , customerLastName c )) ( all_ ( customer chinookDb )) employeeNames = fmap ( \\ e -> ( employeeFirstName e , employeeLastName e )) ( all_ ( employee chinookDb )) in union_ customerNames employeeNames ( SELECT \"t0\" . \"FirstName\" AS \"res0\" , \"t0\" . \"LastName\" AS \"res1\" FROM \"Customer\" AS \"t0\" ) UNION ( SELECT \"t0\" . \"FirstName\" AS \"res0\" , \"t0\" . \"LastName\" AS \"res1\" FROM \"Employee\" AS \"t0\" ) SELECT \"t0\" . \"FirstName\" AS \"res0\" , \"t0\" . \"LastName\" AS \"res1\" FROM \"Customer\" AS \"t0\" UNION SELECT \"t0\" . \"FirstName\" AS \"res0\" , \"t0\" . \"LastName\" AS \"res1\" FROM \"Employee\" AS \"t0\" ; -- With values: [] Combining arbitrary set expressions Suppose we wanted all employee and customer first names that were also customer last names but not also employee last names. We could use UNION to combine the results of a query over the first names of employees and customers, and an EXCEPT to get all customer last names that were not employee ones. Finally, an INTERSECT would give us the result we want. The beam query language allows this and many popular backends do as well, but standard SQL makes it difficult to express. Beam has decided to go with the most common implement solution, which is to allow such nesting. This simplifies the API design. On backends which allow such nesting (like Postgres), the query is translated directly. On backends that do not (like SQLite), an appropriate subselect is generated. Haskell Postgres Sqlite let customerFirstNames = fmap customerFirstName ( all_ ( customer chinookDb )) employeeFirstNames = fmap employeeFirstName ( all_ ( employee chinookDb )) customerLastNames = fmap customerLastName ( all_ ( customer chinookDb )) employeeLastNames = fmap employeeLastName ( all_ ( employee chinookDb )) in ( customerFirstNames ` union_ ` employeeFirstNames ) ` intersect_ ` ( customerLastNames ` except_ ` employeeLastNames ) ( ( SELECT \"t0\" . \"FirstName\" AS \"res0\" FROM \"Customer\" AS \"t0\" ) UNION ( SELECT \"t0\" . \"FirstName\" AS \"res0\" FROM \"Employee\" AS \"t0\" )) INTERSECT ( ( SELECT \"t0\" . \"LastName\" AS \"res0\" FROM \"Customer\" AS \"t0\" ) EXCEPT ( SELECT \"t0\" . \"LastName\" AS \"res0\" FROM \"Employee\" AS \"t0\" )) SELECT \"t0\" . \"res0\" AS \"res0\" FROM ( SELECT \"t0\" . \"FirstName\" AS \"res0\" FROM \"Customer\" AS \"t0\" UNION SELECT \"t0\" . \"FirstName\" AS \"res0\" FROM \"Employee\" AS \"t0\" ) AS \"t0\" INTERSECT SELECT \"t0\" . \"res0\" AS \"res0\" FROM ( SELECT \"t0\" . \"LastName\" AS \"res0\" FROM \"Customer\" AS \"t0\" EXCEPT SELECT \"t0\" . \"LastName\" AS \"res0\" FROM \"Employee\" AS \"t0\" ) AS \"t0\" ; -- With values: [] LIMIT / OFFSET and set operations The LIMIT and OFFSET clauses generated by limit_ and offset_ apply to the entire result of the set operation. Beam will correctly generate the query you specify, placing the LIMIT and OFFSET at the appropriate point. If necessary, it will also generate a sub select to preserve the meaning of the query. For example, to get the second ten full names in common. Haskell Postgres Sqlite let customerNames = fmap ( \\ c -> ( customerFirstName c , customerLastName c )) ( all_ ( customer chinookDb )) employeeNames = fmap ( \\ e -> ( employeeFirstName e , employeeLastName e )) ( all_ ( employee chinookDb )) in limit_ 10 ( union_ customerNames employeeNames ) ( SELECT \"t0\" . \"FirstName\" AS \"res0\" , \"t0\" . \"LastName\" AS \"res1\" FROM \"Customer\" AS \"t0\" ) UNION ( SELECT \"t0\" . \"FirstName\" AS \"res0\" , \"t0\" . \"LastName\" AS \"res1\" FROM \"Employee\" AS \"t0\" ) LIMIT 10 SELECT \"t0\" . \"FirstName\" AS \"res0\" , \"t0\" . \"LastName\" AS \"res1\" FROM \"Customer\" AS \"t0\" UNION SELECT \"t0\" . \"FirstName\" AS \"res0\" , \"t0\" . \"LastName\" AS \"res1\" FROM \"Employee\" AS \"t0\" LIMIT 10 ; -- With values: [] If we only wanted the union of the first 10 names of each. Haskell Postgres Sqlite let customerNames = fmap ( \\ c -> ( customerFirstName c , customerLastName c )) ( all_ ( customer chinookDb )) employeeNames = fmap ( \\ e -> ( employeeFirstName e , employeeLastName e )) ( all_ ( employee chinookDb )) in union_ ( limit_ 10 customerNames ) ( limit_ 10 employeeNames ) ( SELECT \"t0\" . \"res0\" AS \"res0\" , \"t0\" . \"res1\" AS \"res1\" FROM ( SELECT \"t0\" . \"FirstName\" AS \"res0\" , \"t0\" . \"LastName\" AS \"res1\" FROM \"Customer\" AS \"t0\" LIMIT 10 ) AS \"t0\" ) UNION ( SELECT \"t0\" . \"res0\" AS \"res0\" , \"t0\" . \"res1\" AS \"res1\" FROM ( SELECT \"t0\" . \"FirstName\" AS \"res0\" , \"t0\" . \"LastName\" AS \"res1\" FROM \"Employee\" AS \"t0\" LIMIT 10 ) AS \"t0\" ) SELECT \"t0\" . \"res0\" AS \"res0\" , \"t0\" . \"res1\" AS \"res1\" FROM ( SELECT \"t0\" . \"FirstName\" AS \"res0\" , \"t0\" . \"LastName\" AS \"res1\" FROM \"Customer\" AS \"t0\" LIMIT 10 ) AS \"t0\" UNION SELECT \"t0\" . \"res0\" AS \"res0\" , \"t0\" . \"res1\" AS \"res1\" FROM ( SELECT \"t0\" . \"FirstName\" AS \"res0\" , \"t0\" . \"LastName\" AS \"res1\" FROM \"Employee\" AS \"t0\" LIMIT 10 ) AS \"t0\" ; -- With values: []","title":"Combining queries"},{"location":"user-guide/queries/combining-queries/#sql-set-operations","text":"The SQL Set operations are provided as the union_ , intersect_ , and except_ functions. SQL also allows an optional ALL clause to be specified with each of these. Beam implements these as unionAll_ , intersectAll_ , and exceptAll_ respectively. Each combinator takes two queries as arguments. The results of both queries will be combined accordingly. The returned type is the same as the type of both query arguments, which must be the same. For example, suppose we wanted the first and last names of both customers and employees. Haskell Postgres Sqlite let customerNames = fmap ( \\ c -> ( customerFirstName c , customerLastName c )) ( all_ ( customer chinookDb )) employeeNames = fmap ( \\ e -> ( employeeFirstName e , employeeLastName e )) ( all_ ( employee chinookDb )) in union_ customerNames employeeNames ( SELECT \"t0\" . \"FirstName\" AS \"res0\" , \"t0\" . \"LastName\" AS \"res1\" FROM \"Customer\" AS \"t0\" ) UNION ( SELECT \"t0\" . \"FirstName\" AS \"res0\" , \"t0\" . \"LastName\" AS \"res1\" FROM \"Employee\" AS \"t0\" ) SELECT \"t0\" . \"FirstName\" AS \"res0\" , \"t0\" . \"LastName\" AS \"res1\" FROM \"Customer\" AS \"t0\" UNION SELECT \"t0\" . \"FirstName\" AS \"res0\" , \"t0\" . \"LastName\" AS \"res1\" FROM \"Employee\" AS \"t0\" ; -- With values: []","title":"SQL Set operations"},{"location":"user-guide/queries/combining-queries/#combining-arbitrary-set-expressions","text":"Suppose we wanted all employee and customer first names that were also customer last names but not also employee last names. We could use UNION to combine the results of a query over the first names of employees and customers, and an EXCEPT to get all customer last names that were not employee ones. Finally, an INTERSECT would give us the result we want. The beam query language allows this and many popular backends do as well, but standard SQL makes it difficult to express. Beam has decided to go with the most common implement solution, which is to allow such nesting. This simplifies the API design. On backends which allow such nesting (like Postgres), the query is translated directly. On backends that do not (like SQLite), an appropriate subselect is generated. Haskell Postgres Sqlite let customerFirstNames = fmap customerFirstName ( all_ ( customer chinookDb )) employeeFirstNames = fmap employeeFirstName ( all_ ( employee chinookDb )) customerLastNames = fmap customerLastName ( all_ ( customer chinookDb )) employeeLastNames = fmap employeeLastName ( all_ ( employee chinookDb )) in ( customerFirstNames ` union_ ` employeeFirstNames ) ` intersect_ ` ( customerLastNames ` except_ ` employeeLastNames ) ( ( SELECT \"t0\" . \"FirstName\" AS \"res0\" FROM \"Customer\" AS \"t0\" ) UNION ( SELECT \"t0\" . \"FirstName\" AS \"res0\" FROM \"Employee\" AS \"t0\" )) INTERSECT ( ( SELECT \"t0\" . \"LastName\" AS \"res0\" FROM \"Customer\" AS \"t0\" ) EXCEPT ( SELECT \"t0\" . \"LastName\" AS \"res0\" FROM \"Employee\" AS \"t0\" )) SELECT \"t0\" . \"res0\" AS \"res0\" FROM ( SELECT \"t0\" . \"FirstName\" AS \"res0\" FROM \"Customer\" AS \"t0\" UNION SELECT \"t0\" . \"FirstName\" AS \"res0\" FROM \"Employee\" AS \"t0\" ) AS \"t0\" INTERSECT SELECT \"t0\" . \"res0\" AS \"res0\" FROM ( SELECT \"t0\" . \"LastName\" AS \"res0\" FROM \"Customer\" AS \"t0\" EXCEPT SELECT \"t0\" . \"LastName\" AS \"res0\" FROM \"Employee\" AS \"t0\" ) AS \"t0\" ; -- With values: []","title":"Combining arbitrary set expressions"},{"location":"user-guide/queries/combining-queries/#limitoffset-and-set-operations","text":"The LIMIT and OFFSET clauses generated by limit_ and offset_ apply to the entire result of the set operation. Beam will correctly generate the query you specify, placing the LIMIT and OFFSET at the appropriate point. If necessary, it will also generate a sub select to preserve the meaning of the query. For example, to get the second ten full names in common. Haskell Postgres Sqlite let customerNames = fmap ( \\ c -> ( customerFirstName c , customerLastName c )) ( all_ ( customer chinookDb )) employeeNames = fmap ( \\ e -> ( employeeFirstName e , employeeLastName e )) ( all_ ( employee chinookDb )) in limit_ 10 ( union_ customerNames employeeNames ) ( SELECT \"t0\" . \"FirstName\" AS \"res0\" , \"t0\" . \"LastName\" AS \"res1\" FROM \"Customer\" AS \"t0\" ) UNION ( SELECT \"t0\" . \"FirstName\" AS \"res0\" , \"t0\" . \"LastName\" AS \"res1\" FROM \"Employee\" AS \"t0\" ) LIMIT 10 SELECT \"t0\" . \"FirstName\" AS \"res0\" , \"t0\" . \"LastName\" AS \"res1\" FROM \"Customer\" AS \"t0\" UNION SELECT \"t0\" . \"FirstName\" AS \"res0\" , \"t0\" . \"LastName\" AS \"res1\" FROM \"Employee\" AS \"t0\" LIMIT 10 ; -- With values: [] If we only wanted the union of the first 10 names of each. Haskell Postgres Sqlite let customerNames = fmap ( \\ c -> ( customerFirstName c , customerLastName c )) ( all_ ( customer chinookDb )) employeeNames = fmap ( \\ e -> ( employeeFirstName e , employeeLastName e )) ( all_ ( employee chinookDb )) in union_ ( limit_ 10 customerNames ) ( limit_ 10 employeeNames ) ( SELECT \"t0\" . \"res0\" AS \"res0\" , \"t0\" . \"res1\" AS \"res1\" FROM ( SELECT \"t0\" . \"FirstName\" AS \"res0\" , \"t0\" . \"LastName\" AS \"res1\" FROM \"Customer\" AS \"t0\" LIMIT 10 ) AS \"t0\" ) UNION ( SELECT \"t0\" . \"res0\" AS \"res0\" , \"t0\" . \"res1\" AS \"res1\" FROM ( SELECT \"t0\" . \"FirstName\" AS \"res0\" , \"t0\" . \"LastName\" AS \"res1\" FROM \"Employee\" AS \"t0\" LIMIT 10 ) AS \"t0\" ) SELECT \"t0\" . \"res0\" AS \"res0\" , \"t0\" . \"res1\" AS \"res1\" FROM ( SELECT \"t0\" . \"FirstName\" AS \"res0\" , \"t0\" . \"LastName\" AS \"res1\" FROM \"Customer\" AS \"t0\" LIMIT 10 ) AS \"t0\" UNION SELECT \"t0\" . \"res0\" AS \"res0\" , \"t0\" . \"res1\" AS \"res1\" FROM ( SELECT \"t0\" . \"FirstName\" AS \"res0\" , \"t0\" . \"LastName\" AS \"res1\" FROM \"Employee\" AS \"t0\" LIMIT 10 ) AS \"t0\" ; -- With values: []","title":"LIMIT/OFFSET and set operations"},{"location":"user-guide/queries/common-table-expressions/","text":"Common table expressions are a SQL99 feature that allow you to reuse common subqueries in your queries. There is often no semantic difference between CTEs and reusing Beam queries, but backends sometimes have optimizations that will only fire when using CTEs. Common table expressions are also necessary to write recursive queries. Let's start with an example Common table expressions are complicated, so let's start with an example. In the window function section, we saw how to find the genre representing the most tracks in a particular album. Haskell Postgres let albumGenreCnts = aggregate_ ( \\ t -> ( group_ ( trackAlbumId t ) , group_ ( trackGenreId t ) , as_ @ Int32 countAll_ )) $ all_ ( track chinookDb ) withMaxCounts = withWindow_ ( \\ ( albumId , _ , _ ) -> frame_ ( partitionBy_ albumId ) noOrder_ noBounds_ ) ( \\ ( albumId , genreId , trackCnt ) albumWindow -> ( albumId , genreId , trackCnt , max_ trackCnt ` over_ ` albumWindow )) $ albumGenreCnts in filter_' ( \\ ( _ , _ , trackCnt , maxTrackCntPerAlbum ) -> just_ trackCnt ==?. maxTrackCntPerAlbum ) withMaxCounts SELECT \"t0\" . \"res0\" AS \"res0\" , \"t0\" . \"res1\" AS \"res1\" , \"t0\" . \"res2\" AS \"res2\" , \"t0\" . \"res3\" AS \"res3\" FROM ( SELECT \"t0\" . \"AlbumId\" AS \"res0\" , \"t0\" . \"GenreId\" AS \"res1\" , COUNT ( * ) AS \"res2\" , MAX ( COUNT ( * )) OVER ( PARTITION BY \"t0\" . \"AlbumId\" ) AS \"res3\" FROM \"Track\" AS \"t0\" GROUP BY \"t0\" . \"AlbumId\" , \"t0\" . \"GenreId\" ) AS \"t0\" WHERE ( \"t0\" . \"res2\" ) = ( \"t0\" . \"res3\" ) Now, suppose that, instead of the album and genre ids, we wanted the names, along with the name of the artist who produced the album. We can do this by just joining over the above Haskell Postgres let albumGenreCnts = aggregate_ ( \\ t -> ( group_ ( trackAlbumId t ) , group_ ( trackGenreId t ) , as_ @ Int32 countAll_ )) $ all_ ( track chinookDb ) withMaxCounts = withWindow_ ( \\ ( albumId , _ , _ ) -> frame_ ( partitionBy_ albumId ) noOrder_ noBounds_ ) ( \\ ( albumId , genreId , trackCnt ) albumWindow -> ( albumId , genreId , trackCnt , max_ trackCnt ` over_ ` albumWindow )) $ albumGenreCnts albumAndRepresentativeGenres = filter_' ( \\ ( _ , _ , trackCnt , maxTrackCntPerAlbum ) -> just_ trackCnt ==?. maxTrackCntPerAlbum ) withMaxCounts in do ( albumId , genreId , _ , _ ) <- albumAndRepresentativeGenres genre_ <- join_' ( genre chinookDb ) ( \\ g -> genreId ==?. just_ ( primaryKey g )) album_ <- join_' ( album chinookDb ) ( \\ a -> albumId ==?. just_ ( primaryKey a )) artistName <- fmap artistName $ join_ ( artist chinookDb ) ( \\ a -> albumArtist album_ ` references_ ` a ) pure ( artistName , albumTitle album_ , genreName genre_ ) SELECT \"t3\" . \"Name\" AS \"res0\" , \"t2\" . \"Title\" AS \"res1\" , \"t1\" . \"Name\" AS \"res2\" FROM ( SELECT \"t0\" . \"AlbumId\" AS \"res0\" , \"t0\" . \"GenreId\" AS \"res1\" , COUNT ( * ) AS \"res2\" , MAX ( COUNT ( * )) OVER ( PARTITION BY \"t0\" . \"AlbumId\" ) AS \"res3\" FROM \"Track\" AS \"t0\" GROUP BY \"t0\" . \"AlbumId\" , \"t0\" . \"GenreId\" ) AS \"t0\" INNER JOIN \"Genre\" AS \"t1\" ON ( \"t0\" . \"res1\" ) = ( \"t1\" . \"GenreId\" ) INNER JOIN \"Album\" AS \"t2\" ON ( \"t0\" . \"res0\" ) = ( \"t2\" . \"AlbumId\" ) INNER JOIN \"Artist\" AS \"t3\" ON ( \"t2\" . \"ArtistId\" ) = ( \"t3\" . \"ArtistId\" ) WHERE ( \"t0\" . \"res2\" ) = ( \"t0\" . \"res3\" ) Because we're using Beam, and Beam is just Haskell, we are free to compose queries arbitrarily, and get the right results. However, the generated SQL is a bit dense, and some backends may not optimize it well. Moreover, if we plan on using albumAndRepresentativeGenres more than once, then we'd like to express this syntactically, rather than repeating the query wherever we need it. In other words, we want to be able to signal the concept of re-use to the database system. The selectWith function We can rewrite the above query using common table expressions. Firstly, we'll have to tell Beam that we want to write a SELECT statement with a WITH expression. We can do this by using selectWith instead of select . selectWith takes one argument, which is a monadic action returning a query. The monad expected is the With monad from Database.Beam.Query.CTE . Within this monad, bindings represent queries we would like bound at the top-level. So, let's start building our query using selectWith . We'll want to return a list, so we'll use runSelectReturningList . runSelectReturningList $ selectWith $ do Binding subqueries with selecting Now, we're in the With monad, so we can start binding common table expressions. We can bind multiple different types of results here. On all backends that support CTEs, you can bind the results of SELECT , but in some backends, you can bind the results of INSERT , DELETE , UPDATE , etc. In our case, we would like to bind the results of a SELECT statement, so we can use the selecting function. This function takes a query (represented by a value of type Q ) and returns a ReusableQ , which is a query value that can be re-used elsewhere. A ReusableQ is parameterized by three paramaters data ReusableQ be db res be -- This is the backend that the query is in. For example, Postgres for beam-postgres or Sqlite for beam-sqlite . db -- This is the type of the database the query is written over res -- This is the type of each row returned by the query. Notice that ReusableQ has no scoping parameter like regular Q expressions. This is because ReusableQ values can be rescoped at any level. We'll get to this in the next section. We can introduce the albumAndRepresentativeGenres query into the With monad using selecting . albumAndRepresentativeGenres <- selecting $ let albumGenreCnts = aggregate_ ( \\ t -> ( group_ ( trackAlbumId t ) , group_ ( trackGenreId t ) , as_ @ Int32 countAll_ )) $ all_ ( track chinookDb ) withMaxCounts = withWindow_ ( \\ ( albumId , _ , _ ) -> frame_ ( partitionBy_ albumId ) noOrder_ noBounds_ ) ( \\ ( albumId , genreId , trackCnt ) albumWindow -> ( albumId , genreId , trackCnt , max_ trackCnt ` over_ ` albumWindow )) $ albumGenreCnts in filter_' ( \\ ( _ , _ , trackCnt , maxTrackCntPerAlbum ) -> just_ trackCnt ==?. maxTrackCntPerAlbum ) withMaxCounts Using the query with reuse Now that we have taken that query out of scope, we'll need the ability to refer to its result. Queries are in the Q monad, but albumAndRepresentativeGenres has type ReusableQ . In order to use the value in the Q monad, we can use the reuse function. This utility function ensures that the query is able to be used at any nesting level. Also note that since we're in the With monad, we'll need to inject our query into that using pure . pure $ do ( albumId @ ( AlbumId albumIdColumn ), genreId , _ , _ ) <- reuse albumGenreCountQ genre_ <- join_' ( genre chinookDb ) ( \\ g -> genreId ==?. just_ ( primaryKey g )) album_ <- join_' ( album chinookDb ) ( \\ a -> albumId ==?. just_ ( primaryKey a )) artist_ <- join_ ( artist chinookDb ) ( \\ a -> albumArtist album_ ` references_ ` a ) pure ( artistName artist_ , albumTitle album_ , genreName genre_ ) Phew! Putting all of that together, and executing, we get... Haskell Postgres void $ runSelectReturningList $ selectWith $ do albumAndRepresentativeGenres <- selecting $ let albumGenreCnts = aggregate_ ( \\ t -> ( group_ ( trackAlbumId t ) , group_ ( trackGenreId t ) , as_ @ Int32 countAll_ )) $ all_ ( track chinookDb ) withMaxCounts = withWindow_ ( \\ ( albumId , _ , _ ) -> frame_ ( partitionBy_ albumId ) noOrder_ noBounds_ ) ( \\ ( albumId , genreId , trackCnt ) albumWindow -> ( albumId , genreId , trackCnt , max_ trackCnt ` over_ ` albumWindow )) $ albumGenreCnts in filter_' ( \\ ( _ , _ , trackCnt , maxTrackCntPerAlbum ) -> just_ trackCnt ==?. maxTrackCntPerAlbum ) withMaxCounts pure $ do ( albumId , genreId , _ , _ ) <- reuse albumAndRepresentativeGenres genre_ <- join_' ( genre chinookDb ) ( \\ g -> genreId ==?. just_ ( primaryKey g )) album_ <- join_' ( album chinookDb ) ( \\ a -> albumId ==?. just_ ( primaryKey a )) artist_ <- join_ ( artist chinookDb ) ( \\ a -> albumArtist album_ ` references_ ` a ) pure ( artistName artist_ , albumTitle album_ , genreName genre_ ) WITH \"cte0\" ( \"res0\" , \"res1\" , \"res2\" , \"res3\" ) AS ( SELECT \"cte0_0\" . \"res0\" AS \"res0\" , \"cte0_0\" . \"res1\" AS \"res1\" , \"cte0_0\" . \"res2\" AS \"res2\" , \"cte0_0\" . \"res3\" AS \"res3\" FROM ( SELECT \"cte0_0\" . \"AlbumId\" AS \"res0\" , \"cte0_0\" . \"GenreId\" AS \"res1\" , COUNT ( * ) AS \"res2\" , MAX ( COUNT ( * )) OVER ( PARTITION BY \"cte0_0\" . \"AlbumId\" ) AS \"res3\" FROM \"Track\" AS \"cte0_0\" GROUP BY \"cte0_0\" . \"AlbumId\" , \"cte0_0\" . \"GenreId\" ) AS \"cte0_0\" WHERE ( \"cte0_0\" . \"res2\" ) = ( \"cte0_0\" . \"res3\" )) SELECT \"t3\" . \"Name\" AS \"res0\" , \"t2\" . \"Title\" AS \"res1\" , \"t1\" . \"Name\" AS \"res2\" FROM \"cte0\" AS \"t0\" INNER JOIN \"Genre\" AS \"t1\" ON ( \"t0\" . \"res1\" ) = ( \"t1\" . \"GenreId\" ) INNER JOIN \"Album\" AS \"t2\" ON ( \"t0\" . \"res0\" ) = ( \"t2\" . \"AlbumId\" ) INNER JOIN \"Artist\" AS \"t3\" ON ( \"t2\" . \"ArtistId\" ) = ( \"t3\" . \"ArtistId\" ); Refining our query Notice that the filter_' condition on the common table expression resulted in a nasty subselect. We can get rid of that by introducing the filter_' in the outer query. Haskell Postgres void $ runSelectReturningList $ selectWith $ do albumAndRepresentativeGenres <- selecting $ let albumGenreCnts = aggregate_ ( \\ t -> ( group_ ( trackAlbumId t ) , group_ ( trackGenreId t ) , as_ @ Int32 countAll_ )) $ all_ ( track chinookDb ) in withWindow_ ( \\ ( albumId , _ , _ ) -> frame_ ( partitionBy_ albumId ) noOrder_ noBounds_ ) ( \\ ( albumId , genreId , trackCnt ) albumWindow -> ( albumId , genreId , trackCnt , max_ trackCnt ` over_ ` albumWindow )) $ albumGenreCnts pure $ do ( albumId , genreId , _ , _ ) <- filter_' ( \\ ( _ , _ , trackCnt , maxTrackCntInAlbum ) -> just_ trackCnt ==?. maxTrackCntInAlbum ) $ reuse albumAndRepresentativeGenres genre_ <- join_' ( genre chinookDb ) ( \\ g -> genreId ==?. just_ ( primaryKey g )) album_ <- join_' ( album chinookDb ) ( \\ a -> albumId ==?. just_ ( primaryKey a )) artist_ <- join_ ( artist chinookDb ) ( \\ a -> albumArtist album_ ` references_ ` a ) pure ( artistName artist_ , albumTitle album_ , genreName genre_ ) WITH \"cte0\" ( \"res0\" , \"res1\" , \"res2\" , \"res3\" ) AS ( SELECT \"cte0_0\" . \"AlbumId\" AS \"res0\" , \"cte0_0\" . \"GenreId\" AS \"res1\" , COUNT ( * ) AS \"res2\" , MAX ( COUNT ( * )) OVER ( PARTITION BY \"cte0_0\" . \"AlbumId\" ) AS \"res3\" FROM \"Track\" AS \"cte0_0\" GROUP BY \"cte0_0\" . \"AlbumId\" , \"cte0_0\" . \"GenreId\" ) SELECT \"t3\" . \"Name\" AS \"res0\" , \"t2\" . \"Title\" AS \"res1\" , \"t1\" . \"Name\" AS \"res2\" FROM \"cte0\" AS \"t0\" INNER JOIN \"Genre\" AS \"t1\" ON ( \"t0\" . \"res1\" ) = ( \"t1\" . \"GenreId\" ) INNER JOIN \"Album\" AS \"t2\" ON ( \"t0\" . \"res0\" ) = ( \"t2\" . \"AlbumId\" ) INNER JOIN \"Artist\" AS \"t3\" ON ( \"t2\" . \"ArtistId\" ) = ( \"t3\" . \"ArtistId\" ) WHERE ( \"t0\" . \"res2\" ) = ( \"t0\" . \"res3\" ); To demonstrate that we can still use all beam's features, let's only return results for albums with tracks of more than one genre. We can do this by using a quantified comparison operator on the album id column, and a subquery to find all albums with more than one genre. Haskell Postgres void $ runSelectReturningList $ selectWith $ do albumAndRepresentativeGenres <- selecting $ let albumGenreCnts = aggregate_ ( \\ t -> ( group_ ( trackAlbumId t ) , group_ ( trackGenreId t ) , as_ @ Int32 countAll_ )) $ all_ ( track chinookDb ) in withWindow_ ( \\ ( albumId , _ , _ ) -> frame_ ( partitionBy_ albumId ) noOrder_ noBounds_ ) ( \\ ( albumId , genreId , trackCnt ) albumWindow -> ( albumId , genreId , trackCnt , max_ trackCnt ` over_ ` albumWindow )) $ albumGenreCnts pure $ do ( albumId @ ( AlbumId albumIdColumn ), genreId , _ , _ ) <- filter_' ( \\ ( _ , _ , trackCnt , maxTrackCntInAlbum ) -> just_ trackCnt ==?. maxTrackCntInAlbum ) $ reuse albumAndRepresentativeGenres genre_ <- join_' ( genre chinookDb ) ( \\ g -> genreId ==?. just_ ( primaryKey g )) album_ <- join_' ( album chinookDb ) ( \\ a -> albumId ==?. just_ ( primaryKey a )) artist_ <- join_ ( artist chinookDb ) ( \\ a -> albumArtist album_ ` references_ ` a ) -- Filter out all albums with tracks of only one genre guard_' ( albumIdColumn ==*. anyOf_ ( orderBy_ asc_ $ fmap ( \\ ( AlbumId albumIdRaw , _ ) -> albumIdRaw ) $ filter_ ( \\ ( _ , genreCntByAlbum ) -> genreCntByAlbum >. 1 ) $ aggregate_ ( \\ t -> let GenreId genreId = trackGenreId t in ( group_ ( trackAlbumId t ) , as_ @ Int32 ( countOver_ distinctInGroup_ genreId ))) $ all_ ( track chinookDb ))) pure ( artistName artist_ , albumTitle album_ , genreName genre_ ) WITH \"cte0\" ( \"res0\" , \"res1\" , \"res2\" , \"res3\" ) AS ( SELECT \"cte0_0\" . \"AlbumId\" AS \"res0\" , \"cte0_0\" . \"GenreId\" AS \"res1\" , COUNT ( * ) AS \"res2\" , MAX ( COUNT ( * )) OVER ( PARTITION BY \"cte0_0\" . \"AlbumId\" ) AS \"res3\" FROM \"Track\" AS \"cte0_0\" GROUP BY \"cte0_0\" . \"AlbumId\" , \"cte0_0\" . \"GenreId\" ) SELECT \"t3\" . \"Name\" AS \"res0\" , \"t2\" . \"Title\" AS \"res1\" , \"t1\" . \"Name\" AS \"res2\" FROM \"cte0\" AS \"t0\" INNER JOIN \"Genre\" AS \"t1\" ON ( \"t0\" . \"res1\" ) = ( \"t1\" . \"GenreId\" ) INNER JOIN \"Album\" AS \"t2\" ON ( \"t0\" . \"res0\" ) = ( \"t2\" . \"AlbumId\" ) INNER JOIN \"Artist\" AS \"t3\" ON ( \"t2\" . \"ArtistId\" ) = ( \"t3\" . \"ArtistId\" ) WHERE (( \"t0\" . \"res2\" ) = ( \"t0\" . \"res3\" )) AND (( \"t0\" . \"res0\" ) = ANY ( SELECT \"sub_t0\" . \"AlbumId\" AS \"res0\" FROM \"Track\" AS \"sub_t0\" GROUP BY \"sub_t0\" . \"AlbumId\" HAVING ( COUNT ( DISTINCT \"sub_t0\" . \"GenreId\" )) > ( 1 ) ORDER BY \"sub_t0\" . \"AlbumId\" ASC )); without CTEs, Haskell Postgres do let albumGenreCnts = aggregate_ ( \\ t -> ( group_ ( trackAlbumId t ) , group_ ( trackGenreId t ) , as_ @ Int32 countAll_ )) $ all_ ( track chinookDb ) albumAndRepresentativeGenres = withWindow_ ( \\ ( albumId , _ , _ ) -> frame_ ( partitionBy_ albumId ) noOrder_ noBounds_ ) ( \\ ( albumId , genreId , trackCnt ) albumWindow -> ( albumId , genreId , trackCnt , max_ trackCnt ` over_ ` albumWindow )) $ albumGenreCnts ( albumId @ ( AlbumId albumIdColumn ), genreId , _ , _ ) <- filter_' ( \\ ( _ , _ , trackCnt , maxTrackCntInAlbum ) -> just_ trackCnt ==?. maxTrackCntInAlbum ) $ albumAndRepresentativeGenres genre_ <- join_' ( genre chinookDb ) ( \\ g -> genreId ==?. just_ ( primaryKey g )) album_ <- join_' ( album chinookDb ) ( \\ a -> albumId ==?. just_ ( primaryKey a )) artist_ <- join_ ( artist chinookDb ) ( \\ a -> albumArtist album_ ` references_ ` a ) -- Filter out all albums with tracks of only one genre guard_' ( albumIdColumn ==*. anyOf_ ( orderBy_ asc_ $ fmap ( \\ ( AlbumId albumIdRaw , _ ) -> albumIdRaw ) $ filter_ ( \\ ( _ , genreCntByAlbum ) -> genreCntByAlbum >. 1 ) $ aggregate_ ( \\ t -> let GenreId genreId = trackGenreId t in ( group_ ( trackAlbumId t ) , as_ @ Int32 ( countOver_ distinctInGroup_ genreId ))) $ all_ ( track chinookDb ))) pure ( artistName artist_ , albumTitle album_ , genreName genre_ ) SELECT \"t3\" . \"Name\" AS \"res0\" , \"t2\" . \"Title\" AS \"res1\" , \"t1\" . \"Name\" AS \"res2\" FROM ( SELECT \"t0\" . \"AlbumId\" AS \"res0\" , \"t0\" . \"GenreId\" AS \"res1\" , COUNT ( * ) AS \"res2\" , MAX ( COUNT ( * )) OVER ( PARTITION BY \"t0\" . \"AlbumId\" ) AS \"res3\" FROM \"Track\" AS \"t0\" GROUP BY \"t0\" . \"AlbumId\" , \"t0\" . \"GenreId\" ) AS \"t0\" INNER JOIN \"Genre\" AS \"t1\" ON ( \"t0\" . \"res1\" ) = ( \"t1\" . \"GenreId\" ) INNER JOIN \"Album\" AS \"t2\" ON ( \"t0\" . \"res0\" ) = ( \"t2\" . \"AlbumId\" ) INNER JOIN \"Artist\" AS \"t3\" ON ( \"t2\" . \"ArtistId\" ) = ( \"t3\" . \"ArtistId\" ) WHERE (( \"t0\" . \"res2\" ) = ( \"t0\" . \"res3\" )) AND (( \"t0\" . \"res0\" ) = ANY ( SELECT \"sub_t0\" . \"AlbumId\" AS \"res0\" FROM \"Track\" AS \"sub_t0\" GROUP BY \"sub_t0\" . \"AlbumId\" HAVING ( COUNT ( DISTINCT \"sub_t0\" . \"GenreId\" )) > ( 1 ) ORDER BY \"sub_t0\" . \"AlbumId\" ASC )) Reusing queries multiple times Suppose we wanted to ask the question, \"which albums have tracks under the same genre?\". One easy way of doing this is to do multiple self-joins. We can also use a common table expression. Let's call albums A and B \"genre-related\" if A and B contain at least one track under the same genre. Then, the queries above answer if A and B are \"genre-related\". A natural extension of the question above then is \"Which albums are genre-related to the same album?\". For example, suppose A has Jazz and Rock tracks, B has Rock and Country tracks, and C has Country and Blues tracks. Then, (A, B) and (B, C) will both be results of the query above, but (A, C) will not. We can output tuples like (A, C) by self-joining on the results of the same CTE. Recursive queries Okay, so the next natural extension is to extend this relation by relating any two albums A and D where there exist B and C such that A is related to B , B is related to C , and C is related to D . We can keep extending this query forever, but the queries above require that we specify all lengths we're interested in. Conceptually, we could express in Haskell an infinite query: However, the query generator will loop when serializing this query, because database systems don't operate on laziness they way Haskell does! To solve this, some RDBMS systems offer \"recursive\" queries 1 . The canonical example of a recursive query is the Fibonacci sequence: Haskell Postgres void $ runSelectReturningList $ selectWith $ do rec fib <- selecting ( pure ( as_ @ Int32 0 , as_ @ Int32 1 ) ` union_ ` ( do ( a , b ) <- reuse fib guard_ ( b <. 1000 ) pure ( b , a + b ))) pure ( reuse fib ) WITH RECURSIVE \"cte0\" ( \"res0\" , \"res1\" ) AS ( ( SELECT 0 AS \"res0\" , 1 AS \"res1\" ) UNION ( SELECT \"cte0_0\" . \"res1\" AS \"res0\" , ( \"cte0_0\" . \"res0\" ) + ( \"cte0_0\" . \"res1\" ) AS \"res1\" FROM \"cte0\" AS \"cte0_0\" WHERE ( \"cte0_0\" . \"res1\" ) < ( 1000 ))) SELECT \"t0\" . \"res0\" AS \"res0\" , \"t0\" . \"res1\" AS \"res1\" FROM \"cte0\" AS \"t0\" ; \"Recursive\" here is in quotes, because this is not true, general recursion, but rather iteration. Still the term \"recursive\" has stuck around, so Beam adopts the convention. \u21a9","title":"Common table expressions"},{"location":"user-guide/queries/common-table-expressions/#lets-start-with-an-example","text":"Common table expressions are complicated, so let's start with an example. In the window function section, we saw how to find the genre representing the most tracks in a particular album. Haskell Postgres let albumGenreCnts = aggregate_ ( \\ t -> ( group_ ( trackAlbumId t ) , group_ ( trackGenreId t ) , as_ @ Int32 countAll_ )) $ all_ ( track chinookDb ) withMaxCounts = withWindow_ ( \\ ( albumId , _ , _ ) -> frame_ ( partitionBy_ albumId ) noOrder_ noBounds_ ) ( \\ ( albumId , genreId , trackCnt ) albumWindow -> ( albumId , genreId , trackCnt , max_ trackCnt ` over_ ` albumWindow )) $ albumGenreCnts in filter_' ( \\ ( _ , _ , trackCnt , maxTrackCntPerAlbum ) -> just_ trackCnt ==?. maxTrackCntPerAlbum ) withMaxCounts SELECT \"t0\" . \"res0\" AS \"res0\" , \"t0\" . \"res1\" AS \"res1\" , \"t0\" . \"res2\" AS \"res2\" , \"t0\" . \"res3\" AS \"res3\" FROM ( SELECT \"t0\" . \"AlbumId\" AS \"res0\" , \"t0\" . \"GenreId\" AS \"res1\" , COUNT ( * ) AS \"res2\" , MAX ( COUNT ( * )) OVER ( PARTITION BY \"t0\" . \"AlbumId\" ) AS \"res3\" FROM \"Track\" AS \"t0\" GROUP BY \"t0\" . \"AlbumId\" , \"t0\" . \"GenreId\" ) AS \"t0\" WHERE ( \"t0\" . \"res2\" ) = ( \"t0\" . \"res3\" ) Now, suppose that, instead of the album and genre ids, we wanted the names, along with the name of the artist who produced the album. We can do this by just joining over the above Haskell Postgres let albumGenreCnts = aggregate_ ( \\ t -> ( group_ ( trackAlbumId t ) , group_ ( trackGenreId t ) , as_ @ Int32 countAll_ )) $ all_ ( track chinookDb ) withMaxCounts = withWindow_ ( \\ ( albumId , _ , _ ) -> frame_ ( partitionBy_ albumId ) noOrder_ noBounds_ ) ( \\ ( albumId , genreId , trackCnt ) albumWindow -> ( albumId , genreId , trackCnt , max_ trackCnt ` over_ ` albumWindow )) $ albumGenreCnts albumAndRepresentativeGenres = filter_' ( \\ ( _ , _ , trackCnt , maxTrackCntPerAlbum ) -> just_ trackCnt ==?. maxTrackCntPerAlbum ) withMaxCounts in do ( albumId , genreId , _ , _ ) <- albumAndRepresentativeGenres genre_ <- join_' ( genre chinookDb ) ( \\ g -> genreId ==?. just_ ( primaryKey g )) album_ <- join_' ( album chinookDb ) ( \\ a -> albumId ==?. just_ ( primaryKey a )) artistName <- fmap artistName $ join_ ( artist chinookDb ) ( \\ a -> albumArtist album_ ` references_ ` a ) pure ( artistName , albumTitle album_ , genreName genre_ ) SELECT \"t3\" . \"Name\" AS \"res0\" , \"t2\" . \"Title\" AS \"res1\" , \"t1\" . \"Name\" AS \"res2\" FROM ( SELECT \"t0\" . \"AlbumId\" AS \"res0\" , \"t0\" . \"GenreId\" AS \"res1\" , COUNT ( * ) AS \"res2\" , MAX ( COUNT ( * )) OVER ( PARTITION BY \"t0\" . \"AlbumId\" ) AS \"res3\" FROM \"Track\" AS \"t0\" GROUP BY \"t0\" . \"AlbumId\" , \"t0\" . \"GenreId\" ) AS \"t0\" INNER JOIN \"Genre\" AS \"t1\" ON ( \"t0\" . \"res1\" ) = ( \"t1\" . \"GenreId\" ) INNER JOIN \"Album\" AS \"t2\" ON ( \"t0\" . \"res0\" ) = ( \"t2\" . \"AlbumId\" ) INNER JOIN \"Artist\" AS \"t3\" ON ( \"t2\" . \"ArtistId\" ) = ( \"t3\" . \"ArtistId\" ) WHERE ( \"t0\" . \"res2\" ) = ( \"t0\" . \"res3\" ) Because we're using Beam, and Beam is just Haskell, we are free to compose queries arbitrarily, and get the right results. However, the generated SQL is a bit dense, and some backends may not optimize it well. Moreover, if we plan on using albumAndRepresentativeGenres more than once, then we'd like to express this syntactically, rather than repeating the query wherever we need it. In other words, we want to be able to signal the concept of re-use to the database system.","title":"Let's start with an example"},{"location":"user-guide/queries/common-table-expressions/#the-selectwith-function","text":"We can rewrite the above query using common table expressions. Firstly, we'll have to tell Beam that we want to write a SELECT statement with a WITH expression. We can do this by using selectWith instead of select . selectWith takes one argument, which is a monadic action returning a query. The monad expected is the With monad from Database.Beam.Query.CTE . Within this monad, bindings represent queries we would like bound at the top-level. So, let's start building our query using selectWith . We'll want to return a list, so we'll use runSelectReturningList . runSelectReturningList $ selectWith $ do","title":"The selectWith function"},{"location":"user-guide/queries/common-table-expressions/#binding-subqueries-with-selecting","text":"Now, we're in the With monad, so we can start binding common table expressions. We can bind multiple different types of results here. On all backends that support CTEs, you can bind the results of SELECT , but in some backends, you can bind the results of INSERT , DELETE , UPDATE , etc. In our case, we would like to bind the results of a SELECT statement, so we can use the selecting function. This function takes a query (represented by a value of type Q ) and returns a ReusableQ , which is a query value that can be re-used elsewhere. A ReusableQ is parameterized by three paramaters data ReusableQ be db res be -- This is the backend that the query is in. For example, Postgres for beam-postgres or Sqlite for beam-sqlite . db -- This is the type of the database the query is written over res -- This is the type of each row returned by the query. Notice that ReusableQ has no scoping parameter like regular Q expressions. This is because ReusableQ values can be rescoped at any level. We'll get to this in the next section. We can introduce the albumAndRepresentativeGenres query into the With monad using selecting . albumAndRepresentativeGenres <- selecting $ let albumGenreCnts = aggregate_ ( \\ t -> ( group_ ( trackAlbumId t ) , group_ ( trackGenreId t ) , as_ @ Int32 countAll_ )) $ all_ ( track chinookDb ) withMaxCounts = withWindow_ ( \\ ( albumId , _ , _ ) -> frame_ ( partitionBy_ albumId ) noOrder_ noBounds_ ) ( \\ ( albumId , genreId , trackCnt ) albumWindow -> ( albumId , genreId , trackCnt , max_ trackCnt ` over_ ` albumWindow )) $ albumGenreCnts in filter_' ( \\ ( _ , _ , trackCnt , maxTrackCntPerAlbum ) -> just_ trackCnt ==?. maxTrackCntPerAlbum ) withMaxCounts","title":"Binding subqueries with selecting"},{"location":"user-guide/queries/common-table-expressions/#using-the-query-with-reuse","text":"Now that we have taken that query out of scope, we'll need the ability to refer to its result. Queries are in the Q monad, but albumAndRepresentativeGenres has type ReusableQ . In order to use the value in the Q monad, we can use the reuse function. This utility function ensures that the query is able to be used at any nesting level. Also note that since we're in the With monad, we'll need to inject our query into that using pure . pure $ do ( albumId @ ( AlbumId albumIdColumn ), genreId , _ , _ ) <- reuse albumGenreCountQ genre_ <- join_' ( genre chinookDb ) ( \\ g -> genreId ==?. just_ ( primaryKey g )) album_ <- join_' ( album chinookDb ) ( \\ a -> albumId ==?. just_ ( primaryKey a )) artist_ <- join_ ( artist chinookDb ) ( \\ a -> albumArtist album_ ` references_ ` a ) pure ( artistName artist_ , albumTitle album_ , genreName genre_ ) Phew! Putting all of that together, and executing, we get... Haskell Postgres void $ runSelectReturningList $ selectWith $ do albumAndRepresentativeGenres <- selecting $ let albumGenreCnts = aggregate_ ( \\ t -> ( group_ ( trackAlbumId t ) , group_ ( trackGenreId t ) , as_ @ Int32 countAll_ )) $ all_ ( track chinookDb ) withMaxCounts = withWindow_ ( \\ ( albumId , _ , _ ) -> frame_ ( partitionBy_ albumId ) noOrder_ noBounds_ ) ( \\ ( albumId , genreId , trackCnt ) albumWindow -> ( albumId , genreId , trackCnt , max_ trackCnt ` over_ ` albumWindow )) $ albumGenreCnts in filter_' ( \\ ( _ , _ , trackCnt , maxTrackCntPerAlbum ) -> just_ trackCnt ==?. maxTrackCntPerAlbum ) withMaxCounts pure $ do ( albumId , genreId , _ , _ ) <- reuse albumAndRepresentativeGenres genre_ <- join_' ( genre chinookDb ) ( \\ g -> genreId ==?. just_ ( primaryKey g )) album_ <- join_' ( album chinookDb ) ( \\ a -> albumId ==?. just_ ( primaryKey a )) artist_ <- join_ ( artist chinookDb ) ( \\ a -> albumArtist album_ ` references_ ` a ) pure ( artistName artist_ , albumTitle album_ , genreName genre_ ) WITH \"cte0\" ( \"res0\" , \"res1\" , \"res2\" , \"res3\" ) AS ( SELECT \"cte0_0\" . \"res0\" AS \"res0\" , \"cte0_0\" . \"res1\" AS \"res1\" , \"cte0_0\" . \"res2\" AS \"res2\" , \"cte0_0\" . \"res3\" AS \"res3\" FROM ( SELECT \"cte0_0\" . \"AlbumId\" AS \"res0\" , \"cte0_0\" . \"GenreId\" AS \"res1\" , COUNT ( * ) AS \"res2\" , MAX ( COUNT ( * )) OVER ( PARTITION BY \"cte0_0\" . \"AlbumId\" ) AS \"res3\" FROM \"Track\" AS \"cte0_0\" GROUP BY \"cte0_0\" . \"AlbumId\" , \"cte0_0\" . \"GenreId\" ) AS \"cte0_0\" WHERE ( \"cte0_0\" . \"res2\" ) = ( \"cte0_0\" . \"res3\" )) SELECT \"t3\" . \"Name\" AS \"res0\" , \"t2\" . \"Title\" AS \"res1\" , \"t1\" . \"Name\" AS \"res2\" FROM \"cte0\" AS \"t0\" INNER JOIN \"Genre\" AS \"t1\" ON ( \"t0\" . \"res1\" ) = ( \"t1\" . \"GenreId\" ) INNER JOIN \"Album\" AS \"t2\" ON ( \"t0\" . \"res0\" ) = ( \"t2\" . \"AlbumId\" ) INNER JOIN \"Artist\" AS \"t3\" ON ( \"t2\" . \"ArtistId\" ) = ( \"t3\" . \"ArtistId\" );","title":"Using the query with reuse"},{"location":"user-guide/queries/common-table-expressions/#refining-our-query","text":"Notice that the filter_' condition on the common table expression resulted in a nasty subselect. We can get rid of that by introducing the filter_' in the outer query. Haskell Postgres void $ runSelectReturningList $ selectWith $ do albumAndRepresentativeGenres <- selecting $ let albumGenreCnts = aggregate_ ( \\ t -> ( group_ ( trackAlbumId t ) , group_ ( trackGenreId t ) , as_ @ Int32 countAll_ )) $ all_ ( track chinookDb ) in withWindow_ ( \\ ( albumId , _ , _ ) -> frame_ ( partitionBy_ albumId ) noOrder_ noBounds_ ) ( \\ ( albumId , genreId , trackCnt ) albumWindow -> ( albumId , genreId , trackCnt , max_ trackCnt ` over_ ` albumWindow )) $ albumGenreCnts pure $ do ( albumId , genreId , _ , _ ) <- filter_' ( \\ ( _ , _ , trackCnt , maxTrackCntInAlbum ) -> just_ trackCnt ==?. maxTrackCntInAlbum ) $ reuse albumAndRepresentativeGenres genre_ <- join_' ( genre chinookDb ) ( \\ g -> genreId ==?. just_ ( primaryKey g )) album_ <- join_' ( album chinookDb ) ( \\ a -> albumId ==?. just_ ( primaryKey a )) artist_ <- join_ ( artist chinookDb ) ( \\ a -> albumArtist album_ ` references_ ` a ) pure ( artistName artist_ , albumTitle album_ , genreName genre_ ) WITH \"cte0\" ( \"res0\" , \"res1\" , \"res2\" , \"res3\" ) AS ( SELECT \"cte0_0\" . \"AlbumId\" AS \"res0\" , \"cte0_0\" . \"GenreId\" AS \"res1\" , COUNT ( * ) AS \"res2\" , MAX ( COUNT ( * )) OVER ( PARTITION BY \"cte0_0\" . \"AlbumId\" ) AS \"res3\" FROM \"Track\" AS \"cte0_0\" GROUP BY \"cte0_0\" . \"AlbumId\" , \"cte0_0\" . \"GenreId\" ) SELECT \"t3\" . \"Name\" AS \"res0\" , \"t2\" . \"Title\" AS \"res1\" , \"t1\" . \"Name\" AS \"res2\" FROM \"cte0\" AS \"t0\" INNER JOIN \"Genre\" AS \"t1\" ON ( \"t0\" . \"res1\" ) = ( \"t1\" . \"GenreId\" ) INNER JOIN \"Album\" AS \"t2\" ON ( \"t0\" . \"res0\" ) = ( \"t2\" . \"AlbumId\" ) INNER JOIN \"Artist\" AS \"t3\" ON ( \"t2\" . \"ArtistId\" ) = ( \"t3\" . \"ArtistId\" ) WHERE ( \"t0\" . \"res2\" ) = ( \"t0\" . \"res3\" ); To demonstrate that we can still use all beam's features, let's only return results for albums with tracks of more than one genre. We can do this by using a quantified comparison operator on the album id column, and a subquery to find all albums with more than one genre. Haskell Postgres void $ runSelectReturningList $ selectWith $ do albumAndRepresentativeGenres <- selecting $ let albumGenreCnts = aggregate_ ( \\ t -> ( group_ ( trackAlbumId t ) , group_ ( trackGenreId t ) , as_ @ Int32 countAll_ )) $ all_ ( track chinookDb ) in withWindow_ ( \\ ( albumId , _ , _ ) -> frame_ ( partitionBy_ albumId ) noOrder_ noBounds_ ) ( \\ ( albumId , genreId , trackCnt ) albumWindow -> ( albumId , genreId , trackCnt , max_ trackCnt ` over_ ` albumWindow )) $ albumGenreCnts pure $ do ( albumId @ ( AlbumId albumIdColumn ), genreId , _ , _ ) <- filter_' ( \\ ( _ , _ , trackCnt , maxTrackCntInAlbum ) -> just_ trackCnt ==?. maxTrackCntInAlbum ) $ reuse albumAndRepresentativeGenres genre_ <- join_' ( genre chinookDb ) ( \\ g -> genreId ==?. just_ ( primaryKey g )) album_ <- join_' ( album chinookDb ) ( \\ a -> albumId ==?. just_ ( primaryKey a )) artist_ <- join_ ( artist chinookDb ) ( \\ a -> albumArtist album_ ` references_ ` a ) -- Filter out all albums with tracks of only one genre guard_' ( albumIdColumn ==*. anyOf_ ( orderBy_ asc_ $ fmap ( \\ ( AlbumId albumIdRaw , _ ) -> albumIdRaw ) $ filter_ ( \\ ( _ , genreCntByAlbum ) -> genreCntByAlbum >. 1 ) $ aggregate_ ( \\ t -> let GenreId genreId = trackGenreId t in ( group_ ( trackAlbumId t ) , as_ @ Int32 ( countOver_ distinctInGroup_ genreId ))) $ all_ ( track chinookDb ))) pure ( artistName artist_ , albumTitle album_ , genreName genre_ ) WITH \"cte0\" ( \"res0\" , \"res1\" , \"res2\" , \"res3\" ) AS ( SELECT \"cte0_0\" . \"AlbumId\" AS \"res0\" , \"cte0_0\" . \"GenreId\" AS \"res1\" , COUNT ( * ) AS \"res2\" , MAX ( COUNT ( * )) OVER ( PARTITION BY \"cte0_0\" . \"AlbumId\" ) AS \"res3\" FROM \"Track\" AS \"cte0_0\" GROUP BY \"cte0_0\" . \"AlbumId\" , \"cte0_0\" . \"GenreId\" ) SELECT \"t3\" . \"Name\" AS \"res0\" , \"t2\" . \"Title\" AS \"res1\" , \"t1\" . \"Name\" AS \"res2\" FROM \"cte0\" AS \"t0\" INNER JOIN \"Genre\" AS \"t1\" ON ( \"t0\" . \"res1\" ) = ( \"t1\" . \"GenreId\" ) INNER JOIN \"Album\" AS \"t2\" ON ( \"t0\" . \"res0\" ) = ( \"t2\" . \"AlbumId\" ) INNER JOIN \"Artist\" AS \"t3\" ON ( \"t2\" . \"ArtistId\" ) = ( \"t3\" . \"ArtistId\" ) WHERE (( \"t0\" . \"res2\" ) = ( \"t0\" . \"res3\" )) AND (( \"t0\" . \"res0\" ) = ANY ( SELECT \"sub_t0\" . \"AlbumId\" AS \"res0\" FROM \"Track\" AS \"sub_t0\" GROUP BY \"sub_t0\" . \"AlbumId\" HAVING ( COUNT ( DISTINCT \"sub_t0\" . \"GenreId\" )) > ( 1 ) ORDER BY \"sub_t0\" . \"AlbumId\" ASC )); without CTEs, Haskell Postgres do let albumGenreCnts = aggregate_ ( \\ t -> ( group_ ( trackAlbumId t ) , group_ ( trackGenreId t ) , as_ @ Int32 countAll_ )) $ all_ ( track chinookDb ) albumAndRepresentativeGenres = withWindow_ ( \\ ( albumId , _ , _ ) -> frame_ ( partitionBy_ albumId ) noOrder_ noBounds_ ) ( \\ ( albumId , genreId , trackCnt ) albumWindow -> ( albumId , genreId , trackCnt , max_ trackCnt ` over_ ` albumWindow )) $ albumGenreCnts ( albumId @ ( AlbumId albumIdColumn ), genreId , _ , _ ) <- filter_' ( \\ ( _ , _ , trackCnt , maxTrackCntInAlbum ) -> just_ trackCnt ==?. maxTrackCntInAlbum ) $ albumAndRepresentativeGenres genre_ <- join_' ( genre chinookDb ) ( \\ g -> genreId ==?. just_ ( primaryKey g )) album_ <- join_' ( album chinookDb ) ( \\ a -> albumId ==?. just_ ( primaryKey a )) artist_ <- join_ ( artist chinookDb ) ( \\ a -> albumArtist album_ ` references_ ` a ) -- Filter out all albums with tracks of only one genre guard_' ( albumIdColumn ==*. anyOf_ ( orderBy_ asc_ $ fmap ( \\ ( AlbumId albumIdRaw , _ ) -> albumIdRaw ) $ filter_ ( \\ ( _ , genreCntByAlbum ) -> genreCntByAlbum >. 1 ) $ aggregate_ ( \\ t -> let GenreId genreId = trackGenreId t in ( group_ ( trackAlbumId t ) , as_ @ Int32 ( countOver_ distinctInGroup_ genreId ))) $ all_ ( track chinookDb ))) pure ( artistName artist_ , albumTitle album_ , genreName genre_ ) SELECT \"t3\" . \"Name\" AS \"res0\" , \"t2\" . \"Title\" AS \"res1\" , \"t1\" . \"Name\" AS \"res2\" FROM ( SELECT \"t0\" . \"AlbumId\" AS \"res0\" , \"t0\" . \"GenreId\" AS \"res1\" , COUNT ( * ) AS \"res2\" , MAX ( COUNT ( * )) OVER ( PARTITION BY \"t0\" . \"AlbumId\" ) AS \"res3\" FROM \"Track\" AS \"t0\" GROUP BY \"t0\" . \"AlbumId\" , \"t0\" . \"GenreId\" ) AS \"t0\" INNER JOIN \"Genre\" AS \"t1\" ON ( \"t0\" . \"res1\" ) = ( \"t1\" . \"GenreId\" ) INNER JOIN \"Album\" AS \"t2\" ON ( \"t0\" . \"res0\" ) = ( \"t2\" . \"AlbumId\" ) INNER JOIN \"Artist\" AS \"t3\" ON ( \"t2\" . \"ArtistId\" ) = ( \"t3\" . \"ArtistId\" ) WHERE (( \"t0\" . \"res2\" ) = ( \"t0\" . \"res3\" )) AND (( \"t0\" . \"res0\" ) = ANY ( SELECT \"sub_t0\" . \"AlbumId\" AS \"res0\" FROM \"Track\" AS \"sub_t0\" GROUP BY \"sub_t0\" . \"AlbumId\" HAVING ( COUNT ( DISTINCT \"sub_t0\" . \"GenreId\" )) > ( 1 ) ORDER BY \"sub_t0\" . \"AlbumId\" ASC ))","title":"Refining our query"},{"location":"user-guide/queries/common-table-expressions/#reusing-queries-multiple-times","text":"Suppose we wanted to ask the question, \"which albums have tracks under the same genre?\". One easy way of doing this is to do multiple self-joins. We can also use a common table expression. Let's call albums A and B \"genre-related\" if A and B contain at least one track under the same genre. Then, the queries above answer if A and B are \"genre-related\". A natural extension of the question above then is \"Which albums are genre-related to the same album?\". For example, suppose A has Jazz and Rock tracks, B has Rock and Country tracks, and C has Country and Blues tracks. Then, (A, B) and (B, C) will both be results of the query above, but (A, C) will not. We can output tuples like (A, C) by self-joining on the results of the same CTE.","title":"Reusing queries multiple times"},{"location":"user-guide/queries/common-table-expressions/#recursive-queries","text":"Okay, so the next natural extension is to extend this relation by relating any two albums A and D where there exist B and C such that A is related to B , B is related to C , and C is related to D . We can keep extending this query forever, but the queries above require that we specify all lengths we're interested in. Conceptually, we could express in Haskell an infinite query: However, the query generator will loop when serializing this query, because database systems don't operate on laziness they way Haskell does! To solve this, some RDBMS systems offer \"recursive\" queries 1 . The canonical example of a recursive query is the Fibonacci sequence: Haskell Postgres void $ runSelectReturningList $ selectWith $ do rec fib <- selecting ( pure ( as_ @ Int32 0 , as_ @ Int32 1 ) ` union_ ` ( do ( a , b ) <- reuse fib guard_ ( b <. 1000 ) pure ( b , a + b ))) pure ( reuse fib ) WITH RECURSIVE \"cte0\" ( \"res0\" , \"res1\" ) AS ( ( SELECT 0 AS \"res0\" , 1 AS \"res1\" ) UNION ( SELECT \"cte0_0\" . \"res1\" AS \"res0\" , ( \"cte0_0\" . \"res0\" ) + ( \"cte0_0\" . \"res1\" ) AS \"res1\" FROM \"cte0\" AS \"cte0_0\" WHERE ( \"cte0_0\" . \"res1\" ) < ( 1000 ))) SELECT \"t0\" . \"res0\" AS \"res0\" , \"t0\" . \"res1\" AS \"res1\" FROM \"cte0\" AS \"t0\" ; \"Recursive\" here is in quotes, because this is not true, general recursion, but rather iteration. Still the term \"recursive\" has stuck around, so Beam adopts the convention. \u21a9","title":"Recursive queries"},{"location":"user-guide/queries/data-types/","text":"data types","title":"Data types"},{"location":"user-guide/queries/ordering/","text":"Usually, queries are ordered before LIMIT and OFFSET are applied. Beam supports the standard SQL ORDER BY construct through the orderBy_ function. orderBy_ works like the Haskell function sortBy , with some restrictions. Its first argument is a function which takes as input the output of the given query. The function should return a sorting key, which is either a single sort ordering or a tuple of them. A sort ordering specifies an expression and a direction by which to sort. The result is then sorted lexicographically based on these sort expressions. The second argument to orderBy_ is the query whose results to sort. Use the asc_ and desc_ functions to specify the sort ordering over an arbitrary expression. Note Use nullsFirst_ and nullsLast_ to control the ordering of nulls. See advanced features for more information. For example, to get the first ten albums when sorted lexicographically, use Haskell Postgres Sqlite limit_ 10 $ orderBy_ ( asc_ . albumTitle ) $ all_ ( album chinookDb ) SELECT \"t0\" . \"AlbumId\" AS \"res0\" , \"t0\" . \"Title\" AS \"res1\" , \"t0\" . \"ArtistId\" AS \"res2\" FROM \"Album\" AS \"t0\" ORDER BY \"t0\" . \"Title\" ASC LIMIT 10 SELECT \"t0\" . \"AlbumId\" AS \"res0\" , \"t0\" . \"Title\" AS \"res1\" , \"t0\" . \"ArtistId\" AS \"res2\" FROM \"Album\" AS \"t0\" ORDER BY \"t0\" . \"Title\" ASC LIMIT 10 ; -- With values: [] Again, note that the ordering in which you apply the limit_ and orderBy_ matters. In general, you want to sort before you limit or offset, to keep your result set stable. However, if you really want to sort a limited number of arbitrarily chosen rows, you can use a different ordering. Haskell Postgres Sqlite orderBy_ ( asc_ . albumTitle ) $ limit_ 10 $ all_ ( album chinookDb ) SELECT \"t0\" . \"res0\" AS \"res0\" , \"t0\" . \"res1\" AS \"res1\" , \"t0\" . \"res2\" AS \"res2\" FROM ( SELECT \"t0\" . \"AlbumId\" AS \"res0\" , \"t0\" . \"Title\" AS \"res1\" , \"t0\" . \"ArtistId\" AS \"res2\" FROM \"Album\" AS \"t0\" LIMIT 10 ) AS \"t0\" ORDER BY \"t0\" . \"res1\" ASC SELECT \"t0\" . \"res0\" AS \"res0\" , \"t0\" . \"res1\" AS \"res1\" , \"t0\" . \"res2\" AS \"res2\" FROM ( SELECT \"t0\" . \"AlbumId\" AS \"res0\" , \"t0\" . \"Title\" AS \"res1\" , \"t0\" . \"ArtistId\" AS \"res2\" FROM \"Album\" AS \"t0\" LIMIT 10 ) AS \"t0\" ORDER BY \"t0\" . \"res1\" ASC ; -- With values: [] Multiple ordering keys You can specify multiple keys to order by as well. Keys are sorted lexicographically in the given direction, as specified in the SQL standard. For example, we can sort all employees by their state of residence in ascending order and by their city name in descending order. Haskell Postgres Sqlite limit_ 10 $ orderBy_ ( \\ e -> ( asc_ ( addressState ( employeeAddress e )), desc_ ( addressCity ( employeeAddress e )))) $ all_ ( employee chinookDb ) SELECT \"t0\" . \"EmployeeId\" AS \"res0\" , \"t0\" . \"LastName\" AS \"res1\" , \"t0\" . \"FirstName\" AS \"res2\" , \"t0\" . \"Title\" AS \"res3\" , \"t0\" . \"ReportsTo\" AS \"res4\" , \"t0\" . \"BirthDate\" AS \"res5\" , \"t0\" . \"HireDate\" AS \"res6\" , \"t0\" . \"Address\" AS \"res7\" , \"t0\" . \"City\" AS \"res8\" , \"t0\" . \"State\" AS \"res9\" , \"t0\" . \"Country\" AS \"res10\" , \"t0\" . \"PostalCode\" AS \"res11\" , \"t0\" . \"Phone\" AS \"res12\" , \"t0\" . \"Fax\" AS \"res13\" , \"t0\" . \"Email\" AS \"res14\" FROM \"Employee\" AS \"t0\" ORDER BY \"t0\" . \"State\" ASC , \"t0\" . \"City\" DESC LIMIT 10 SELECT \"t0\" . \"EmployeeId\" AS \"res0\" , \"t0\" . \"LastName\" AS \"res1\" , \"t0\" . \"FirstName\" AS \"res2\" , \"t0\" . \"Title\" AS \"res3\" , \"t0\" . \"ReportsTo\" AS \"res4\" , \"t0\" . \"BirthDate\" AS \"res5\" , \"t0\" . \"HireDate\" AS \"res6\" , \"t0\" . \"Address\" AS \"res7\" , \"t0\" . \"City\" AS \"res8\" , \"t0\" . \"State\" AS \"res9\" , \"t0\" . \"Country\" AS \"res10\" , \"t0\" . \"PostalCode\" AS \"res11\" , \"t0\" . \"Phone\" AS \"res12\" , \"t0\" . \"Fax\" AS \"res13\" , \"t0\" . \"Email\" AS \"res14\" FROM \"Employee\" AS \"t0\" ORDER BY \"t0\" . \"State\" ASC , \"t0\" . \"City\" DESC LIMIT 10 ; -- With values: []","title":"Ordering"},{"location":"user-guide/queries/ordering/#multiple-ordering-keys","text":"You can specify multiple keys to order by as well. Keys are sorted lexicographically in the given direction, as specified in the SQL standard. For example, we can sort all employees by their state of residence in ascending order and by their city name in descending order. Haskell Postgres Sqlite limit_ 10 $ orderBy_ ( \\ e -> ( asc_ ( addressState ( employeeAddress e )), desc_ ( addressCity ( employeeAddress e )))) $ all_ ( employee chinookDb ) SELECT \"t0\" . \"EmployeeId\" AS \"res0\" , \"t0\" . \"LastName\" AS \"res1\" , \"t0\" . \"FirstName\" AS \"res2\" , \"t0\" . \"Title\" AS \"res3\" , \"t0\" . \"ReportsTo\" AS \"res4\" , \"t0\" . \"BirthDate\" AS \"res5\" , \"t0\" . \"HireDate\" AS \"res6\" , \"t0\" . \"Address\" AS \"res7\" , \"t0\" . \"City\" AS \"res8\" , \"t0\" . \"State\" AS \"res9\" , \"t0\" . \"Country\" AS \"res10\" , \"t0\" . \"PostalCode\" AS \"res11\" , \"t0\" . \"Phone\" AS \"res12\" , \"t0\" . \"Fax\" AS \"res13\" , \"t0\" . \"Email\" AS \"res14\" FROM \"Employee\" AS \"t0\" ORDER BY \"t0\" . \"State\" ASC , \"t0\" . \"City\" DESC LIMIT 10 SELECT \"t0\" . \"EmployeeId\" AS \"res0\" , \"t0\" . \"LastName\" AS \"res1\" , \"t0\" . \"FirstName\" AS \"res2\" , \"t0\" . \"Title\" AS \"res3\" , \"t0\" . \"ReportsTo\" AS \"res4\" , \"t0\" . \"BirthDate\" AS \"res5\" , \"t0\" . \"HireDate\" AS \"res6\" , \"t0\" . \"Address\" AS \"res7\" , \"t0\" . \"City\" AS \"res8\" , \"t0\" . \"State\" AS \"res9\" , \"t0\" . \"Country\" AS \"res10\" , \"t0\" . \"PostalCode\" AS \"res11\" , \"t0\" . \"Phone\" AS \"res12\" , \"t0\" . \"Fax\" AS \"res13\" , \"t0\" . \"Email\" AS \"res14\" FROM \"Employee\" AS \"t0\" ORDER BY \"t0\" . \"State\" ASC , \"t0\" . \"City\" DESC LIMIT 10 ; -- With values: []","title":"Multiple ordering keys"},{"location":"user-guide/queries/relationships/","text":"Relational databases are so-named because they're good at expressing relations among data and providing related data in queries. Beam exposes these features in its DSL. For these examples, we're going to use the beam-sqlite backend with the provided sample Chinook database. First, create a SQLite database from the included example. > sqlite3 chinook.db < beam-sqlite/examples/chinook.sql Now, load the chinook database schema in GHCi. Prelude Database . Beam . Sqlite > : load beam - sqlite / examples / Chinook / Schema . hs Prelude Chinook . Schema > chinook <- open \"chinook.db\" One more thing, before we explore how beam handles relationships. Before we do, let's define a quick utility function. Prelude Chinook . Schema > let withConnectionTutorial = runBeamSqliteDebug putStrLn chinook This function prints each of our queries to standard output before running them. Using this function will let us see what SQL is executing. Full inner joins Recall that the Q type is a monad. In many respects, Q operates like the list monad. For those unfamiliar, the monadic bind operator for [] is defined as concatMap . Thus, do a <- [ 1 , 2 , 3 ] b <- [ 4 , 5 , 6 ] return ( a , b ) is equivalent to [(1,4),(1,5),(1,6),(2,4),(2,5),(2,6),(3,4),(3,5),(3,6)] . This operation is similar to the cartesian product from set theory or the inner join from relational algebra. The Q monad fully supports this notion of join, and in fact, every other join is built off of this primitive. For example, to get every row from the invoice table and every row from the invoice line table, with no attention paid to any relationship between the two: Haskell Postgres Sqlite do i <- all_ ( invoice chinookDb ) ln <- all_ ( invoiceLine chinookDb ) pure ( i , ln ) SELECT \"t0\" . \"InvoiceId\" AS \"res0\" , \"t0\" . \"CustomerId\" AS \"res1\" , \"t0\" . \"InvoiceDate\" AS \"res2\" , \"t0\" . \"BillingAddress\" AS \"res3\" , \"t0\" . \"BillingCity\" AS \"res4\" , \"t0\" . \"BillingState\" AS \"res5\" , \"t0\" . \"BillingCountry\" AS \"res6\" , \"t0\" . \"BillingPostalCode\" AS \"res7\" , \"t0\" . \"Total\" AS \"res8\" , \"t1\" . \"InvoiceLineId\" AS \"res9\" , \"t1\" . \"InvoiceId\" AS \"res10\" , \"t1\" . \"TrackId\" AS \"res11\" , \"t1\" . \"UnitPrice\" AS \"res12\" , \"t1\" . \"Quantity\" AS \"res13\" FROM \"Invoice\" AS \"t0\" CROSS JOIN \"InvoiceLine\" AS \"t1\" SELECT \"t0\" . \"InvoiceId\" AS \"res0\" , \"t0\" . \"CustomerId\" AS \"res1\" , \"t0\" . \"InvoiceDate\" AS \"res2\" , \"t0\" . \"BillingAddress\" AS \"res3\" , \"t0\" . \"BillingCity\" AS \"res4\" , \"t0\" . \"BillingState\" AS \"res5\" , \"t0\" . \"BillingCountry\" AS \"res6\" , \"t0\" . \"BillingPostalCode\" AS \"res7\" , \"t0\" . \"Total\" AS \"res8\" , \"t1\" . \"InvoiceLineId\" AS \"res9\" , \"t1\" . \"InvoiceId\" AS \"res10\" , \"t1\" . \"TrackId\" AS \"res11\" , \"t1\" . \"UnitPrice\" AS \"res12\" , \"t1\" . \"Quantity\" AS \"res13\" FROM \"Invoice\" AS \"t0\" INNER JOIN \"InvoiceLine\" AS \"t1\" ; -- With values: [] Of course, most of the time you only want to fetch relevant rows. Going back to the list monad example, suppose we only want to fetch pairs where the second number is less than or equal to twice the first. In Haskell, we'd use the guard function. do a <- [ 1 , 2 , 3 ] b <- [ 4 , 5 , 6 ] guard ( b <= a * 2 ) return ( a , b ) This would return [(2,4),(3,4),(3,5)] . Beam offers a similar function for the Q monad, named guard_ . Note that whereas the Q bind operator is the same as the Haskell monadic bind, the corresponding guard function is not from MonadZero , as it is in Haskell. The technical reason is that the argument to guard_ in Q represents a SQL expression returning a boolean, rather than a Haskell boolean itself. Going back to our invoice line example above, we can fetch every invoice along with only those invoice lines corresponding to that invoice. Haskell Postgres Sqlite do i <- all_ ( invoice chinookDb ) ln <- all_ ( invoiceLine chinookDb ) guard_ ( invoiceLineInvoice ln ` references_ ` i ) pure ( i , ln ) SELECT \"t0\" . \"InvoiceId\" AS \"res0\" , \"t0\" . \"CustomerId\" AS \"res1\" , \"t0\" . \"InvoiceDate\" AS \"res2\" , \"t0\" . \"BillingAddress\" AS \"res3\" , \"t0\" . \"BillingCity\" AS \"res4\" , \"t0\" . \"BillingState\" AS \"res5\" , \"t0\" . \"BillingCountry\" AS \"res6\" , \"t0\" . \"BillingPostalCode\" AS \"res7\" , \"t0\" . \"Total\" AS \"res8\" , \"t1\" . \"InvoiceLineId\" AS \"res9\" , \"t1\" . \"InvoiceId\" AS \"res10\" , \"t1\" . \"TrackId\" AS \"res11\" , \"t1\" . \"UnitPrice\" AS \"res12\" , \"t1\" . \"Quantity\" AS \"res13\" FROM \"Invoice\" AS \"t0\" CROSS JOIN \"InvoiceLine\" AS \"t1\" WHERE ( \"t1\" . \"InvoiceId\" ) = ( \"t0\" . \"InvoiceId\" ) SELECT \"t0\" . \"InvoiceId\" AS \"res0\" , \"t0\" . \"CustomerId\" AS \"res1\" , \"t0\" . \"InvoiceDate\" AS \"res2\" , \"t0\" . \"BillingAddress\" AS \"res3\" , \"t0\" . \"BillingCity\" AS \"res4\" , \"t0\" . \"BillingState\" AS \"res5\" , \"t0\" . \"BillingCountry\" AS \"res6\" , \"t0\" . \"BillingPostalCode\" AS \"res7\" , \"t0\" . \"Total\" AS \"res8\" , \"t1\" . \"InvoiceLineId\" AS \"res9\" , \"t1\" . \"InvoiceId\" AS \"res10\" , \"t1\" . \"TrackId\" AS \"res11\" , \"t1\" . \"UnitPrice\" AS \"res12\" , \"t1\" . \"Quantity\" AS \"res13\" FROM \"Invoice\" AS \"t0\" INNER JOIN \"InvoiceLine\" AS \"t1\" WHERE ( \"t1\" . \"InvoiceId\" ) = ( \"t0\" . \"InvoiceId\" ); -- With values: [] Note that beam has floated the guard_ expression into the WHERE clause, rather than the ON clause, This is fine for most inner joins on most database engines, as the query optimizer will execute both queries similarly. However, some backends are more temperamental, so Beam offers several more idiomatic ways to express joins which more closely reflect the underlying SQL. In practice, most users will use the methods below to express JOINs, but it is nevertheless important to understand that joining is fundamental to the structure of the Q moand. One-to-many Beam supports querying for one-to-many joins. For example, to get every InvoiceLine for each Invoice , use the oneToMany_ combinator. Haskell Postgres Sqlite do i <- all_ ( invoice chinookDb ) ln <- oneToMany_ ( invoiceLine chinookDb ) invoiceLineInvoice i pure ( i , ln ) SELECT \"t0\" . \"InvoiceId\" AS \"res0\" , \"t0\" . \"CustomerId\" AS \"res1\" , \"t0\" . \"InvoiceDate\" AS \"res2\" , \"t0\" . \"BillingAddress\" AS \"res3\" , \"t0\" . \"BillingCity\" AS \"res4\" , \"t0\" . \"BillingState\" AS \"res5\" , \"t0\" . \"BillingCountry\" AS \"res6\" , \"t0\" . \"BillingPostalCode\" AS \"res7\" , \"t0\" . \"Total\" AS \"res8\" , \"t1\" . \"InvoiceLineId\" AS \"res9\" , \"t1\" . \"InvoiceId\" AS \"res10\" , \"t1\" . \"TrackId\" AS \"res11\" , \"t1\" . \"UnitPrice\" AS \"res12\" , \"t1\" . \"Quantity\" AS \"res13\" FROM \"Invoice\" AS \"t0\" INNER JOIN \"InvoiceLine\" AS \"t1\" ON ( \"t1\" . \"InvoiceId\" ) = ( \"t0\" . \"InvoiceId\" ) SELECT \"t0\" . \"InvoiceId\" AS \"res0\" , \"t0\" . \"CustomerId\" AS \"res1\" , \"t0\" . \"InvoiceDate\" AS \"res2\" , \"t0\" . \"BillingAddress\" AS \"res3\" , \"t0\" . \"BillingCity\" AS \"res4\" , \"t0\" . \"BillingState\" AS \"res5\" , \"t0\" . \"BillingCountry\" AS \"res6\" , \"t0\" . \"BillingPostalCode\" AS \"res7\" , \"t0\" . \"Total\" AS \"res8\" , \"t1\" . \"InvoiceLineId\" AS \"res9\" , \"t1\" . \"InvoiceId\" AS \"res10\" , \"t1\" . \"TrackId\" AS \"res11\" , \"t1\" . \"UnitPrice\" AS \"res12\" , \"t1\" . \"Quantity\" AS \"res13\" FROM \"Invoice\" AS \"t0\" INNER JOIN \"InvoiceLine\" AS \"t1\" ON ( \"t1\" . \"InvoiceId\" ) = ( \"t0\" . \"InvoiceId\" ); -- With values: [] Or, if you have an actual Invoice (called oneInvoice ) and you want all the associated InvoiceLine s, you can use val_ to convert oneInvoice to the SQL expression level. oneToMany_ ( invoiceLine chinookDb ) invoiceLineInvoice ( val_ oneInvoice ) If you find yourself repeating yourself constantly, you can define a helper. invoiceLines_ :: OneToMany InvoiceT InvoiceLineT invoiceLines_ = oneToMany_ ( invoiceLine chinookDb ) invoiceLineInvoice Then the above queries become do i <- all_ ( invoice chinookDb ) ln <- invoiceLines_ i and invoiceLines ( val_ i ) Notice that, instead of floating the join condition to the WHERE clause, beam generates an INNER JOIN ... ON expression. These statements are equivalent, although the ON expression is more idiomatic. Nullable columns If you have a nullable foreign key in your many table, you can use oneToManyOptional_ and OneToManyOptional , respectively. For example, One-to-one One to one relationships are a special case of one to many relationships, save for a unique constraint on one column. Thus, there are no special constructs for one-to-one relationships. For convenience, oneToOne_ and OneToOne are equivalent to oneToMany_ and OneToMany . Additionally, oneToMaybe_ and OneToMaybe correspond to oneToManyOptional_ and OneToManyOptional . Many-to-many Many to many relationships require a linking table, with foreign keys to each table part of the relationship. The manyToMany_ construct can be used to fetch both, one, or no sides of a many-to-many relationship. manyToMany_ :: ( Database be db , Table joinThrough , Table left , Table right , Sql92SelectSanityCheck syntax , IsSql92SelectSyntax syntax , SqlEq ( QExpr ( Sql92SelectExpressionSyntax syntax ) s ) ( PrimaryKey left ( QExpr ( Sql92SelectExpressionSyntax syntax ) s )) , SqlEq ( QExpr ( Sql92SelectExpressionSyntax syntax ) s ) ( PrimaryKey right ( QExpr ( Sql92SelectExpressionSyntax syntax ) s )) ) => DatabaseEntity be db ( TableEntity joinThrough ) -> ( joinThrough ( QExpr ( Sql92SelectExpressionSyntax syntax ) s ) -> PrimaryKey left ( QExpr ( Sql92SelectExpressionSyntax syntax ) s )) -> ( joinThrough ( QExpr ( Sql92SelectExpressionSyntax syntax ) s ) -> PrimaryKey right ( QExpr ( Sql92SelectExpressionSyntax syntax ) s )) -> Q syntax db s ( left ( QExpr ( Sql92SelectExpressionSyntax syntax ) s )) -> Q syntax db s ( right ( QExpr ( Sql92SelectExpressionSyntax syntax ) s )) -> Q syntax db s ( left ( QExpr ( Sql92SelectExpressionSyntax syntax ) s ), right ( QExpr ( Sql92SelectExpressionSyntax syntax ) s )) This reads: for any database db ; tables joinThrough , left , and right ; and sane select syntax syntax , where the primary keys of left and right are comparable as value expressions and we have some way of extracting a primary key of left and right from joinThrough , associate all entries of left with those of right through joinThrough and return the results of left and right . The Chinook database associates multiple tracks with a playlist via the playlist_track table. For example, to get all tracks from the playlists named either \"Movies\" or \"Music\". Haskell Postgres Sqlite manyToMany_ ( playlistTrack chinookDb ) playlistTrackPlaylistId playlistTrackTrackId ( filter_ ( \\ p -> playlistName p ==. just_ ( val_ \"Music\" ) ||. playlistName p ==. just_ ( val_ \"Movies\" )) ( all_ ( playlist chinookDb ))) ( all_ ( track chinookDb )) SELECT \"t0\" . \"PlaylistId\" AS \"res0\" , \"t0\" . \"Name\" AS \"res1\" , \"t1\" . \"TrackId\" AS \"res2\" , \"t1\" . \"Name\" AS \"res3\" , \"t1\" . \"AlbumId\" AS \"res4\" , \"t1\" . \"MediaTypeId\" AS \"res5\" , \"t1\" . \"GenreId\" AS \"res6\" , \"t1\" . \"Composer\" AS \"res7\" , \"t1\" . \"Milliseconds\" AS \"res8\" , \"t1\" . \"Bytes\" AS \"res9\" , \"t1\" . \"UnitPrice\" AS \"res10\" FROM \"Playlist\" AS \"t0\" CROSS JOIN \"Track\" AS \"t1\" INNER JOIN \"PlaylistTrack\" AS \"t2\" ON (( \"t2\" . \"PlaylistId\" ) = ( \"t0\" . \"PlaylistId\" )) AND (( \"t2\" . \"TrackId\" ) = ( \"t1\" . \"TrackId\" )) WHERE (( \"t0\" . \"Name\" ) IS NOT DISTINCT FROM ( 'Music' )) OR (( \"t0\" . \"Name\" ) IS NOT DISTINCT FROM ( 'Movies' )) SELECT \"t0\" . \"PlaylistId\" AS \"res0\" , \"t0\" . \"Name\" AS \"res1\" , \"t1\" . \"TrackId\" AS \"res2\" , \"t1\" . \"Name\" AS \"res3\" , \"t1\" . \"AlbumId\" AS \"res4\" , \"t1\" . \"MediaTypeId\" AS \"res5\" , \"t1\" . \"GenreId\" AS \"res6\" , \"t1\" . \"Composer\" AS \"res7\" , \"t1\" . \"Milliseconds\" AS \"res8\" , \"t1\" . \"Bytes\" AS \"res9\" , \"t1\" . \"UnitPrice\" AS \"res10\" FROM \"Playlist\" AS \"t0\" INNER JOIN \"Track\" AS \"t1\" INNER JOIN \"PlaylistTrack\" AS \"t2\" ON (( \"t2\" . \"PlaylistId\" ) = ( \"t0\" . \"PlaylistId\" )) AND (( \"t2\" . \"TrackId\" ) = ( \"t1\" . \"TrackId\" )) WHERE ( CASE WHEN (( \"t0\" . \"Name\" ) IS NULL ) AND (( ? ) IS NULL ) THEN ? WHEN (( \"t0\" . \"Name\" ) IS NULL ) OR (( ? ) IS NULL ) THEN ? ELSE ( \"t0\" . \"Name\" ) = ( ? ) END ) OR ( CASE WHEN (( \"t0\" . \"Name\" ) IS NULL ) AND (( ? ) IS NULL ) THEN ? WHEN (( \"t0\" . \"Name\" ) IS NULL ) OR (( ? ) IS NULL ) THEN ? ELSE ( \"t0\" . \"Name\" ) = ( ? ) END ); -- With values: [SQLText \"Music\",SQLInteger 1,SQLText \"Music\",SQLInteger 0,SQLText \"Music\",SQLText \"Movies\",SQLInteger 1,SQLText \"Movies\",SQLInteger 0,SQLText \"Movies\"] Many-to-many with arbitrary data Sometimes you want to have additional data for each relationship. For this, use manyToManyPassthrough_ . manyToManyPassthrough_ :: ( Database be db , Table joinThrough , Table left , Table right , Sql92SelectSanityCheck syntax , IsSql92SelectSyntax syntax , SqlEq ( QExpr ( Sql92SelectExpressionSyntax syntax ) s ) ( PrimaryKey left ( QExpr ( Sql92SelectExpressionSyntax syntax ) s )) , SqlEq ( QExpr ( Sql92SelectExpressionSyntax syntax ) s ) ( PrimaryKey right ( QExpr ( Sql92SelectExpressionSyntax syntax ) s )) ) => DatabaseEntity be db ( TableEntity joinThrough ) -> ( joinThrough ( QExpr ( Sql92SelectExpressionSyntax syntax ) s ) -> PrimaryKey left ( QExpr ( Sql92SelectExpressionSyntax syntax ) s )) -> ( joinThrough ( QExpr ( Sql92SelectExpressionSyntax syntax ) s ) -> PrimaryKey right ( QExpr ( Sql92SelectExpressionSyntax syntax ) s )) -> Q syntax db s ( left ( QExpr ( Sql92SelectExpressionSyntax syntax ) s )) -> Q syntax db s ( right ( QExpr ( Sql92SelectExpressionSyntax syntax ) s )) -> Q syntax db s ( joinThrough ( QExpr ( Sql92SelectExpressionSyntax syntax ) s ) , left ( QExpr ( Sql92SelectExpressionSyntax syntax ) s ) , right ( QExpr ( Sql92SelectExpressionSyntax syntax ) s )) Under the hood manyToMany_ is defined simply as manyToMany_ = fmap ( \\ ( _ , left , right ) -> ( left , right )) manyToManyPassthrough_ Declaring many-to-many relationships Like one-to-many relationships, beam allows you to extract commonly used many-to-many relationships, via the ManyToMany type. For example, the playlist/track relationship above can be defined as follows playlistTrackRelationship :: ManyToMany ChinookDb PlaylistT TrackT playlistTrackRelationship = manyToMany_ ( playlistTrack chinookDb ) playlistTrackPlaylistId playlistTrackTrackId And we can use it as expected: Haskell Postgres Sqlite playlistTrackRelationship ( filter_ ( \\ p -> playlistName p ==. just_ ( val_ \"Music\" ) ||. playlistName p ==. just_ ( val_ \"Movies\" )) ( all_ ( playlist chinookDb ))) ( all_ ( track chinookDb )) SELECT \"t0\" . \"PlaylistId\" AS \"res0\" , \"t0\" . \"Name\" AS \"res1\" , \"t1\" . \"TrackId\" AS \"res2\" , \"t1\" . \"Name\" AS \"res3\" , \"t1\" . \"AlbumId\" AS \"res4\" , \"t1\" . \"MediaTypeId\" AS \"res5\" , \"t1\" . \"GenreId\" AS \"res6\" , \"t1\" . \"Composer\" AS \"res7\" , \"t1\" . \"Milliseconds\" AS \"res8\" , \"t1\" . \"Bytes\" AS \"res9\" , \"t1\" . \"UnitPrice\" AS \"res10\" FROM \"Playlist\" AS \"t0\" CROSS JOIN \"Track\" AS \"t1\" INNER JOIN \"PlaylistTrack\" AS \"t2\" ON (( \"t2\" . \"PlaylistId\" ) = ( \"t0\" . \"PlaylistId\" )) AND (( \"t2\" . \"TrackId\" ) = ( \"t1\" . \"TrackId\" )) WHERE (( \"t0\" . \"Name\" ) IS NOT DISTINCT FROM ( 'Music' )) OR (( \"t0\" . \"Name\" ) IS NOT DISTINCT FROM ( 'Movies' )) SELECT \"t0\" . \"PlaylistId\" AS \"res0\" , \"t0\" . \"Name\" AS \"res1\" , \"t1\" . \"TrackId\" AS \"res2\" , \"t1\" . \"Name\" AS \"res3\" , \"t1\" . \"AlbumId\" AS \"res4\" , \"t1\" . \"MediaTypeId\" AS \"res5\" , \"t1\" . \"GenreId\" AS \"res6\" , \"t1\" . \"Composer\" AS \"res7\" , \"t1\" . \"Milliseconds\" AS \"res8\" , \"t1\" . \"Bytes\" AS \"res9\" , \"t1\" . \"UnitPrice\" AS \"res10\" FROM \"Playlist\" AS \"t0\" INNER JOIN \"Track\" AS \"t1\" INNER JOIN \"PlaylistTrack\" AS \"t2\" ON (( \"t2\" . \"PlaylistId\" ) = ( \"t0\" . \"PlaylistId\" )) AND (( \"t2\" . \"TrackId\" ) = ( \"t1\" . \"TrackId\" )) WHERE ( CASE WHEN (( \"t0\" . \"Name\" ) IS NULL ) AND (( ? ) IS NULL ) THEN ? WHEN (( \"t0\" . \"Name\" ) IS NULL ) OR (( ? ) IS NULL ) THEN ? ELSE ( \"t0\" . \"Name\" ) = ( ? ) END ) OR ( CASE WHEN (( \"t0\" . \"Name\" ) IS NULL ) AND (( ? ) IS NULL ) THEN ? WHEN (( \"t0\" . \"Name\" ) IS NULL ) OR (( ? ) IS NULL ) THEN ? ELSE ( \"t0\" . \"Name\" ) = ( ? ) END ); -- With values: [SQLText \"Music\",SQLInteger 1,SQLText \"Music\",SQLInteger 0,SQLText \"Music\",SQLText \"Movies\",SQLInteger 1,SQLText \"Movies\",SQLInteger 0,SQLText \"Movies\"] ManyToManyThrough is the equivalent for manyToManyThrough_ , except it takes another table parameter for the 'through' table. Arbitrary Joins Joins with arbitrary conditions can be specified using the join_ construct. For example, oneToMany_ is implemented as oneToMany_ rel getKey tbl = join_ rel ( \\ rel -> getKey rel ==. pk tbl ) Thus, the invoice example above could be rewritten. For example, instead of do i <- all_ ( invoice chinookDb ) ln <- oneToMany_ ( invoiceLine chinookDb ) invoiceLineInvoice i pure ( i , ln ) We could write Haskell Postgres Sqlite do i <- all_ ( invoice chinookDb ) ln <- join_ ( invoiceLine chinookDb ) ( \\ line -> invoiceLineInvoice line ==. primaryKey i ) pure ( i , ln ) SELECT \"t0\" . \"InvoiceId\" AS \"res0\" , \"t0\" . \"CustomerId\" AS \"res1\" , \"t0\" . \"InvoiceDate\" AS \"res2\" , \"t0\" . \"BillingAddress\" AS \"res3\" , \"t0\" . \"BillingCity\" AS \"res4\" , \"t0\" . \"BillingState\" AS \"res5\" , \"t0\" . \"BillingCountry\" AS \"res6\" , \"t0\" . \"BillingPostalCode\" AS \"res7\" , \"t0\" . \"Total\" AS \"res8\" , \"t1\" . \"InvoiceLineId\" AS \"res9\" , \"t1\" . \"InvoiceId\" AS \"res10\" , \"t1\" . \"TrackId\" AS \"res11\" , \"t1\" . \"UnitPrice\" AS \"res12\" , \"t1\" . \"Quantity\" AS \"res13\" FROM \"Invoice\" AS \"t0\" INNER JOIN \"InvoiceLine\" AS \"t1\" ON ( \"t1\" . \"InvoiceId\" ) = ( \"t0\" . \"InvoiceId\" ) SELECT \"t0\" . \"InvoiceId\" AS \"res0\" , \"t0\" . \"CustomerId\" AS \"res1\" , \"t0\" . \"InvoiceDate\" AS \"res2\" , \"t0\" . \"BillingAddress\" AS \"res3\" , \"t0\" . \"BillingCity\" AS \"res4\" , \"t0\" . \"BillingState\" AS \"res5\" , \"t0\" . \"BillingCountry\" AS \"res6\" , \"t0\" . \"BillingPostalCode\" AS \"res7\" , \"t0\" . \"Total\" AS \"res8\" , \"t1\" . \"InvoiceLineId\" AS \"res9\" , \"t1\" . \"InvoiceId\" AS \"res10\" , \"t1\" . \"TrackId\" AS \"res11\" , \"t1\" . \"UnitPrice\" AS \"res12\" , \"t1\" . \"Quantity\" AS \"res13\" FROM \"Invoice\" AS \"t0\" INNER JOIN \"InvoiceLine\" AS \"t1\" ON ( \"t1\" . \"InvoiceId\" ) = ( \"t0\" . \"InvoiceId\" ); -- With values: [] Outer joins Left and right joins Left joins with arbitrary conditions can be specified with the leftJoin_ construct. leftJoin_ takes an arbitrary query and a join condition. It associates each result record with a record of the table given or a fully NULL row of that table in case no row matches. For this reason, the result of leftJoin_ has an extra Nullable column tag, which converts each field into the corresponding Maybe type. Note The table parameter passed in as the join condition does not have a Nullable column tag. The join condition should be written as if a concrete row from that table exists. For example, to get every artist along with their albums, but always including every artist, use leftJoin_ as follows. Haskell Postgres Sqlite do artist <- all_ ( artist chinookDb ) album <- leftJoin_ ( all_ ( album chinookDb )) ( \\ album -> albumArtist album ==. primaryKey artist ) pure ( artist , album ) SELECT \"t0\" . \"ArtistId\" AS \"res0\" , \"t0\" . \"Name\" AS \"res1\" , \"t1\" . \"AlbumId\" AS \"res2\" , \"t1\" . \"Title\" AS \"res3\" , \"t1\" . \"ArtistId\" AS \"res4\" FROM \"Artist\" AS \"t0\" LEFT JOIN \"Album\" AS \"t1\" ON ( \"t1\" . \"ArtistId\" ) = ( \"t0\" . \"ArtistId\" ) SELECT \"t0\" . \"ArtistId\" AS \"res0\" , \"t0\" . \"Name\" AS \"res1\" , \"t1\" . \"AlbumId\" AS \"res2\" , \"t1\" . \"Title\" AS \"res3\" , \"t1\" . \"ArtistId\" AS \"res4\" FROM \"Artist\" AS \"t0\" LEFT JOIN \"Album\" AS \"t1\" ON ( \"t1\" . \"ArtistId\" ) = ( \"t0\" . \"ArtistId\" ); -- With values: [] Right joins are not yet supported. They can always be rewritten as left joins. If you have a compelling use case, please file an issue! Handling SQL NULL s NULL is a value that SQL treats as an 'unknown' value. Unfortunately, this can cause a lot of unexpected issues. Beam tries to normalize the handling of NULLs to some extent, but it ultimately cannot save you from the database. One thing you can be sure of is that -- assuming your beam schema matches that of the database -- any beam expression that does not yield a Maybe type cannot be NULL at run-time. Also, beam treats equality between Maybe types correctly using the standard ==. and /=. operators. This means that beam will sometimes generate obtuse CASE expressions. This is because beam's philosophy is that SQL operators be named after their equivalent Haskell ones, suffixed by a . , and that these operators should follow Haskell semantics. Sometimes though, this care isn't necessary. When you are okay with SQL equality, you can use the (==?.) and (/=?.) operators. These work the same as the (==.) and (/=.) , except they return a SqlBool instead of Bool . SqlBool can only occur as the result of a SQL expression, and it cannot be deserialized directly into Haskell on any backend. A SqlBool value can contain TRUE , FALSE , and UNKNOWN (the third SQL boolean value). You can marshal between SqlBool and Bool using isTrue_ , isFalse_ , or isUnknown_ to determine which value a SqlBool contains. The unknownAs_ function takes a default Haskell Bool and SQL expression returning SqlBool . It returns the given Haskell Bool value in the case the SQL expression is indeterminate. You can also convert any expression returning Bool to one returning SqlBool by using the sqlBool_ function. The various beam functions that deal with Bool also have corresponding versions that operate on SqlBool . For example, whereas leftJoin_ expects its join condition to be a Bool , the corresponding leftJoin_' (notice the prime) method takes a SqlBool . There are corresponding guard_' , join_' , etc methods. Boolean operators, such as (&&.) and (||.) , have SqlBool equivalents suffixed with ? ( (&&?.) and (||?.) for SqlBool AND and OR respectively). One place where this can really bite is when generating ON conditions. Many RDBMSes use a rather unintelligent means of choosing which indices to use, by directly matching on syntaxes. For example, postgres determines index usage by directly seeing if two columns are compared. If you wrap the comparison in the IS TRUE operator, the index is no longer used. In these cases, using the proper boolean handling can severely impact performance. For example, to get every customer along with employees in their area, we can left join the customer table with employees on their city. Haskell Postgres Sqlite do c <- all_ ( customer chinookDb ) e <- leftJoin_ ( all_ ( employee chinookDb )) ( \\ e -> addressCity ( employeeAddress e ) ==. addressCity ( customerAddress c )) pure ( c , e ) SELECT \"t0\" . \"CustomerId\" AS \"res0\" , \"t0\" . \"FirstName\" AS \"res1\" , \"t0\" . \"LastName\" AS \"res2\" , \"t0\" . \"Company\" AS \"res3\" , \"t0\" . \"Address\" AS \"res4\" , \"t0\" . \"City\" AS \"res5\" , \"t0\" . \"State\" AS \"res6\" , \"t0\" . \"Country\" AS \"res7\" , \"t0\" . \"PostalCode\" AS \"res8\" , \"t0\" . \"Phone\" AS \"res9\" , \"t0\" . \"Fax\" AS \"res10\" , \"t0\" . \"Email\" AS \"res11\" , \"t0\" . \"SupportRepId\" AS \"res12\" , \"t1\" . \"EmployeeId\" AS \"res13\" , \"t1\" . \"LastName\" AS \"res14\" , \"t1\" . \"FirstName\" AS \"res15\" , \"t1\" . \"Title\" AS \"res16\" , \"t1\" . \"ReportsTo\" AS \"res17\" , \"t1\" . \"BirthDate\" AS \"res18\" , \"t1\" . \"HireDate\" AS \"res19\" , \"t1\" . \"Address\" AS \"res20\" , \"t1\" . \"City\" AS \"res21\" , \"t1\" . \"State\" AS \"res22\" , \"t1\" . \"Country\" AS \"res23\" , \"t1\" . \"PostalCode\" AS \"res24\" , \"t1\" . \"Phone\" AS \"res25\" , \"t1\" . \"Fax\" AS \"res26\" , \"t1\" . \"Email\" AS \"res27\" FROM \"Customer\" AS \"t0\" LEFT JOIN \"Employee\" AS \"t1\" ON ( \"t1\" . \"City\" ) IS NOT DISTINCT FROM ( \"t0\" . \"City\" ) SELECT \"t0\" . \"CustomerId\" AS \"res0\" , \"t0\" . \"FirstName\" AS \"res1\" , \"t0\" . \"LastName\" AS \"res2\" , \"t0\" . \"Company\" AS \"res3\" , \"t0\" . \"Address\" AS \"res4\" , \"t0\" . \"City\" AS \"res5\" , \"t0\" . \"State\" AS \"res6\" , \"t0\" . \"Country\" AS \"res7\" , \"t0\" . \"PostalCode\" AS \"res8\" , \"t0\" . \"Phone\" AS \"res9\" , \"t0\" . \"Fax\" AS \"res10\" , \"t0\" . \"Email\" AS \"res11\" , \"t0\" . \"SupportRepId\" AS \"res12\" , \"t1\" . \"EmployeeId\" AS \"res13\" , \"t1\" . \"LastName\" AS \"res14\" , \"t1\" . \"FirstName\" AS \"res15\" , \"t1\" . \"Title\" AS \"res16\" , \"t1\" . \"ReportsTo\" AS \"res17\" , \"t1\" . \"BirthDate\" AS \"res18\" , \"t1\" . \"HireDate\" AS \"res19\" , \"t1\" . \"Address\" AS \"res20\" , \"t1\" . \"City\" AS \"res21\" , \"t1\" . \"State\" AS \"res22\" , \"t1\" . \"Country\" AS \"res23\" , \"t1\" . \"PostalCode\" AS \"res24\" , \"t1\" . \"Phone\" AS \"res25\" , \"t1\" . \"Fax\" AS \"res26\" , \"t1\" . \"Email\" AS \"res27\" FROM \"Customer\" AS \"t0\" LEFT JOIN \"Employee\" AS \"t1\" ON CASE WHEN (( \"t1\" . \"City\" ) IS NULL ) AND (( \"t0\" . \"City\" ) IS NULL ) THEN ? WHEN (( \"t1\" . \"City\" ) IS NULL ) OR (( \"t0\" . \"City\" ) IS NULL ) THEN ? ELSE ( \"t1\" . \"City\" ) = ( \"t0\" . \"City\" ) END ; -- With values: [SQLInteger 1,SQLInteger 0] Notice that the join condition is not just a simple = . This will cause postgres to ignore any index on these columns. We can instead use leftJoin_' and ==?. to be more direct. Haskell Postgres Sqlite do c <- all_ ( customer chinookDb ) e <- leftJoin_' ( all_ ( employee chinookDb )) ( \\ e -> addressCity ( employeeAddress e ) ==?. addressCity ( customerAddress c )) pure ( c , e ) SELECT \"t0\" . \"CustomerId\" AS \"res0\" , \"t0\" . \"FirstName\" AS \"res1\" , \"t0\" . \"LastName\" AS \"res2\" , \"t0\" . \"Company\" AS \"res3\" , \"t0\" . \"Address\" AS \"res4\" , \"t0\" . \"City\" AS \"res5\" , \"t0\" . \"State\" AS \"res6\" , \"t0\" . \"Country\" AS \"res7\" , \"t0\" . \"PostalCode\" AS \"res8\" , \"t0\" . \"Phone\" AS \"res9\" , \"t0\" . \"Fax\" AS \"res10\" , \"t0\" . \"Email\" AS \"res11\" , \"t0\" . \"SupportRepId\" AS \"res12\" , \"t1\" . \"EmployeeId\" AS \"res13\" , \"t1\" . \"LastName\" AS \"res14\" , \"t1\" . \"FirstName\" AS \"res15\" , \"t1\" . \"Title\" AS \"res16\" , \"t1\" . \"ReportsTo\" AS \"res17\" , \"t1\" . \"BirthDate\" AS \"res18\" , \"t1\" . \"HireDate\" AS \"res19\" , \"t1\" . \"Address\" AS \"res20\" , \"t1\" . \"City\" AS \"res21\" , \"t1\" . \"State\" AS \"res22\" , \"t1\" . \"Country\" AS \"res23\" , \"t1\" . \"PostalCode\" AS \"res24\" , \"t1\" . \"Phone\" AS \"res25\" , \"t1\" . \"Fax\" AS \"res26\" , \"t1\" . \"Email\" AS \"res27\" FROM \"Customer\" AS \"t0\" LEFT JOIN \"Employee\" AS \"t1\" ON ( \"t1\" . \"City\" ) = ( \"t0\" . \"City\" ) SELECT \"t0\" . \"CustomerId\" AS \"res0\" , \"t0\" . \"FirstName\" AS \"res1\" , \"t0\" . \"LastName\" AS \"res2\" , \"t0\" . \"Company\" AS \"res3\" , \"t0\" . \"Address\" AS \"res4\" , \"t0\" . \"City\" AS \"res5\" , \"t0\" . \"State\" AS \"res6\" , \"t0\" . \"Country\" AS \"res7\" , \"t0\" . \"PostalCode\" AS \"res8\" , \"t0\" . \"Phone\" AS \"res9\" , \"t0\" . \"Fax\" AS \"res10\" , \"t0\" . \"Email\" AS \"res11\" , \"t0\" . \"SupportRepId\" AS \"res12\" , \"t1\" . \"EmployeeId\" AS \"res13\" , \"t1\" . \"LastName\" AS \"res14\" , \"t1\" . \"FirstName\" AS \"res15\" , \"t1\" . \"Title\" AS \"res16\" , \"t1\" . \"ReportsTo\" AS \"res17\" , \"t1\" . \"BirthDate\" AS \"res18\" , \"t1\" . \"HireDate\" AS \"res19\" , \"t1\" . \"Address\" AS \"res20\" , \"t1\" . \"City\" AS \"res21\" , \"t1\" . \"State\" AS \"res22\" , \"t1\" . \"Country\" AS \"res23\" , \"t1\" . \"PostalCode\" AS \"res24\" , \"t1\" . \"Phone\" AS \"res25\" , \"t1\" . \"Fax\" AS \"res26\" , \"t1\" . \"Email\" AS \"res27\" FROM \"Customer\" AS \"t0\" LEFT JOIN \"Employee\" AS \"t1\" ON ( \"t1\" . \"City\" ) = ( \"t0\" . \"City\" ); -- With values: [] Now postgres will use an index. Full Outer joins Outer joins are supported with the outerJoin_ function. outerJoin_ takes two queries and a join condition and returns a Q that represents the FULL OUTER JOIN of the two queries. Because either table may be nullable, the output of the result has an additional Nullable tag. NOTE Outer joins are only supported in backends whose SQL FROM syntax implements IsSql92FromOuterJoinSyntax . Notably, this does not include SQLite. For example, to get join all employees with customers with the same first name but including all employees and customers, we can run the query Haskell Postgres outerJoin_ ( all_ ( employee chinookDb )) ( all_ ( customer chinookDb )) ( \\ ( employee , customer ) -> employeeFirstName employee ==. customerFirstName customer ) SELECT \"t0\" . \"EmployeeId\" AS \"res0\" , \"t0\" . \"LastName\" AS \"res1\" , \"t0\" . \"FirstName\" AS \"res2\" , \"t0\" . \"Title\" AS \"res3\" , \"t0\" . \"ReportsTo\" AS \"res4\" , \"t0\" . \"BirthDate\" AS \"res5\" , \"t0\" . \"HireDate\" AS \"res6\" , \"t0\" . \"Address\" AS \"res7\" , \"t0\" . \"City\" AS \"res8\" , \"t0\" . \"State\" AS \"res9\" , \"t0\" . \"Country\" AS \"res10\" , \"t0\" . \"PostalCode\" AS \"res11\" , \"t0\" . \"Phone\" AS \"res12\" , \"t0\" . \"Fax\" AS \"res13\" , \"t0\" . \"Email\" AS \"res14\" , \"t1\" . \"CustomerId\" AS \"res15\" , \"t1\" . \"FirstName\" AS \"res16\" , \"t1\" . \"LastName\" AS \"res17\" , \"t1\" . \"Company\" AS \"res18\" , \"t1\" . \"Address\" AS \"res19\" , \"t1\" . \"City\" AS \"res20\" , \"t1\" . \"State\" AS \"res21\" , \"t1\" . \"Country\" AS \"res22\" , \"t1\" . \"PostalCode\" AS \"res23\" , \"t1\" . \"Phone\" AS \"res24\" , \"t1\" . \"Fax\" AS \"res25\" , \"t1\" . \"Email\" AS \"res26\" , \"t1\" . \"SupportRepId\" AS \"res27\" FROM \"Employee\" AS \"t0\" FULL OUTER JOIN \"Customer\" AS \"t1\" ON ( \"t0\" . \"FirstName\" ) = ( \"t1\" . \"FirstName\" ) Subqueries Sometimes you want to join against a subquery rather than a table. For the most part, beam will automatically figure out when certain queries need to be written using subqueries. For example, to join two result sets cointaining a SQL LIMIT, you would normally have to write both queries as subqueries. In beam, you can write such queries as you'd expect. The library takes care of creating subqueries as expected. For example, the following query generates the code you'd expect. Haskell Postgres Sqlite do i <- limit_ 10 $ all_ ( invoice chinookDb ) line <- invoiceLines i pure ( i , line ) SELECT \"t0\" . \"res0\" AS \"res0\" , \"t0\" . \"res1\" AS \"res1\" , \"t0\" . \"res2\" AS \"res2\" , \"t0\" . \"res3\" AS \"res3\" , \"t0\" . \"res4\" AS \"res4\" , \"t0\" . \"res5\" AS \"res5\" , \"t0\" . \"res6\" AS \"res6\" , \"t0\" . \"res7\" AS \"res7\" , \"t0\" . \"res8\" AS \"res8\" , \"t1\" . \"InvoiceLineId\" AS \"res9\" , \"t1\" . \"InvoiceId\" AS \"res10\" , \"t1\" . \"TrackId\" AS \"res11\" , \"t1\" . \"UnitPrice\" AS \"res12\" , \"t1\" . \"Quantity\" AS \"res13\" FROM ( SELECT \"t0\" . \"InvoiceId\" AS \"res0\" , \"t0\" . \"CustomerId\" AS \"res1\" , \"t0\" . \"InvoiceDate\" AS \"res2\" , \"t0\" . \"BillingAddress\" AS \"res3\" , \"t0\" . \"BillingCity\" AS \"res4\" , \"t0\" . \"BillingState\" AS \"res5\" , \"t0\" . \"BillingCountry\" AS \"res6\" , \"t0\" . \"BillingPostalCode\" AS \"res7\" , \"t0\" . \"Total\" AS \"res8\" FROM \"Invoice\" AS \"t0\" LIMIT 10 ) AS \"t0\" INNER JOIN \"InvoiceLine\" AS \"t1\" ON ( \"t1\" . \"InvoiceId\" ) = ( \"t0\" . \"res0\" ) SELECT \"t0\" . \"res0\" AS \"res0\" , \"t0\" . \"res1\" AS \"res1\" , \"t0\" . \"res2\" AS \"res2\" , \"t0\" . \"res3\" AS \"res3\" , \"t0\" . \"res4\" AS \"res4\" , \"t0\" . \"res5\" AS \"res5\" , \"t0\" . \"res6\" AS \"res6\" , \"t0\" . \"res7\" AS \"res7\" , \"t0\" . \"res8\" AS \"res8\" , \"t1\" . \"InvoiceLineId\" AS \"res9\" , \"t1\" . \"InvoiceId\" AS \"res10\" , \"t1\" . \"TrackId\" AS \"res11\" , \"t1\" . \"UnitPrice\" AS \"res12\" , \"t1\" . \"Quantity\" AS \"res13\" FROM ( SELECT \"t0\" . \"InvoiceId\" AS \"res0\" , \"t0\" . \"CustomerId\" AS \"res1\" , \"t0\" . \"InvoiceDate\" AS \"res2\" , \"t0\" . \"BillingAddress\" AS \"res3\" , \"t0\" . \"BillingCity\" AS \"res4\" , \"t0\" . \"BillingState\" AS \"res5\" , \"t0\" . \"BillingCountry\" AS \"res6\" , \"t0\" . \"BillingPostalCode\" AS \"res7\" , \"t0\" . \"Total\" AS \"res8\" FROM \"Invoice\" AS \"t0\" LIMIT 10 ) AS \"t0\" INNER JOIN \"InvoiceLine\" AS \"t1\" ON ( \"t1\" . \"InvoiceId\" ) = ( \"t0\" . \"res0\" ); -- With values: [] If you need to (for efficiency for example), you can also generate subqueries explicitly, using subselect_ . The subselect_ will force a new query to be output in most cases. For simple queries, such as all_ , subselect_ will have no effect. Haskell Postgres Sqlite -- Same as above, but with explicit sub select do i <- subselect_ $ limit_ 10 $ all_ ( invoice chinookDb ) line <- invoiceLines i pure ( i , line ) SELECT \"t0\" . \"res0\" AS \"res0\" , \"t0\" . \"res1\" AS \"res1\" , \"t0\" . \"res2\" AS \"res2\" , \"t0\" . \"res3\" AS \"res3\" , \"t0\" . \"res4\" AS \"res4\" , \"t0\" . \"res5\" AS \"res5\" , \"t0\" . \"res6\" AS \"res6\" , \"t0\" . \"res7\" AS \"res7\" , \"t0\" . \"res8\" AS \"res8\" , \"t1\" . \"InvoiceLineId\" AS \"res9\" , \"t1\" . \"InvoiceId\" AS \"res10\" , \"t1\" . \"TrackId\" AS \"res11\" , \"t1\" . \"UnitPrice\" AS \"res12\" , \"t1\" . \"Quantity\" AS \"res13\" FROM ( SELECT \"t0\" . \"InvoiceId\" AS \"res0\" , \"t0\" . \"CustomerId\" AS \"res1\" , \"t0\" . \"InvoiceDate\" AS \"res2\" , \"t0\" . \"BillingAddress\" AS \"res3\" , \"t0\" . \"BillingCity\" AS \"res4\" , \"t0\" . \"BillingState\" AS \"res5\" , \"t0\" . \"BillingCountry\" AS \"res6\" , \"t0\" . \"BillingPostalCode\" AS \"res7\" , \"t0\" . \"Total\" AS \"res8\" FROM \"Invoice\" AS \"t0\" LIMIT 10 ) AS \"t0\" INNER JOIN \"InvoiceLine\" AS \"t1\" ON ( \"t1\" . \"InvoiceId\" ) = ( \"t0\" . \"res0\" ) SELECT \"t0\" . \"res0\" AS \"res0\" , \"t0\" . \"res1\" AS \"res1\" , \"t0\" . \"res2\" AS \"res2\" , \"t0\" . \"res3\" AS \"res3\" , \"t0\" . \"res4\" AS \"res4\" , \"t0\" . \"res5\" AS \"res5\" , \"t0\" . \"res6\" AS \"res6\" , \"t0\" . \"res7\" AS \"res7\" , \"t0\" . \"res8\" AS \"res8\" , \"t1\" . \"InvoiceLineId\" AS \"res9\" , \"t1\" . \"InvoiceId\" AS \"res10\" , \"t1\" . \"TrackId\" AS \"res11\" , \"t1\" . \"UnitPrice\" AS \"res12\" , \"t1\" . \"Quantity\" AS \"res13\" FROM ( SELECT \"t0\" . \"InvoiceId\" AS \"res0\" , \"t0\" . \"CustomerId\" AS \"res1\" , \"t0\" . \"InvoiceDate\" AS \"res2\" , \"t0\" . \"BillingAddress\" AS \"res3\" , \"t0\" . \"BillingCity\" AS \"res4\" , \"t0\" . \"BillingState\" AS \"res5\" , \"t0\" . \"BillingCountry\" AS \"res6\" , \"t0\" . \"BillingPostalCode\" AS \"res7\" , \"t0\" . \"Total\" AS \"res8\" FROM \"Invoice\" AS \"t0\" LIMIT 10 ) AS \"t0\" INNER JOIN \"InvoiceLine\" AS \"t1\" ON ( \"t1\" . \"InvoiceId\" ) = ( \"t0\" . \"res0\" ); -- With values: []","title":"Relationships"},{"location":"user-guide/queries/relationships/#full-inner-joins","text":"Recall that the Q type is a monad. In many respects, Q operates like the list monad. For those unfamiliar, the monadic bind operator for [] is defined as concatMap . Thus, do a <- [ 1 , 2 , 3 ] b <- [ 4 , 5 , 6 ] return ( a , b ) is equivalent to [(1,4),(1,5),(1,6),(2,4),(2,5),(2,6),(3,4),(3,5),(3,6)] . This operation is similar to the cartesian product from set theory or the inner join from relational algebra. The Q monad fully supports this notion of join, and in fact, every other join is built off of this primitive. For example, to get every row from the invoice table and every row from the invoice line table, with no attention paid to any relationship between the two: Haskell Postgres Sqlite do i <- all_ ( invoice chinookDb ) ln <- all_ ( invoiceLine chinookDb ) pure ( i , ln ) SELECT \"t0\" . \"InvoiceId\" AS \"res0\" , \"t0\" . \"CustomerId\" AS \"res1\" , \"t0\" . \"InvoiceDate\" AS \"res2\" , \"t0\" . \"BillingAddress\" AS \"res3\" , \"t0\" . \"BillingCity\" AS \"res4\" , \"t0\" . \"BillingState\" AS \"res5\" , \"t0\" . \"BillingCountry\" AS \"res6\" , \"t0\" . \"BillingPostalCode\" AS \"res7\" , \"t0\" . \"Total\" AS \"res8\" , \"t1\" . \"InvoiceLineId\" AS \"res9\" , \"t1\" . \"InvoiceId\" AS \"res10\" , \"t1\" . \"TrackId\" AS \"res11\" , \"t1\" . \"UnitPrice\" AS \"res12\" , \"t1\" . \"Quantity\" AS \"res13\" FROM \"Invoice\" AS \"t0\" CROSS JOIN \"InvoiceLine\" AS \"t1\" SELECT \"t0\" . \"InvoiceId\" AS \"res0\" , \"t0\" . \"CustomerId\" AS \"res1\" , \"t0\" . \"InvoiceDate\" AS \"res2\" , \"t0\" . \"BillingAddress\" AS \"res3\" , \"t0\" . \"BillingCity\" AS \"res4\" , \"t0\" . \"BillingState\" AS \"res5\" , \"t0\" . \"BillingCountry\" AS \"res6\" , \"t0\" . \"BillingPostalCode\" AS \"res7\" , \"t0\" . \"Total\" AS \"res8\" , \"t1\" . \"InvoiceLineId\" AS \"res9\" , \"t1\" . \"InvoiceId\" AS \"res10\" , \"t1\" . \"TrackId\" AS \"res11\" , \"t1\" . \"UnitPrice\" AS \"res12\" , \"t1\" . \"Quantity\" AS \"res13\" FROM \"Invoice\" AS \"t0\" INNER JOIN \"InvoiceLine\" AS \"t1\" ; -- With values: [] Of course, most of the time you only want to fetch relevant rows. Going back to the list monad example, suppose we only want to fetch pairs where the second number is less than or equal to twice the first. In Haskell, we'd use the guard function. do a <- [ 1 , 2 , 3 ] b <- [ 4 , 5 , 6 ] guard ( b <= a * 2 ) return ( a , b ) This would return [(2,4),(3,4),(3,5)] . Beam offers a similar function for the Q monad, named guard_ . Note that whereas the Q bind operator is the same as the Haskell monadic bind, the corresponding guard function is not from MonadZero , as it is in Haskell. The technical reason is that the argument to guard_ in Q represents a SQL expression returning a boolean, rather than a Haskell boolean itself. Going back to our invoice line example above, we can fetch every invoice along with only those invoice lines corresponding to that invoice. Haskell Postgres Sqlite do i <- all_ ( invoice chinookDb ) ln <- all_ ( invoiceLine chinookDb ) guard_ ( invoiceLineInvoice ln ` references_ ` i ) pure ( i , ln ) SELECT \"t0\" . \"InvoiceId\" AS \"res0\" , \"t0\" . \"CustomerId\" AS \"res1\" , \"t0\" . \"InvoiceDate\" AS \"res2\" , \"t0\" . \"BillingAddress\" AS \"res3\" , \"t0\" . \"BillingCity\" AS \"res4\" , \"t0\" . \"BillingState\" AS \"res5\" , \"t0\" . \"BillingCountry\" AS \"res6\" , \"t0\" . \"BillingPostalCode\" AS \"res7\" , \"t0\" . \"Total\" AS \"res8\" , \"t1\" . \"InvoiceLineId\" AS \"res9\" , \"t1\" . \"InvoiceId\" AS \"res10\" , \"t1\" . \"TrackId\" AS \"res11\" , \"t1\" . \"UnitPrice\" AS \"res12\" , \"t1\" . \"Quantity\" AS \"res13\" FROM \"Invoice\" AS \"t0\" CROSS JOIN \"InvoiceLine\" AS \"t1\" WHERE ( \"t1\" . \"InvoiceId\" ) = ( \"t0\" . \"InvoiceId\" ) SELECT \"t0\" . \"InvoiceId\" AS \"res0\" , \"t0\" . \"CustomerId\" AS \"res1\" , \"t0\" . \"InvoiceDate\" AS \"res2\" , \"t0\" . \"BillingAddress\" AS \"res3\" , \"t0\" . \"BillingCity\" AS \"res4\" , \"t0\" . \"BillingState\" AS \"res5\" , \"t0\" . \"BillingCountry\" AS \"res6\" , \"t0\" . \"BillingPostalCode\" AS \"res7\" , \"t0\" . \"Total\" AS \"res8\" , \"t1\" . \"InvoiceLineId\" AS \"res9\" , \"t1\" . \"InvoiceId\" AS \"res10\" , \"t1\" . \"TrackId\" AS \"res11\" , \"t1\" . \"UnitPrice\" AS \"res12\" , \"t1\" . \"Quantity\" AS \"res13\" FROM \"Invoice\" AS \"t0\" INNER JOIN \"InvoiceLine\" AS \"t1\" WHERE ( \"t1\" . \"InvoiceId\" ) = ( \"t0\" . \"InvoiceId\" ); -- With values: [] Note that beam has floated the guard_ expression into the WHERE clause, rather than the ON clause, This is fine for most inner joins on most database engines, as the query optimizer will execute both queries similarly. However, some backends are more temperamental, so Beam offers several more idiomatic ways to express joins which more closely reflect the underlying SQL. In practice, most users will use the methods below to express JOINs, but it is nevertheless important to understand that joining is fundamental to the structure of the Q moand.","title":"Full inner joins"},{"location":"user-guide/queries/relationships/#one-to-many","text":"Beam supports querying for one-to-many joins. For example, to get every InvoiceLine for each Invoice , use the oneToMany_ combinator. Haskell Postgres Sqlite do i <- all_ ( invoice chinookDb ) ln <- oneToMany_ ( invoiceLine chinookDb ) invoiceLineInvoice i pure ( i , ln ) SELECT \"t0\" . \"InvoiceId\" AS \"res0\" , \"t0\" . \"CustomerId\" AS \"res1\" , \"t0\" . \"InvoiceDate\" AS \"res2\" , \"t0\" . \"BillingAddress\" AS \"res3\" , \"t0\" . \"BillingCity\" AS \"res4\" , \"t0\" . \"BillingState\" AS \"res5\" , \"t0\" . \"BillingCountry\" AS \"res6\" , \"t0\" . \"BillingPostalCode\" AS \"res7\" , \"t0\" . \"Total\" AS \"res8\" , \"t1\" . \"InvoiceLineId\" AS \"res9\" , \"t1\" . \"InvoiceId\" AS \"res10\" , \"t1\" . \"TrackId\" AS \"res11\" , \"t1\" . \"UnitPrice\" AS \"res12\" , \"t1\" . \"Quantity\" AS \"res13\" FROM \"Invoice\" AS \"t0\" INNER JOIN \"InvoiceLine\" AS \"t1\" ON ( \"t1\" . \"InvoiceId\" ) = ( \"t0\" . \"InvoiceId\" ) SELECT \"t0\" . \"InvoiceId\" AS \"res0\" , \"t0\" . \"CustomerId\" AS \"res1\" , \"t0\" . \"InvoiceDate\" AS \"res2\" , \"t0\" . \"BillingAddress\" AS \"res3\" , \"t0\" . \"BillingCity\" AS \"res4\" , \"t0\" . \"BillingState\" AS \"res5\" , \"t0\" . \"BillingCountry\" AS \"res6\" , \"t0\" . \"BillingPostalCode\" AS \"res7\" , \"t0\" . \"Total\" AS \"res8\" , \"t1\" . \"InvoiceLineId\" AS \"res9\" , \"t1\" . \"InvoiceId\" AS \"res10\" , \"t1\" . \"TrackId\" AS \"res11\" , \"t1\" . \"UnitPrice\" AS \"res12\" , \"t1\" . \"Quantity\" AS \"res13\" FROM \"Invoice\" AS \"t0\" INNER JOIN \"InvoiceLine\" AS \"t1\" ON ( \"t1\" . \"InvoiceId\" ) = ( \"t0\" . \"InvoiceId\" ); -- With values: [] Or, if you have an actual Invoice (called oneInvoice ) and you want all the associated InvoiceLine s, you can use val_ to convert oneInvoice to the SQL expression level. oneToMany_ ( invoiceLine chinookDb ) invoiceLineInvoice ( val_ oneInvoice ) If you find yourself repeating yourself constantly, you can define a helper. invoiceLines_ :: OneToMany InvoiceT InvoiceLineT invoiceLines_ = oneToMany_ ( invoiceLine chinookDb ) invoiceLineInvoice Then the above queries become do i <- all_ ( invoice chinookDb ) ln <- invoiceLines_ i and invoiceLines ( val_ i ) Notice that, instead of floating the join condition to the WHERE clause, beam generates an INNER JOIN ... ON expression. These statements are equivalent, although the ON expression is more idiomatic.","title":"One-to-many"},{"location":"user-guide/queries/relationships/#nullable-columns","text":"If you have a nullable foreign key in your many table, you can use oneToManyOptional_ and OneToManyOptional , respectively. For example,","title":"Nullable columns"},{"location":"user-guide/queries/relationships/#one-to-one","text":"One to one relationships are a special case of one to many relationships, save for a unique constraint on one column. Thus, there are no special constructs for one-to-one relationships. For convenience, oneToOne_ and OneToOne are equivalent to oneToMany_ and OneToMany . Additionally, oneToMaybe_ and OneToMaybe correspond to oneToManyOptional_ and OneToManyOptional .","title":"One-to-one"},{"location":"user-guide/queries/relationships/#many-to-many","text":"Many to many relationships require a linking table, with foreign keys to each table part of the relationship. The manyToMany_ construct can be used to fetch both, one, or no sides of a many-to-many relationship. manyToMany_ :: ( Database be db , Table joinThrough , Table left , Table right , Sql92SelectSanityCheck syntax , IsSql92SelectSyntax syntax , SqlEq ( QExpr ( Sql92SelectExpressionSyntax syntax ) s ) ( PrimaryKey left ( QExpr ( Sql92SelectExpressionSyntax syntax ) s )) , SqlEq ( QExpr ( Sql92SelectExpressionSyntax syntax ) s ) ( PrimaryKey right ( QExpr ( Sql92SelectExpressionSyntax syntax ) s )) ) => DatabaseEntity be db ( TableEntity joinThrough ) -> ( joinThrough ( QExpr ( Sql92SelectExpressionSyntax syntax ) s ) -> PrimaryKey left ( QExpr ( Sql92SelectExpressionSyntax syntax ) s )) -> ( joinThrough ( QExpr ( Sql92SelectExpressionSyntax syntax ) s ) -> PrimaryKey right ( QExpr ( Sql92SelectExpressionSyntax syntax ) s )) -> Q syntax db s ( left ( QExpr ( Sql92SelectExpressionSyntax syntax ) s )) -> Q syntax db s ( right ( QExpr ( Sql92SelectExpressionSyntax syntax ) s )) -> Q syntax db s ( left ( QExpr ( Sql92SelectExpressionSyntax syntax ) s ), right ( QExpr ( Sql92SelectExpressionSyntax syntax ) s )) This reads: for any database db ; tables joinThrough , left , and right ; and sane select syntax syntax , where the primary keys of left and right are comparable as value expressions and we have some way of extracting a primary key of left and right from joinThrough , associate all entries of left with those of right through joinThrough and return the results of left and right . The Chinook database associates multiple tracks with a playlist via the playlist_track table. For example, to get all tracks from the playlists named either \"Movies\" or \"Music\". Haskell Postgres Sqlite manyToMany_ ( playlistTrack chinookDb ) playlistTrackPlaylistId playlistTrackTrackId ( filter_ ( \\ p -> playlistName p ==. just_ ( val_ \"Music\" ) ||. playlistName p ==. just_ ( val_ \"Movies\" )) ( all_ ( playlist chinookDb ))) ( all_ ( track chinookDb )) SELECT \"t0\" . \"PlaylistId\" AS \"res0\" , \"t0\" . \"Name\" AS \"res1\" , \"t1\" . \"TrackId\" AS \"res2\" , \"t1\" . \"Name\" AS \"res3\" , \"t1\" . \"AlbumId\" AS \"res4\" , \"t1\" . \"MediaTypeId\" AS \"res5\" , \"t1\" . \"GenreId\" AS \"res6\" , \"t1\" . \"Composer\" AS \"res7\" , \"t1\" . \"Milliseconds\" AS \"res8\" , \"t1\" . \"Bytes\" AS \"res9\" , \"t1\" . \"UnitPrice\" AS \"res10\" FROM \"Playlist\" AS \"t0\" CROSS JOIN \"Track\" AS \"t1\" INNER JOIN \"PlaylistTrack\" AS \"t2\" ON (( \"t2\" . \"PlaylistId\" ) = ( \"t0\" . \"PlaylistId\" )) AND (( \"t2\" . \"TrackId\" ) = ( \"t1\" . \"TrackId\" )) WHERE (( \"t0\" . \"Name\" ) IS NOT DISTINCT FROM ( 'Music' )) OR (( \"t0\" . \"Name\" ) IS NOT DISTINCT FROM ( 'Movies' )) SELECT \"t0\" . \"PlaylistId\" AS \"res0\" , \"t0\" . \"Name\" AS \"res1\" , \"t1\" . \"TrackId\" AS \"res2\" , \"t1\" . \"Name\" AS \"res3\" , \"t1\" . \"AlbumId\" AS \"res4\" , \"t1\" . \"MediaTypeId\" AS \"res5\" , \"t1\" . \"GenreId\" AS \"res6\" , \"t1\" . \"Composer\" AS \"res7\" , \"t1\" . \"Milliseconds\" AS \"res8\" , \"t1\" . \"Bytes\" AS \"res9\" , \"t1\" . \"UnitPrice\" AS \"res10\" FROM \"Playlist\" AS \"t0\" INNER JOIN \"Track\" AS \"t1\" INNER JOIN \"PlaylistTrack\" AS \"t2\" ON (( \"t2\" . \"PlaylistId\" ) = ( \"t0\" . \"PlaylistId\" )) AND (( \"t2\" . \"TrackId\" ) = ( \"t1\" . \"TrackId\" )) WHERE ( CASE WHEN (( \"t0\" . \"Name\" ) IS NULL ) AND (( ? ) IS NULL ) THEN ? WHEN (( \"t0\" . \"Name\" ) IS NULL ) OR (( ? ) IS NULL ) THEN ? ELSE ( \"t0\" . \"Name\" ) = ( ? ) END ) OR ( CASE WHEN (( \"t0\" . \"Name\" ) IS NULL ) AND (( ? ) IS NULL ) THEN ? WHEN (( \"t0\" . \"Name\" ) IS NULL ) OR (( ? ) IS NULL ) THEN ? ELSE ( \"t0\" . \"Name\" ) = ( ? ) END ); -- With values: [SQLText \"Music\",SQLInteger 1,SQLText \"Music\",SQLInteger 0,SQLText \"Music\",SQLText \"Movies\",SQLInteger 1,SQLText \"Movies\",SQLInteger 0,SQLText \"Movies\"]","title":"Many-to-many"},{"location":"user-guide/queries/relationships/#many-to-many-with-arbitrary-data","text":"Sometimes you want to have additional data for each relationship. For this, use manyToManyPassthrough_ . manyToManyPassthrough_ :: ( Database be db , Table joinThrough , Table left , Table right , Sql92SelectSanityCheck syntax , IsSql92SelectSyntax syntax , SqlEq ( QExpr ( Sql92SelectExpressionSyntax syntax ) s ) ( PrimaryKey left ( QExpr ( Sql92SelectExpressionSyntax syntax ) s )) , SqlEq ( QExpr ( Sql92SelectExpressionSyntax syntax ) s ) ( PrimaryKey right ( QExpr ( Sql92SelectExpressionSyntax syntax ) s )) ) => DatabaseEntity be db ( TableEntity joinThrough ) -> ( joinThrough ( QExpr ( Sql92SelectExpressionSyntax syntax ) s ) -> PrimaryKey left ( QExpr ( Sql92SelectExpressionSyntax syntax ) s )) -> ( joinThrough ( QExpr ( Sql92SelectExpressionSyntax syntax ) s ) -> PrimaryKey right ( QExpr ( Sql92SelectExpressionSyntax syntax ) s )) -> Q syntax db s ( left ( QExpr ( Sql92SelectExpressionSyntax syntax ) s )) -> Q syntax db s ( right ( QExpr ( Sql92SelectExpressionSyntax syntax ) s )) -> Q syntax db s ( joinThrough ( QExpr ( Sql92SelectExpressionSyntax syntax ) s ) , left ( QExpr ( Sql92SelectExpressionSyntax syntax ) s ) , right ( QExpr ( Sql92SelectExpressionSyntax syntax ) s )) Under the hood manyToMany_ is defined simply as manyToMany_ = fmap ( \\ ( _ , left , right ) -> ( left , right )) manyToManyPassthrough_","title":"Many-to-many with arbitrary data"},{"location":"user-guide/queries/relationships/#declaring-many-to-many-relationships","text":"Like one-to-many relationships, beam allows you to extract commonly used many-to-many relationships, via the ManyToMany type. For example, the playlist/track relationship above can be defined as follows playlistTrackRelationship :: ManyToMany ChinookDb PlaylistT TrackT playlistTrackRelationship = manyToMany_ ( playlistTrack chinookDb ) playlistTrackPlaylistId playlistTrackTrackId And we can use it as expected: Haskell Postgres Sqlite playlistTrackRelationship ( filter_ ( \\ p -> playlistName p ==. just_ ( val_ \"Music\" ) ||. playlistName p ==. just_ ( val_ \"Movies\" )) ( all_ ( playlist chinookDb ))) ( all_ ( track chinookDb )) SELECT \"t0\" . \"PlaylistId\" AS \"res0\" , \"t0\" . \"Name\" AS \"res1\" , \"t1\" . \"TrackId\" AS \"res2\" , \"t1\" . \"Name\" AS \"res3\" , \"t1\" . \"AlbumId\" AS \"res4\" , \"t1\" . \"MediaTypeId\" AS \"res5\" , \"t1\" . \"GenreId\" AS \"res6\" , \"t1\" . \"Composer\" AS \"res7\" , \"t1\" . \"Milliseconds\" AS \"res8\" , \"t1\" . \"Bytes\" AS \"res9\" , \"t1\" . \"UnitPrice\" AS \"res10\" FROM \"Playlist\" AS \"t0\" CROSS JOIN \"Track\" AS \"t1\" INNER JOIN \"PlaylistTrack\" AS \"t2\" ON (( \"t2\" . \"PlaylistId\" ) = ( \"t0\" . \"PlaylistId\" )) AND (( \"t2\" . \"TrackId\" ) = ( \"t1\" . \"TrackId\" )) WHERE (( \"t0\" . \"Name\" ) IS NOT DISTINCT FROM ( 'Music' )) OR (( \"t0\" . \"Name\" ) IS NOT DISTINCT FROM ( 'Movies' )) SELECT \"t0\" . \"PlaylistId\" AS \"res0\" , \"t0\" . \"Name\" AS \"res1\" , \"t1\" . \"TrackId\" AS \"res2\" , \"t1\" . \"Name\" AS \"res3\" , \"t1\" . \"AlbumId\" AS \"res4\" , \"t1\" . \"MediaTypeId\" AS \"res5\" , \"t1\" . \"GenreId\" AS \"res6\" , \"t1\" . \"Composer\" AS \"res7\" , \"t1\" . \"Milliseconds\" AS \"res8\" , \"t1\" . \"Bytes\" AS \"res9\" , \"t1\" . \"UnitPrice\" AS \"res10\" FROM \"Playlist\" AS \"t0\" INNER JOIN \"Track\" AS \"t1\" INNER JOIN \"PlaylistTrack\" AS \"t2\" ON (( \"t2\" . \"PlaylistId\" ) = ( \"t0\" . \"PlaylistId\" )) AND (( \"t2\" . \"TrackId\" ) = ( \"t1\" . \"TrackId\" )) WHERE ( CASE WHEN (( \"t0\" . \"Name\" ) IS NULL ) AND (( ? ) IS NULL ) THEN ? WHEN (( \"t0\" . \"Name\" ) IS NULL ) OR (( ? ) IS NULL ) THEN ? ELSE ( \"t0\" . \"Name\" ) = ( ? ) END ) OR ( CASE WHEN (( \"t0\" . \"Name\" ) IS NULL ) AND (( ? ) IS NULL ) THEN ? WHEN (( \"t0\" . \"Name\" ) IS NULL ) OR (( ? ) IS NULL ) THEN ? ELSE ( \"t0\" . \"Name\" ) = ( ? ) END ); -- With values: [SQLText \"Music\",SQLInteger 1,SQLText \"Music\",SQLInteger 0,SQLText \"Music\",SQLText \"Movies\",SQLInteger 1,SQLText \"Movies\",SQLInteger 0,SQLText \"Movies\"] ManyToManyThrough is the equivalent for manyToManyThrough_ , except it takes another table parameter for the 'through' table.","title":"Declaring many-to-many relationships"},{"location":"user-guide/queries/relationships/#arbitrary-joins","text":"Joins with arbitrary conditions can be specified using the join_ construct. For example, oneToMany_ is implemented as oneToMany_ rel getKey tbl = join_ rel ( \\ rel -> getKey rel ==. pk tbl ) Thus, the invoice example above could be rewritten. For example, instead of do i <- all_ ( invoice chinookDb ) ln <- oneToMany_ ( invoiceLine chinookDb ) invoiceLineInvoice i pure ( i , ln ) We could write Haskell Postgres Sqlite do i <- all_ ( invoice chinookDb ) ln <- join_ ( invoiceLine chinookDb ) ( \\ line -> invoiceLineInvoice line ==. primaryKey i ) pure ( i , ln ) SELECT \"t0\" . \"InvoiceId\" AS \"res0\" , \"t0\" . \"CustomerId\" AS \"res1\" , \"t0\" . \"InvoiceDate\" AS \"res2\" , \"t0\" . \"BillingAddress\" AS \"res3\" , \"t0\" . \"BillingCity\" AS \"res4\" , \"t0\" . \"BillingState\" AS \"res5\" , \"t0\" . \"BillingCountry\" AS \"res6\" , \"t0\" . \"BillingPostalCode\" AS \"res7\" , \"t0\" . \"Total\" AS \"res8\" , \"t1\" . \"InvoiceLineId\" AS \"res9\" , \"t1\" . \"InvoiceId\" AS \"res10\" , \"t1\" . \"TrackId\" AS \"res11\" , \"t1\" . \"UnitPrice\" AS \"res12\" , \"t1\" . \"Quantity\" AS \"res13\" FROM \"Invoice\" AS \"t0\" INNER JOIN \"InvoiceLine\" AS \"t1\" ON ( \"t1\" . \"InvoiceId\" ) = ( \"t0\" . \"InvoiceId\" ) SELECT \"t0\" . \"InvoiceId\" AS \"res0\" , \"t0\" . \"CustomerId\" AS \"res1\" , \"t0\" . \"InvoiceDate\" AS \"res2\" , \"t0\" . \"BillingAddress\" AS \"res3\" , \"t0\" . \"BillingCity\" AS \"res4\" , \"t0\" . \"BillingState\" AS \"res5\" , \"t0\" . \"BillingCountry\" AS \"res6\" , \"t0\" . \"BillingPostalCode\" AS \"res7\" , \"t0\" . \"Total\" AS \"res8\" , \"t1\" . \"InvoiceLineId\" AS \"res9\" , \"t1\" . \"InvoiceId\" AS \"res10\" , \"t1\" . \"TrackId\" AS \"res11\" , \"t1\" . \"UnitPrice\" AS \"res12\" , \"t1\" . \"Quantity\" AS \"res13\" FROM \"Invoice\" AS \"t0\" INNER JOIN \"InvoiceLine\" AS \"t1\" ON ( \"t1\" . \"InvoiceId\" ) = ( \"t0\" . \"InvoiceId\" ); -- With values: []","title":"Arbitrary Joins"},{"location":"user-guide/queries/relationships/#outer-joins","text":"","title":"Outer joins"},{"location":"user-guide/queries/relationships/#left-and-right-joins","text":"Left joins with arbitrary conditions can be specified with the leftJoin_ construct. leftJoin_ takes an arbitrary query and a join condition. It associates each result record with a record of the table given or a fully NULL row of that table in case no row matches. For this reason, the result of leftJoin_ has an extra Nullable column tag, which converts each field into the corresponding Maybe type. Note The table parameter passed in as the join condition does not have a Nullable column tag. The join condition should be written as if a concrete row from that table exists. For example, to get every artist along with their albums, but always including every artist, use leftJoin_ as follows. Haskell Postgres Sqlite do artist <- all_ ( artist chinookDb ) album <- leftJoin_ ( all_ ( album chinookDb )) ( \\ album -> albumArtist album ==. primaryKey artist ) pure ( artist , album ) SELECT \"t0\" . \"ArtistId\" AS \"res0\" , \"t0\" . \"Name\" AS \"res1\" , \"t1\" . \"AlbumId\" AS \"res2\" , \"t1\" . \"Title\" AS \"res3\" , \"t1\" . \"ArtistId\" AS \"res4\" FROM \"Artist\" AS \"t0\" LEFT JOIN \"Album\" AS \"t1\" ON ( \"t1\" . \"ArtistId\" ) = ( \"t0\" . \"ArtistId\" ) SELECT \"t0\" . \"ArtistId\" AS \"res0\" , \"t0\" . \"Name\" AS \"res1\" , \"t1\" . \"AlbumId\" AS \"res2\" , \"t1\" . \"Title\" AS \"res3\" , \"t1\" . \"ArtistId\" AS \"res4\" FROM \"Artist\" AS \"t0\" LEFT JOIN \"Album\" AS \"t1\" ON ( \"t1\" . \"ArtistId\" ) = ( \"t0\" . \"ArtistId\" ); -- With values: [] Right joins are not yet supported. They can always be rewritten as left joins. If you have a compelling use case, please file an issue!","title":"Left and right joins"},{"location":"user-guide/queries/relationships/#handling-sql-nulls","text":"NULL is a value that SQL treats as an 'unknown' value. Unfortunately, this can cause a lot of unexpected issues. Beam tries to normalize the handling of NULLs to some extent, but it ultimately cannot save you from the database. One thing you can be sure of is that -- assuming your beam schema matches that of the database -- any beam expression that does not yield a Maybe type cannot be NULL at run-time. Also, beam treats equality between Maybe types correctly using the standard ==. and /=. operators. This means that beam will sometimes generate obtuse CASE expressions. This is because beam's philosophy is that SQL operators be named after their equivalent Haskell ones, suffixed by a . , and that these operators should follow Haskell semantics. Sometimes though, this care isn't necessary. When you are okay with SQL equality, you can use the (==?.) and (/=?.) operators. These work the same as the (==.) and (/=.) , except they return a SqlBool instead of Bool . SqlBool can only occur as the result of a SQL expression, and it cannot be deserialized directly into Haskell on any backend. A SqlBool value can contain TRUE , FALSE , and UNKNOWN (the third SQL boolean value). You can marshal between SqlBool and Bool using isTrue_ , isFalse_ , or isUnknown_ to determine which value a SqlBool contains. The unknownAs_ function takes a default Haskell Bool and SQL expression returning SqlBool . It returns the given Haskell Bool value in the case the SQL expression is indeterminate. You can also convert any expression returning Bool to one returning SqlBool by using the sqlBool_ function. The various beam functions that deal with Bool also have corresponding versions that operate on SqlBool . For example, whereas leftJoin_ expects its join condition to be a Bool , the corresponding leftJoin_' (notice the prime) method takes a SqlBool . There are corresponding guard_' , join_' , etc methods. Boolean operators, such as (&&.) and (||.) , have SqlBool equivalents suffixed with ? ( (&&?.) and (||?.) for SqlBool AND and OR respectively). One place where this can really bite is when generating ON conditions. Many RDBMSes use a rather unintelligent means of choosing which indices to use, by directly matching on syntaxes. For example, postgres determines index usage by directly seeing if two columns are compared. If you wrap the comparison in the IS TRUE operator, the index is no longer used. In these cases, using the proper boolean handling can severely impact performance. For example, to get every customer along with employees in their area, we can left join the customer table with employees on their city. Haskell Postgres Sqlite do c <- all_ ( customer chinookDb ) e <- leftJoin_ ( all_ ( employee chinookDb )) ( \\ e -> addressCity ( employeeAddress e ) ==. addressCity ( customerAddress c )) pure ( c , e ) SELECT \"t0\" . \"CustomerId\" AS \"res0\" , \"t0\" . \"FirstName\" AS \"res1\" , \"t0\" . \"LastName\" AS \"res2\" , \"t0\" . \"Company\" AS \"res3\" , \"t0\" . \"Address\" AS \"res4\" , \"t0\" . \"City\" AS \"res5\" , \"t0\" . \"State\" AS \"res6\" , \"t0\" . \"Country\" AS \"res7\" , \"t0\" . \"PostalCode\" AS \"res8\" , \"t0\" . \"Phone\" AS \"res9\" , \"t0\" . \"Fax\" AS \"res10\" , \"t0\" . \"Email\" AS \"res11\" , \"t0\" . \"SupportRepId\" AS \"res12\" , \"t1\" . \"EmployeeId\" AS \"res13\" , \"t1\" . \"LastName\" AS \"res14\" , \"t1\" . \"FirstName\" AS \"res15\" , \"t1\" . \"Title\" AS \"res16\" , \"t1\" . \"ReportsTo\" AS \"res17\" , \"t1\" . \"BirthDate\" AS \"res18\" , \"t1\" . \"HireDate\" AS \"res19\" , \"t1\" . \"Address\" AS \"res20\" , \"t1\" . \"City\" AS \"res21\" , \"t1\" . \"State\" AS \"res22\" , \"t1\" . \"Country\" AS \"res23\" , \"t1\" . \"PostalCode\" AS \"res24\" , \"t1\" . \"Phone\" AS \"res25\" , \"t1\" . \"Fax\" AS \"res26\" , \"t1\" . \"Email\" AS \"res27\" FROM \"Customer\" AS \"t0\" LEFT JOIN \"Employee\" AS \"t1\" ON ( \"t1\" . \"City\" ) IS NOT DISTINCT FROM ( \"t0\" . \"City\" ) SELECT \"t0\" . \"CustomerId\" AS \"res0\" , \"t0\" . \"FirstName\" AS \"res1\" , \"t0\" . \"LastName\" AS \"res2\" , \"t0\" . \"Company\" AS \"res3\" , \"t0\" . \"Address\" AS \"res4\" , \"t0\" . \"City\" AS \"res5\" , \"t0\" . \"State\" AS \"res6\" , \"t0\" . \"Country\" AS \"res7\" , \"t0\" . \"PostalCode\" AS \"res8\" , \"t0\" . \"Phone\" AS \"res9\" , \"t0\" . \"Fax\" AS \"res10\" , \"t0\" . \"Email\" AS \"res11\" , \"t0\" . \"SupportRepId\" AS \"res12\" , \"t1\" . \"EmployeeId\" AS \"res13\" , \"t1\" . \"LastName\" AS \"res14\" , \"t1\" . \"FirstName\" AS \"res15\" , \"t1\" . \"Title\" AS \"res16\" , \"t1\" . \"ReportsTo\" AS \"res17\" , \"t1\" . \"BirthDate\" AS \"res18\" , \"t1\" . \"HireDate\" AS \"res19\" , \"t1\" . \"Address\" AS \"res20\" , \"t1\" . \"City\" AS \"res21\" , \"t1\" . \"State\" AS \"res22\" , \"t1\" . \"Country\" AS \"res23\" , \"t1\" . \"PostalCode\" AS \"res24\" , \"t1\" . \"Phone\" AS \"res25\" , \"t1\" . \"Fax\" AS \"res26\" , \"t1\" . \"Email\" AS \"res27\" FROM \"Customer\" AS \"t0\" LEFT JOIN \"Employee\" AS \"t1\" ON CASE WHEN (( \"t1\" . \"City\" ) IS NULL ) AND (( \"t0\" . \"City\" ) IS NULL ) THEN ? WHEN (( \"t1\" . \"City\" ) IS NULL ) OR (( \"t0\" . \"City\" ) IS NULL ) THEN ? ELSE ( \"t1\" . \"City\" ) = ( \"t0\" . \"City\" ) END ; -- With values: [SQLInteger 1,SQLInteger 0] Notice that the join condition is not just a simple = . This will cause postgres to ignore any index on these columns. We can instead use leftJoin_' and ==?. to be more direct. Haskell Postgres Sqlite do c <- all_ ( customer chinookDb ) e <- leftJoin_' ( all_ ( employee chinookDb )) ( \\ e -> addressCity ( employeeAddress e ) ==?. addressCity ( customerAddress c )) pure ( c , e ) SELECT \"t0\" . \"CustomerId\" AS \"res0\" , \"t0\" . \"FirstName\" AS \"res1\" , \"t0\" . \"LastName\" AS \"res2\" , \"t0\" . \"Company\" AS \"res3\" , \"t0\" . \"Address\" AS \"res4\" , \"t0\" . \"City\" AS \"res5\" , \"t0\" . \"State\" AS \"res6\" , \"t0\" . \"Country\" AS \"res7\" , \"t0\" . \"PostalCode\" AS \"res8\" , \"t0\" . \"Phone\" AS \"res9\" , \"t0\" . \"Fax\" AS \"res10\" , \"t0\" . \"Email\" AS \"res11\" , \"t0\" . \"SupportRepId\" AS \"res12\" , \"t1\" . \"EmployeeId\" AS \"res13\" , \"t1\" . \"LastName\" AS \"res14\" , \"t1\" . \"FirstName\" AS \"res15\" , \"t1\" . \"Title\" AS \"res16\" , \"t1\" . \"ReportsTo\" AS \"res17\" , \"t1\" . \"BirthDate\" AS \"res18\" , \"t1\" . \"HireDate\" AS \"res19\" , \"t1\" . \"Address\" AS \"res20\" , \"t1\" . \"City\" AS \"res21\" , \"t1\" . \"State\" AS \"res22\" , \"t1\" . \"Country\" AS \"res23\" , \"t1\" . \"PostalCode\" AS \"res24\" , \"t1\" . \"Phone\" AS \"res25\" , \"t1\" . \"Fax\" AS \"res26\" , \"t1\" . \"Email\" AS \"res27\" FROM \"Customer\" AS \"t0\" LEFT JOIN \"Employee\" AS \"t1\" ON ( \"t1\" . \"City\" ) = ( \"t0\" . \"City\" ) SELECT \"t0\" . \"CustomerId\" AS \"res0\" , \"t0\" . \"FirstName\" AS \"res1\" , \"t0\" . \"LastName\" AS \"res2\" , \"t0\" . \"Company\" AS \"res3\" , \"t0\" . \"Address\" AS \"res4\" , \"t0\" . \"City\" AS \"res5\" , \"t0\" . \"State\" AS \"res6\" , \"t0\" . \"Country\" AS \"res7\" , \"t0\" . \"PostalCode\" AS \"res8\" , \"t0\" . \"Phone\" AS \"res9\" , \"t0\" . \"Fax\" AS \"res10\" , \"t0\" . \"Email\" AS \"res11\" , \"t0\" . \"SupportRepId\" AS \"res12\" , \"t1\" . \"EmployeeId\" AS \"res13\" , \"t1\" . \"LastName\" AS \"res14\" , \"t1\" . \"FirstName\" AS \"res15\" , \"t1\" . \"Title\" AS \"res16\" , \"t1\" . \"ReportsTo\" AS \"res17\" , \"t1\" . \"BirthDate\" AS \"res18\" , \"t1\" . \"HireDate\" AS \"res19\" , \"t1\" . \"Address\" AS \"res20\" , \"t1\" . \"City\" AS \"res21\" , \"t1\" . \"State\" AS \"res22\" , \"t1\" . \"Country\" AS \"res23\" , \"t1\" . \"PostalCode\" AS \"res24\" , \"t1\" . \"Phone\" AS \"res25\" , \"t1\" . \"Fax\" AS \"res26\" , \"t1\" . \"Email\" AS \"res27\" FROM \"Customer\" AS \"t0\" LEFT JOIN \"Employee\" AS \"t1\" ON ( \"t1\" . \"City\" ) = ( \"t0\" . \"City\" ); -- With values: [] Now postgres will use an index.","title":"Handling SQL NULLs"},{"location":"user-guide/queries/relationships/#full-outer-joins","text":"Outer joins are supported with the outerJoin_ function. outerJoin_ takes two queries and a join condition and returns a Q that represents the FULL OUTER JOIN of the two queries. Because either table may be nullable, the output of the result has an additional Nullable tag. NOTE Outer joins are only supported in backends whose SQL FROM syntax implements IsSql92FromOuterJoinSyntax . Notably, this does not include SQLite. For example, to get join all employees with customers with the same first name but including all employees and customers, we can run the query Haskell Postgres outerJoin_ ( all_ ( employee chinookDb )) ( all_ ( customer chinookDb )) ( \\ ( employee , customer ) -> employeeFirstName employee ==. customerFirstName customer ) SELECT \"t0\" . \"EmployeeId\" AS \"res0\" , \"t0\" . \"LastName\" AS \"res1\" , \"t0\" . \"FirstName\" AS \"res2\" , \"t0\" . \"Title\" AS \"res3\" , \"t0\" . \"ReportsTo\" AS \"res4\" , \"t0\" . \"BirthDate\" AS \"res5\" , \"t0\" . \"HireDate\" AS \"res6\" , \"t0\" . \"Address\" AS \"res7\" , \"t0\" . \"City\" AS \"res8\" , \"t0\" . \"State\" AS \"res9\" , \"t0\" . \"Country\" AS \"res10\" , \"t0\" . \"PostalCode\" AS \"res11\" , \"t0\" . \"Phone\" AS \"res12\" , \"t0\" . \"Fax\" AS \"res13\" , \"t0\" . \"Email\" AS \"res14\" , \"t1\" . \"CustomerId\" AS \"res15\" , \"t1\" . \"FirstName\" AS \"res16\" , \"t1\" . \"LastName\" AS \"res17\" , \"t1\" . \"Company\" AS \"res18\" , \"t1\" . \"Address\" AS \"res19\" , \"t1\" . \"City\" AS \"res20\" , \"t1\" . \"State\" AS \"res21\" , \"t1\" . \"Country\" AS \"res22\" , \"t1\" . \"PostalCode\" AS \"res23\" , \"t1\" . \"Phone\" AS \"res24\" , \"t1\" . \"Fax\" AS \"res25\" , \"t1\" . \"Email\" AS \"res26\" , \"t1\" . \"SupportRepId\" AS \"res27\" FROM \"Employee\" AS \"t0\" FULL OUTER JOIN \"Customer\" AS \"t1\" ON ( \"t0\" . \"FirstName\" ) = ( \"t1\" . \"FirstName\" )","title":"Full Outer joins"},{"location":"user-guide/queries/relationships/#subqueries","text":"Sometimes you want to join against a subquery rather than a table. For the most part, beam will automatically figure out when certain queries need to be written using subqueries. For example, to join two result sets cointaining a SQL LIMIT, you would normally have to write both queries as subqueries. In beam, you can write such queries as you'd expect. The library takes care of creating subqueries as expected. For example, the following query generates the code you'd expect. Haskell Postgres Sqlite do i <- limit_ 10 $ all_ ( invoice chinookDb ) line <- invoiceLines i pure ( i , line ) SELECT \"t0\" . \"res0\" AS \"res0\" , \"t0\" . \"res1\" AS \"res1\" , \"t0\" . \"res2\" AS \"res2\" , \"t0\" . \"res3\" AS \"res3\" , \"t0\" . \"res4\" AS \"res4\" , \"t0\" . \"res5\" AS \"res5\" , \"t0\" . \"res6\" AS \"res6\" , \"t0\" . \"res7\" AS \"res7\" , \"t0\" . \"res8\" AS \"res8\" , \"t1\" . \"InvoiceLineId\" AS \"res9\" , \"t1\" . \"InvoiceId\" AS \"res10\" , \"t1\" . \"TrackId\" AS \"res11\" , \"t1\" . \"UnitPrice\" AS \"res12\" , \"t1\" . \"Quantity\" AS \"res13\" FROM ( SELECT \"t0\" . \"InvoiceId\" AS \"res0\" , \"t0\" . \"CustomerId\" AS \"res1\" , \"t0\" . \"InvoiceDate\" AS \"res2\" , \"t0\" . \"BillingAddress\" AS \"res3\" , \"t0\" . \"BillingCity\" AS \"res4\" , \"t0\" . \"BillingState\" AS \"res5\" , \"t0\" . \"BillingCountry\" AS \"res6\" , \"t0\" . \"BillingPostalCode\" AS \"res7\" , \"t0\" . \"Total\" AS \"res8\" FROM \"Invoice\" AS \"t0\" LIMIT 10 ) AS \"t0\" INNER JOIN \"InvoiceLine\" AS \"t1\" ON ( \"t1\" . \"InvoiceId\" ) = ( \"t0\" . \"res0\" ) SELECT \"t0\" . \"res0\" AS \"res0\" , \"t0\" . \"res1\" AS \"res1\" , \"t0\" . \"res2\" AS \"res2\" , \"t0\" . \"res3\" AS \"res3\" , \"t0\" . \"res4\" AS \"res4\" , \"t0\" . \"res5\" AS \"res5\" , \"t0\" . \"res6\" AS \"res6\" , \"t0\" . \"res7\" AS \"res7\" , \"t0\" . \"res8\" AS \"res8\" , \"t1\" . \"InvoiceLineId\" AS \"res9\" , \"t1\" . \"InvoiceId\" AS \"res10\" , \"t1\" . \"TrackId\" AS \"res11\" , \"t1\" . \"UnitPrice\" AS \"res12\" , \"t1\" . \"Quantity\" AS \"res13\" FROM ( SELECT \"t0\" . \"InvoiceId\" AS \"res0\" , \"t0\" . \"CustomerId\" AS \"res1\" , \"t0\" . \"InvoiceDate\" AS \"res2\" , \"t0\" . \"BillingAddress\" AS \"res3\" , \"t0\" . \"BillingCity\" AS \"res4\" , \"t0\" . \"BillingState\" AS \"res5\" , \"t0\" . \"BillingCountry\" AS \"res6\" , \"t0\" . \"BillingPostalCode\" AS \"res7\" , \"t0\" . \"Total\" AS \"res8\" FROM \"Invoice\" AS \"t0\" LIMIT 10 ) AS \"t0\" INNER JOIN \"InvoiceLine\" AS \"t1\" ON ( \"t1\" . \"InvoiceId\" ) = ( \"t0\" . \"res0\" ); -- With values: [] If you need to (for efficiency for example), you can also generate subqueries explicitly, using subselect_ . The subselect_ will force a new query to be output in most cases. For simple queries, such as all_ , subselect_ will have no effect. Haskell Postgres Sqlite -- Same as above, but with explicit sub select do i <- subselect_ $ limit_ 10 $ all_ ( invoice chinookDb ) line <- invoiceLines i pure ( i , line ) SELECT \"t0\" . \"res0\" AS \"res0\" , \"t0\" . \"res1\" AS \"res1\" , \"t0\" . \"res2\" AS \"res2\" , \"t0\" . \"res3\" AS \"res3\" , \"t0\" . \"res4\" AS \"res4\" , \"t0\" . \"res5\" AS \"res5\" , \"t0\" . \"res6\" AS \"res6\" , \"t0\" . \"res7\" AS \"res7\" , \"t0\" . \"res8\" AS \"res8\" , \"t1\" . \"InvoiceLineId\" AS \"res9\" , \"t1\" . \"InvoiceId\" AS \"res10\" , \"t1\" . \"TrackId\" AS \"res11\" , \"t1\" . \"UnitPrice\" AS \"res12\" , \"t1\" . \"Quantity\" AS \"res13\" FROM ( SELECT \"t0\" . \"InvoiceId\" AS \"res0\" , \"t0\" . \"CustomerId\" AS \"res1\" , \"t0\" . \"InvoiceDate\" AS \"res2\" , \"t0\" . \"BillingAddress\" AS \"res3\" , \"t0\" . \"BillingCity\" AS \"res4\" , \"t0\" . \"BillingState\" AS \"res5\" , \"t0\" . \"BillingCountry\" AS \"res6\" , \"t0\" . \"BillingPostalCode\" AS \"res7\" , \"t0\" . \"Total\" AS \"res8\" FROM \"Invoice\" AS \"t0\" LIMIT 10 ) AS \"t0\" INNER JOIN \"InvoiceLine\" AS \"t1\" ON ( \"t1\" . \"InvoiceId\" ) = ( \"t0\" . \"res0\" ) SELECT \"t0\" . \"res0\" AS \"res0\" , \"t0\" . \"res1\" AS \"res1\" , \"t0\" . \"res2\" AS \"res2\" , \"t0\" . \"res3\" AS \"res3\" , \"t0\" . \"res4\" AS \"res4\" , \"t0\" . \"res5\" AS \"res5\" , \"t0\" . \"res6\" AS \"res6\" , \"t0\" . \"res7\" AS \"res7\" , \"t0\" . \"res8\" AS \"res8\" , \"t1\" . \"InvoiceLineId\" AS \"res9\" , \"t1\" . \"InvoiceId\" AS \"res10\" , \"t1\" . \"TrackId\" AS \"res11\" , \"t1\" . \"UnitPrice\" AS \"res12\" , \"t1\" . \"Quantity\" AS \"res13\" FROM ( SELECT \"t0\" . \"InvoiceId\" AS \"res0\" , \"t0\" . \"CustomerId\" AS \"res1\" , \"t0\" . \"InvoiceDate\" AS \"res2\" , \"t0\" . \"BillingAddress\" AS \"res3\" , \"t0\" . \"BillingCity\" AS \"res4\" , \"t0\" . \"BillingState\" AS \"res5\" , \"t0\" . \"BillingCountry\" AS \"res6\" , \"t0\" . \"BillingPostalCode\" AS \"res7\" , \"t0\" . \"Total\" AS \"res8\" FROM \"Invoice\" AS \"t0\" LIMIT 10 ) AS \"t0\" INNER JOIN \"InvoiceLine\" AS \"t1\" ON ( \"t1\" . \"InvoiceId\" ) = ( \"t0\" . \"res0\" ); -- With values: []","title":"Subqueries"},{"location":"user-guide/queries/select/","text":"We've seen how to create simple queries from our schema. Beam supports other clauses in the SQL SELECT statement. For these examples, we're going to use the beam-sqlite backend with the provided sample Chinook database. The Chinook database schema is modeled after a fictional record store. It provides several tables containing information on the music as well as the billing operations. Thus, it provides a good 'real-world' demonstration of beam's capabalities. First, create a SQLite database from the included example. $ sqlite3 chinook.db < beam-sqlite/examples/chinook.sql Now, load the chinook database schema in GHCi. Prelude Database . Beam . Sqlite > : load beam - sqlite / examples / Chinook / Schema . hs Prelude Chinook . Schema > chinook <- open \"chinook.db\" One more thing, before we see more complex examples, let's define a quick utility function. Prelude Chinook . Schema > let withConnectionTutorial = runBeamSqliteDebug putStrLn chinook Let's test it! We can run all our queries like: withConnectionTutorial $ runSelectReturningList $ select $ < query > Let's select all the tracks. withConnectionTutorial $ runSelectReturningList $ select $ all_ ( track chinookDb ) For the rest of the guide, we will also show the generated SQL code for both sqlite and postgres. Haskell Postgres Sqlite all_ ( track chinookDb ) SELECT \"t0\" . \"TrackId\" AS \"res0\" , \"t0\" . \"Name\" AS \"res1\" , \"t0\" . \"AlbumId\" AS \"res2\" , \"t0\" . \"MediaTypeId\" AS \"res3\" , \"t0\" . \"GenreId\" AS \"res4\" , \"t0\" . \"Composer\" AS \"res5\" , \"t0\" . \"Milliseconds\" AS \"res6\" , \"t0\" . \"Bytes\" AS \"res7\" , \"t0\" . \"UnitPrice\" AS \"res8\" FROM \"Track\" AS \"t0\" SELECT \"t0\" . \"TrackId\" AS \"res0\" , \"t0\" . \"Name\" AS \"res1\" , \"t0\" . \"AlbumId\" AS \"res2\" , \"t0\" . \"MediaTypeId\" AS \"res3\" , \"t0\" . \"GenreId\" AS \"res4\" , \"t0\" . \"Composer\" AS \"res5\" , \"t0\" . \"Milliseconds\" AS \"res6\" , \"t0\" . \"Bytes\" AS \"res7\" , \"t0\" . \"UnitPrice\" AS \"res8\" FROM \"Track\" AS \"t0\" ; -- With values: [] Returning a subset of columns Oftentimes we only care about the value of a few columns, rather than every column in the table. Beam fully supports taking projections of tables. As said before, Q is a Monad . Thus, we can use monadic do notation to only select a certain subset of columns. For example, to fetch only the name of every track: Haskell Postgres Sqlite do tracks <- all_ ( track chinookDb ) pure ( trackName tracks ) SELECT \"t0\" . \"Name\" AS \"res0\" FROM \"Track\" AS \"t0\" SELECT \"t0\" . \"Name\" AS \"res0\" FROM \"Track\" AS \"t0\" ; -- With values: [] Notice that beam has properly written the SELECT projection to only include the Name field. We can also return multiple fields, by returning a tuple. Perhaps we would also like to know the composer: Haskell Postgres Sqlite do tracks <- all_ ( track chinookDb ) pure ( trackName tracks , trackComposer tracks ) SELECT \"t0\" . \"Name\" AS \"res0\" , \"t0\" . \"Composer\" AS \"res1\" FROM \"Track\" AS \"t0\" SELECT \"t0\" . \"Name\" AS \"res0\" , \"t0\" . \"Composer\" AS \"res1\" FROM \"Track\" AS \"t0\" ; -- With values: [] You can also return arbitrary expressions in the projection. For example to return the name, composer, unit price, and length in seconds (where the database stores it in milliseconds): Haskell Postgres Sqlite do tracks <- all_ ( track chinookDb ) pure ( trackName tracks , trackComposer tracks , trackMilliseconds tracks ` div_ ` 1000 ) SELECT \"t0\" . \"Name\" AS \"res0\" , \"t0\" . \"Composer\" AS \"res1\" , ( \"t0\" . \"Milliseconds\" ) / ( 1000 ) AS \"res2\" FROM \"Track\" AS \"t0\" SELECT \"t0\" . \"Name\" AS \"res0\" , \"t0\" . \"Composer\" AS \"res1\" , ( \"t0\" . \"Milliseconds\" ) / ( ? ) AS \"res2\" FROM \"Track\" AS \"t0\" ; -- With values: [SQLInteger 1000] Beam includes instances to support returning up to 8-tuples. To return more, feel free to nest tuples. As an example, we can write the above query as Haskell Postgres Sqlite do tracks <- all_ ( track chinookDb ) pure (( trackName tracks , trackComposer tracks ), trackMilliseconds tracks ` div_ ` 1000 ) SELECT \"t0\" . \"Name\" AS \"res0\" , \"t0\" . \"Composer\" AS \"res1\" , ( \"t0\" . \"Milliseconds\" ) / ( 1000 ) AS \"res2\" FROM \"Track\" AS \"t0\" SELECT \"t0\" . \"Name\" AS \"res0\" , \"t0\" . \"Composer\" AS \"res1\" , ( \"t0\" . \"Milliseconds\" ) / ( ? ) AS \"res2\" FROM \"Track\" AS \"t0\" ; -- With values: [SQLInteger 1000] Notice that the nesting of tuples does not affect the generated SQL projection. The tuple structure is only used when reading back the row from the database. The Q monad is perfectly rule-abiding, which means it also implements a valid Functor instance. Thus the above could more easily be written. Haskell Postgres Sqlite fmap ( \\ tracks -> ( trackName tracks , trackComposer tracks , trackMilliseconds tracks ` div_ ` 1000 )) $ all_ ( track chinookDb ) SELECT \"t0\" . \"Name\" AS \"res0\" , \"t0\" . \"Composer\" AS \"res1\" , ( \"t0\" . \"Milliseconds\" ) / ( 1000 ) AS \"res2\" FROM \"Track\" AS \"t0\" SELECT \"t0\" . \"Name\" AS \"res0\" , \"t0\" . \"Composer\" AS \"res1\" , ( \"t0\" . \"Milliseconds\" ) / ( ? ) AS \"res2\" FROM \"Track\" AS \"t0\" ; -- With values: [SQLInteger 1000] WHERE clause We've seen how to use all_ to select all rows of a table. Sometimes, you would like to filter results based on the result of some condition. For example, perhaps you would like to fetch all customers whose names start with \"Jo\". We can filter over results using the filter_ function. Haskell Postgres Sqlite filter_ ( \\ customer -> customerFirstName customer ` like_ ` \"Jo%\" ) $ all_ ( customer chinookDb ) SELECT \"t0\" . \"CustomerId\" AS \"res0\" , \"t0\" . \"FirstName\" AS \"res1\" , \"t0\" . \"LastName\" AS \"res2\" , \"t0\" . \"Company\" AS \"res3\" , \"t0\" . \"Address\" AS \"res4\" , \"t0\" . \"City\" AS \"res5\" , \"t0\" . \"State\" AS \"res6\" , \"t0\" . \"Country\" AS \"res7\" , \"t0\" . \"PostalCode\" AS \"res8\" , \"t0\" . \"Phone\" AS \"res9\" , \"t0\" . \"Fax\" AS \"res10\" , \"t0\" . \"Email\" AS \"res11\" , \"t0\" . \"SupportRepId\" AS \"res12\" FROM \"Customer\" AS \"t0\" WHERE ( \"t0\" . \"FirstName\" ) LIKE ( 'Jo%' ) SELECT \"t0\" . \"CustomerId\" AS \"res0\" , \"t0\" . \"FirstName\" AS \"res1\" , \"t0\" . \"LastName\" AS \"res2\" , \"t0\" . \"Company\" AS \"res3\" , \"t0\" . \"Address\" AS \"res4\" , \"t0\" . \"City\" AS \"res5\" , \"t0\" . \"State\" AS \"res6\" , \"t0\" . \"Country\" AS \"res7\" , \"t0\" . \"PostalCode\" AS \"res8\" , \"t0\" . \"Phone\" AS \"res9\" , \"t0\" . \"Fax\" AS \"res10\" , \"t0\" . \"Email\" AS \"res11\" , \"t0\" . \"SupportRepId\" AS \"res12\" FROM \"Customer\" AS \"t0\" WHERE ( \"t0\" . \"FirstName\" ) LIKE ( ? ); -- With values: [SQLText \"Jo%\"] You can use (&&.) and (||.) to combine boolean expressions, as you'd expect. For example, to select all customers whose first name begins with \"Jo\", last name begins with \"S\", and who live in either California or Washington: Haskell Postgres Sqlite filter_ ( \\ customer -> (( customerFirstName customer ` like_ ` \"Jo%\" ) &&. ( customerLastName customer ` like_ ` \"S%\" )) &&. ( addressState ( customerAddress customer ) ==. just_ \"CA\" ||. addressState ( customerAddress customer ) ==. just_ \"WA\" )) $ all_ ( customer chinookDb ) SELECT \"t0\" . \"CustomerId\" AS \"res0\" , \"t0\" . \"FirstName\" AS \"res1\" , \"t0\" . \"LastName\" AS \"res2\" , \"t0\" . \"Company\" AS \"res3\" , \"t0\" . \"Address\" AS \"res4\" , \"t0\" . \"City\" AS \"res5\" , \"t0\" . \"State\" AS \"res6\" , \"t0\" . \"Country\" AS \"res7\" , \"t0\" . \"PostalCode\" AS \"res8\" , \"t0\" . \"Phone\" AS \"res9\" , \"t0\" . \"Fax\" AS \"res10\" , \"t0\" . \"Email\" AS \"res11\" , \"t0\" . \"SupportRepId\" AS \"res12\" FROM \"Customer\" AS \"t0\" WHERE ((( \"t0\" . \"FirstName\" ) LIKE ( 'Jo%' )) AND (( \"t0\" . \"LastName\" ) LIKE ( 'S%' ))) AND ((( \"t0\" . \"State\" ) IS NOT DISTINCT FROM ( 'CA' )) OR (( \"t0\" . \"State\" ) IS NOT DISTINCT FROM ( 'WA' ))) SELECT \"t0\" . \"CustomerId\" AS \"res0\" , \"t0\" . \"FirstName\" AS \"res1\" , \"t0\" . \"LastName\" AS \"res2\" , \"t0\" . \"Company\" AS \"res3\" , \"t0\" . \"Address\" AS \"res4\" , \"t0\" . \"City\" AS \"res5\" , \"t0\" . \"State\" AS \"res6\" , \"t0\" . \"Country\" AS \"res7\" , \"t0\" . \"PostalCode\" AS \"res8\" , \"t0\" . \"Phone\" AS \"res9\" , \"t0\" . \"Fax\" AS \"res10\" , \"t0\" . \"Email\" AS \"res11\" , \"t0\" . \"SupportRepId\" AS \"res12\" FROM \"Customer\" AS \"t0\" WHERE ((( \"t0\" . \"FirstName\" ) LIKE ( ? )) AND (( \"t0\" . \"LastName\" ) LIKE ( ? ))) AND (( CASE WHEN (( \"t0\" . \"State\" ) IS NULL ) AND (( ? ) IS NULL ) THEN ? WHEN (( \"t0\" . \"State\" ) IS NULL ) OR (( ? ) IS NULL ) THEN ? ELSE ( \"t0\" . \"State\" ) = ( ? ) END ) OR ( CASE WHEN (( \"t0\" . \"State\" ) IS NULL ) AND (( ? ) IS NULL ) THEN ? WHEN (( \"t0\" . \"State\" ) IS NULL ) OR (( ? ) IS NULL ) THEN ? ELSE ( \"t0\" . \"State\" ) = ( ? ) END )); -- With values: [SQLText \"Jo%\",SQLText \"S%\",SQLText \"CA\",SQLInteger 1,SQLText \"CA\",SQLInteger 0,SQLText \"CA\",SQLText \"WA\",SQLInteger 1,SQLText \"WA\",SQLInteger 0,SQLText \"WA\"] Note We had to use the just_ function above to compare addressState (customerAddress customer) . This is because addressState (customerAddress customer) represents a nullable column which beam types as Maybe Text . Just as in Haskell, we need to explicitly unwrap the Maybe type. This is an example of beam offering stronger typing than SQL itself. LIMIT / OFFSET support The limit_ and offset_ functions can be used to truncate the result set at a certain length and fetch different portions of the result. They correspond to the LIMIT and OFFSET SQL constructs. Haskell Postgres Sqlite limit_ 10 $ offset_ 100 $ filter_ ( \\ customer -> (( customerFirstName customer ` like_ ` \"Jo%\" ) &&. ( customerLastName customer ` like_ ` \"S%\" )) &&. ( addressState ( customerAddress customer ) ==. just_ \"CA\" ||. addressState ( customerAddress customer ) ==. just_ \"WA\" )) $ all_ ( customer chinookDb ) SELECT \"t0\" . \"CustomerId\" AS \"res0\" , \"t0\" . \"FirstName\" AS \"res1\" , \"t0\" . \"LastName\" AS \"res2\" , \"t0\" . \"Company\" AS \"res3\" , \"t0\" . \"Address\" AS \"res4\" , \"t0\" . \"City\" AS \"res5\" , \"t0\" . \"State\" AS \"res6\" , \"t0\" . \"Country\" AS \"res7\" , \"t0\" . \"PostalCode\" AS \"res8\" , \"t0\" . \"Phone\" AS \"res9\" , \"t0\" . \"Fax\" AS \"res10\" , \"t0\" . \"Email\" AS \"res11\" , \"t0\" . \"SupportRepId\" AS \"res12\" FROM \"Customer\" AS \"t0\" WHERE ((( \"t0\" . \"FirstName\" ) LIKE ( 'Jo%' )) AND (( \"t0\" . \"LastName\" ) LIKE ( 'S%' ))) AND ((( \"t0\" . \"State\" ) IS NOT DISTINCT FROM ( 'CA' )) OR (( \"t0\" . \"State\" ) IS NOT DISTINCT FROM ( 'WA' ))) LIMIT 10 OFFSET 100 SELECT \"t0\" . \"CustomerId\" AS \"res0\" , \"t0\" . \"FirstName\" AS \"res1\" , \"t0\" . \"LastName\" AS \"res2\" , \"t0\" . \"Company\" AS \"res3\" , \"t0\" . \"Address\" AS \"res4\" , \"t0\" . \"City\" AS \"res5\" , \"t0\" . \"State\" AS \"res6\" , \"t0\" . \"Country\" AS \"res7\" , \"t0\" . \"PostalCode\" AS \"res8\" , \"t0\" . \"Phone\" AS \"res9\" , \"t0\" . \"Fax\" AS \"res10\" , \"t0\" . \"Email\" AS \"res11\" , \"t0\" . \"SupportRepId\" AS \"res12\" FROM \"Customer\" AS \"t0\" WHERE ((( \"t0\" . \"FirstName\" ) LIKE ( ? )) AND (( \"t0\" . \"LastName\" ) LIKE ( ? ))) AND (( CASE WHEN (( \"t0\" . \"State\" ) IS NULL ) AND (( ? ) IS NULL ) THEN ? WHEN (( \"t0\" . \"State\" ) IS NULL ) OR (( ? ) IS NULL ) THEN ? ELSE ( \"t0\" . \"State\" ) = ( ? ) END ) OR ( CASE WHEN (( \"t0\" . \"State\" ) IS NULL ) AND (( ? ) IS NULL ) THEN ? WHEN (( \"t0\" . \"State\" ) IS NULL ) OR (( ? ) IS NULL ) THEN ? ELSE ( \"t0\" . \"State\" ) = ( ? ) END )) LIMIT 10 OFFSET 100 ; -- With values: [SQLText \"Jo%\",SQLText \"S%\",SQLText \"CA\",SQLInteger 1,SQLText \"CA\",SQLInteger 0,SQLText \"CA\",SQLText \"WA\",SQLInteger 1,SQLText \"WA\",SQLInteger 0,SQLText \"WA\"] Note Nested limit_ s and offset_ s compose in the way you'd expect without generating extraneous subqueries. Warning Note that the order of the limit_ and offset_ functions matter. Offseting an already limited result is not the same as limiting an offseted result. For example, if you offset three rows into a limited set of five results, you will get at most two rows. On the other hand, if you offset three rows and then limit the result to the next five, you may get up to five. Beam will generate exactly the query you specify. Notice the difference below, where the order of the clauses made beam generate a query that returns no results. Haskell Postgres Sqlite offset_ 100 $ limit_ 10 $ filter_ ( \\ customer -> (( customerFirstName customer ` like_ ` \"Jo%\" ) &&. ( customerLastName customer ` like_ ` \"S%\" )) &&. ( addressState ( customerAddress customer ) ==. just_ \"CA\" ||. addressState ( customerAddress customer ) ==. just_ \"WA\" )) $ all_ ( customer chinookDb ) SELECT \"t0\" . \"CustomerId\" AS \"res0\" , \"t0\" . \"FirstName\" AS \"res1\" , \"t0\" . \"LastName\" AS \"res2\" , \"t0\" . \"Company\" AS \"res3\" , \"t0\" . \"Address\" AS \"res4\" , \"t0\" . \"City\" AS \"res5\" , \"t0\" . \"State\" AS \"res6\" , \"t0\" . \"Country\" AS \"res7\" , \"t0\" . \"PostalCode\" AS \"res8\" , \"t0\" . \"Phone\" AS \"res9\" , \"t0\" . \"Fax\" AS \"res10\" , \"t0\" . \"Email\" AS \"res11\" , \"t0\" . \"SupportRepId\" AS \"res12\" FROM \"Customer\" AS \"t0\" WHERE ((( \"t0\" . \"FirstName\" ) LIKE ( 'Jo%' )) AND (( \"t0\" . \"LastName\" ) LIKE ( 'S%' ))) AND ((( \"t0\" . \"State\" ) IS NOT DISTINCT FROM ( 'CA' )) OR (( \"t0\" . \"State\" ) IS NOT DISTINCT FROM ( 'WA' ))) LIMIT 0 OFFSET 100 SELECT \"t0\" . \"CustomerId\" AS \"res0\" , \"t0\" . \"FirstName\" AS \"res1\" , \"t0\" . \"LastName\" AS \"res2\" , \"t0\" . \"Company\" AS \"res3\" , \"t0\" . \"Address\" AS \"res4\" , \"t0\" . \"City\" AS \"res5\" , \"t0\" . \"State\" AS \"res6\" , \"t0\" . \"Country\" AS \"res7\" , \"t0\" . \"PostalCode\" AS \"res8\" , \"t0\" . \"Phone\" AS \"res9\" , \"t0\" . \"Fax\" AS \"res10\" , \"t0\" . \"Email\" AS \"res11\" , \"t0\" . \"SupportRepId\" AS \"res12\" FROM \"Customer\" AS \"t0\" WHERE ((( \"t0\" . \"FirstName\" ) LIKE ( ? )) AND (( \"t0\" . \"LastName\" ) LIKE ( ? ))) AND (( CASE WHEN (( \"t0\" . \"State\" ) IS NULL ) AND (( ? ) IS NULL ) THEN ? WHEN (( \"t0\" . \"State\" ) IS NULL ) OR (( ? ) IS NULL ) THEN ? ELSE ( \"t0\" . \"State\" ) = ( ? ) END ) OR ( CASE WHEN (( \"t0\" . \"State\" ) IS NULL ) AND (( ? ) IS NULL ) THEN ? WHEN (( \"t0\" . \"State\" ) IS NULL ) OR (( ? ) IS NULL ) THEN ? ELSE ( \"t0\" . \"State\" ) = ( ? ) END )) LIMIT 0 OFFSET 100 ; -- With values: [SQLText \"Jo%\",SQLText \"S%\",SQLText \"CA\",SQLInteger 1,SQLText \"CA\",SQLInteger 0,SQLText \"CA\",SQLText \"WA\",SQLInteger 1,SQLText \"WA\",SQLInteger 0,SQLText \"WA\"] Backends often differ as to how they implement LIMIT/OFFSET. For example, SQLite requires that LIMIT always be given if an OFFSET is provided. Beam correctly handles this behavior. Haskell Postgres Sqlite offset_ 100 $ filter_ ( \\ customer -> (( customerFirstName customer ` like_ ` \"Jo%\" ) &&. ( customerLastName customer ` like_ ` \"S%\" )) &&. ( addressState ( customerAddress customer ) ==. just_ \"CA\" ||. addressState ( customerAddress customer ) ==. just_ \"WA\" )) $ all_ ( customer chinookDb ) SELECT \"t0\" . \"CustomerId\" AS \"res0\" , \"t0\" . \"FirstName\" AS \"res1\" , \"t0\" . \"LastName\" AS \"res2\" , \"t0\" . \"Company\" AS \"res3\" , \"t0\" . \"Address\" AS \"res4\" , \"t0\" . \"City\" AS \"res5\" , \"t0\" . \"State\" AS \"res6\" , \"t0\" . \"Country\" AS \"res7\" , \"t0\" . \"PostalCode\" AS \"res8\" , \"t0\" . \"Phone\" AS \"res9\" , \"t0\" . \"Fax\" AS \"res10\" , \"t0\" . \"Email\" AS \"res11\" , \"t0\" . \"SupportRepId\" AS \"res12\" FROM \"Customer\" AS \"t0\" WHERE ((( \"t0\" . \"FirstName\" ) LIKE ( 'Jo%' )) AND (( \"t0\" . \"LastName\" ) LIKE ( 'S%' ))) AND ((( \"t0\" . \"State\" ) IS NOT DISTINCT FROM ( 'CA' )) OR (( \"t0\" . \"State\" ) IS NOT DISTINCT FROM ( 'WA' ))) OFFSET 100 SELECT \"t0\" . \"CustomerId\" AS \"res0\" , \"t0\" . \"FirstName\" AS \"res1\" , \"t0\" . \"LastName\" AS \"res2\" , \"t0\" . \"Company\" AS \"res3\" , \"t0\" . \"Address\" AS \"res4\" , \"t0\" . \"City\" AS \"res5\" , \"t0\" . \"State\" AS \"res6\" , \"t0\" . \"Country\" AS \"res7\" , \"t0\" . \"PostalCode\" AS \"res8\" , \"t0\" . \"Phone\" AS \"res9\" , \"t0\" . \"Fax\" AS \"res10\" , \"t0\" . \"Email\" AS \"res11\" , \"t0\" . \"SupportRepId\" AS \"res12\" FROM \"Customer\" AS \"t0\" WHERE ((( \"t0\" . \"FirstName\" ) LIKE ( ? )) AND (( \"t0\" . \"LastName\" ) LIKE ( ? ))) AND (( CASE WHEN (( \"t0\" . \"State\" ) IS NULL ) AND (( ? ) IS NULL ) THEN ? WHEN (( \"t0\" . \"State\" ) IS NULL ) OR (( ? ) IS NULL ) THEN ? ELSE ( \"t0\" . \"State\" ) = ( ? ) END ) OR ( CASE WHEN (( \"t0\" . \"State\" ) IS NULL ) AND (( ? ) IS NULL ) THEN ? WHEN (( \"t0\" . \"State\" ) IS NULL ) OR (( ? ) IS NULL ) THEN ? ELSE ( \"t0\" . \"State\" ) = ( ? ) END )) LIMIT - 1 OFFSET 100 ; -- With values: [SQLText \"Jo%\",SQLText \"S%\",SQLText \"CA\",SQLInteger 1,SQLText \"CA\",SQLInteger 0,SQLText \"CA\",SQLText \"WA\",SQLInteger 1,SQLText \"WA\",SQLInteger 0,SQLText \"WA\"] Notice that the SQLite query output has provided a dummy LIMIT -1 clause, while the Postgres query has not. DISTINCT support SQL can only return unique results from a query through the SELECT DISTINCT statement. Beam supports this using the nub_ command. For example, to get all the unique postal codes where our customers live. Haskell Postgres Sqlite nub_ $ fmap ( addressPostalCode . customerAddress ) $ all_ ( customer chinookDb ) SELECT DISTINCT \"t0\" . \"PostalCode\" AS \"res0\" FROM \"Customer\" AS \"t0\" SELECT DISTINCT \"t0\" . \"PostalCode\" AS \"res0\" FROM \"Customer\" AS \"t0\" ; -- With values: [] VALUES support Sometimes you want to select from an explicit group of values. This is most helpful if you want to join against a set of values that isn't in the database. For example, to get all customers we know to be in New York, California, and Texas. Haskell Postgres do c <- all_ ( customer chinookDb ) st <- values_ [ \"NY\" , \"CA\" , \"TX\" ] guard_' ( just_ st ==?. addressState ( customerAddress c )) pure c SELECT \"t0\" . \"CustomerId\" AS \"res0\" , \"t0\" . \"FirstName\" AS \"res1\" , \"t0\" . \"LastName\" AS \"res2\" , \"t0\" . \"Company\" AS \"res3\" , \"t0\" . \"Address\" AS \"res4\" , \"t0\" . \"City\" AS \"res5\" , \"t0\" . \"State\" AS \"res6\" , \"t0\" . \"Country\" AS \"res7\" , \"t0\" . \"PostalCode\" AS \"res8\" , \"t0\" . \"Phone\" AS \"res9\" , \"t0\" . \"Fax\" AS \"res10\" , \"t0\" . \"Email\" AS \"res11\" , \"t0\" . \"SupportRepId\" AS \"res12\" FROM \"Customer\" AS \"t0\" CROSS JOIN ( VALUES ( 'NY' ), ( 'CA' ), ( 'TX' )) AS \"t1\" ( \"res0\" ) WHERE ( \"t1\" . \"res0\" ) = ( \"t0\" . \"State\" ) Note beam-sqlite does not support VALUES clauses anywhere within a query, but only within a common table expression. Ad-hoc queries Sometimes you want to quickly query a database without having to write out all the boilerplate. Beam supports this with a feature called 'ad-hoc' queries. For example, to get all the names of customers, without having to use chinookDb at all. To use this functionality, import Database.Beam.Query.Adhoc . Note that we use TypeApplications to give each field an explicit type. If you don't, GHC will often infer the type, but it's nice to be explicit. Haskell Postgres Sqlite -- import qualified Database.Beam.Query.Adhoc as Adhoc do ( cId , firstName , lastName ) <- Adhoc . table_ Nothing {- Schema Name -} \"Customer\" ( Adhoc . field_ @ Int32 \"CustomerId\" , Adhoc . field_ @ Text \"FirstName\" , Adhoc . field_ @ Text \"LastName\" ) guard_ ( firstName ` like_ ` \"Jo%\" ) return ( cId , firstName , lastName ) SELECT \"t0\" . \"CustomerId\" AS \"res0\" , \"t0\" . \"FirstName\" AS \"res1\" , \"t0\" . \"LastName\" AS \"res2\" FROM \"Customer\" AS \"t0\" WHERE ( \"t0\" . \"FirstName\" ) LIKE ( 'Jo%' ) SELECT \"t0\" . \"CustomerId\" AS \"res0\" , \"t0\" . \"FirstName\" AS \"res1\" , \"t0\" . \"LastName\" AS \"res2\" FROM \"Customer\" AS \"t0\" WHERE ( \"t0\" . \"FirstName\" ) LIKE ( ? ); -- With values: [SQLText \"Jo%\"]","title":"More complex SELECTs"},{"location":"user-guide/queries/select/#returning-a-subset-of-columns","text":"Oftentimes we only care about the value of a few columns, rather than every column in the table. Beam fully supports taking projections of tables. As said before, Q is a Monad . Thus, we can use monadic do notation to only select a certain subset of columns. For example, to fetch only the name of every track: Haskell Postgres Sqlite do tracks <- all_ ( track chinookDb ) pure ( trackName tracks ) SELECT \"t0\" . \"Name\" AS \"res0\" FROM \"Track\" AS \"t0\" SELECT \"t0\" . \"Name\" AS \"res0\" FROM \"Track\" AS \"t0\" ; -- With values: [] Notice that beam has properly written the SELECT projection to only include the Name field. We can also return multiple fields, by returning a tuple. Perhaps we would also like to know the composer: Haskell Postgres Sqlite do tracks <- all_ ( track chinookDb ) pure ( trackName tracks , trackComposer tracks ) SELECT \"t0\" . \"Name\" AS \"res0\" , \"t0\" . \"Composer\" AS \"res1\" FROM \"Track\" AS \"t0\" SELECT \"t0\" . \"Name\" AS \"res0\" , \"t0\" . \"Composer\" AS \"res1\" FROM \"Track\" AS \"t0\" ; -- With values: [] You can also return arbitrary expressions in the projection. For example to return the name, composer, unit price, and length in seconds (where the database stores it in milliseconds): Haskell Postgres Sqlite do tracks <- all_ ( track chinookDb ) pure ( trackName tracks , trackComposer tracks , trackMilliseconds tracks ` div_ ` 1000 ) SELECT \"t0\" . \"Name\" AS \"res0\" , \"t0\" . \"Composer\" AS \"res1\" , ( \"t0\" . \"Milliseconds\" ) / ( 1000 ) AS \"res2\" FROM \"Track\" AS \"t0\" SELECT \"t0\" . \"Name\" AS \"res0\" , \"t0\" . \"Composer\" AS \"res1\" , ( \"t0\" . \"Milliseconds\" ) / ( ? ) AS \"res2\" FROM \"Track\" AS \"t0\" ; -- With values: [SQLInteger 1000] Beam includes instances to support returning up to 8-tuples. To return more, feel free to nest tuples. As an example, we can write the above query as Haskell Postgres Sqlite do tracks <- all_ ( track chinookDb ) pure (( trackName tracks , trackComposer tracks ), trackMilliseconds tracks ` div_ ` 1000 ) SELECT \"t0\" . \"Name\" AS \"res0\" , \"t0\" . \"Composer\" AS \"res1\" , ( \"t0\" . \"Milliseconds\" ) / ( 1000 ) AS \"res2\" FROM \"Track\" AS \"t0\" SELECT \"t0\" . \"Name\" AS \"res0\" , \"t0\" . \"Composer\" AS \"res1\" , ( \"t0\" . \"Milliseconds\" ) / ( ? ) AS \"res2\" FROM \"Track\" AS \"t0\" ; -- With values: [SQLInteger 1000] Notice that the nesting of tuples does not affect the generated SQL projection. The tuple structure is only used when reading back the row from the database. The Q monad is perfectly rule-abiding, which means it also implements a valid Functor instance. Thus the above could more easily be written. Haskell Postgres Sqlite fmap ( \\ tracks -> ( trackName tracks , trackComposer tracks , trackMilliseconds tracks ` div_ ` 1000 )) $ all_ ( track chinookDb ) SELECT \"t0\" . \"Name\" AS \"res0\" , \"t0\" . \"Composer\" AS \"res1\" , ( \"t0\" . \"Milliseconds\" ) / ( 1000 ) AS \"res2\" FROM \"Track\" AS \"t0\" SELECT \"t0\" . \"Name\" AS \"res0\" , \"t0\" . \"Composer\" AS \"res1\" , ( \"t0\" . \"Milliseconds\" ) / ( ? ) AS \"res2\" FROM \"Track\" AS \"t0\" ; -- With values: [SQLInteger 1000]","title":"Returning a subset of columns"},{"location":"user-guide/queries/select/#where-clause","text":"We've seen how to use all_ to select all rows of a table. Sometimes, you would like to filter results based on the result of some condition. For example, perhaps you would like to fetch all customers whose names start with \"Jo\". We can filter over results using the filter_ function. Haskell Postgres Sqlite filter_ ( \\ customer -> customerFirstName customer ` like_ ` \"Jo%\" ) $ all_ ( customer chinookDb ) SELECT \"t0\" . \"CustomerId\" AS \"res0\" , \"t0\" . \"FirstName\" AS \"res1\" , \"t0\" . \"LastName\" AS \"res2\" , \"t0\" . \"Company\" AS \"res3\" , \"t0\" . \"Address\" AS \"res4\" , \"t0\" . \"City\" AS \"res5\" , \"t0\" . \"State\" AS \"res6\" , \"t0\" . \"Country\" AS \"res7\" , \"t0\" . \"PostalCode\" AS \"res8\" , \"t0\" . \"Phone\" AS \"res9\" , \"t0\" . \"Fax\" AS \"res10\" , \"t0\" . \"Email\" AS \"res11\" , \"t0\" . \"SupportRepId\" AS \"res12\" FROM \"Customer\" AS \"t0\" WHERE ( \"t0\" . \"FirstName\" ) LIKE ( 'Jo%' ) SELECT \"t0\" . \"CustomerId\" AS \"res0\" , \"t0\" . \"FirstName\" AS \"res1\" , \"t0\" . \"LastName\" AS \"res2\" , \"t0\" . \"Company\" AS \"res3\" , \"t0\" . \"Address\" AS \"res4\" , \"t0\" . \"City\" AS \"res5\" , \"t0\" . \"State\" AS \"res6\" , \"t0\" . \"Country\" AS \"res7\" , \"t0\" . \"PostalCode\" AS \"res8\" , \"t0\" . \"Phone\" AS \"res9\" , \"t0\" . \"Fax\" AS \"res10\" , \"t0\" . \"Email\" AS \"res11\" , \"t0\" . \"SupportRepId\" AS \"res12\" FROM \"Customer\" AS \"t0\" WHERE ( \"t0\" . \"FirstName\" ) LIKE ( ? ); -- With values: [SQLText \"Jo%\"] You can use (&&.) and (||.) to combine boolean expressions, as you'd expect. For example, to select all customers whose first name begins with \"Jo\", last name begins with \"S\", and who live in either California or Washington: Haskell Postgres Sqlite filter_ ( \\ customer -> (( customerFirstName customer ` like_ ` \"Jo%\" ) &&. ( customerLastName customer ` like_ ` \"S%\" )) &&. ( addressState ( customerAddress customer ) ==. just_ \"CA\" ||. addressState ( customerAddress customer ) ==. just_ \"WA\" )) $ all_ ( customer chinookDb ) SELECT \"t0\" . \"CustomerId\" AS \"res0\" , \"t0\" . \"FirstName\" AS \"res1\" , \"t0\" . \"LastName\" AS \"res2\" , \"t0\" . \"Company\" AS \"res3\" , \"t0\" . \"Address\" AS \"res4\" , \"t0\" . \"City\" AS \"res5\" , \"t0\" . \"State\" AS \"res6\" , \"t0\" . \"Country\" AS \"res7\" , \"t0\" . \"PostalCode\" AS \"res8\" , \"t0\" . \"Phone\" AS \"res9\" , \"t0\" . \"Fax\" AS \"res10\" , \"t0\" . \"Email\" AS \"res11\" , \"t0\" . \"SupportRepId\" AS \"res12\" FROM \"Customer\" AS \"t0\" WHERE ((( \"t0\" . \"FirstName\" ) LIKE ( 'Jo%' )) AND (( \"t0\" . \"LastName\" ) LIKE ( 'S%' ))) AND ((( \"t0\" . \"State\" ) IS NOT DISTINCT FROM ( 'CA' )) OR (( \"t0\" . \"State\" ) IS NOT DISTINCT FROM ( 'WA' ))) SELECT \"t0\" . \"CustomerId\" AS \"res0\" , \"t0\" . \"FirstName\" AS \"res1\" , \"t0\" . \"LastName\" AS \"res2\" , \"t0\" . \"Company\" AS \"res3\" , \"t0\" . \"Address\" AS \"res4\" , \"t0\" . \"City\" AS \"res5\" , \"t0\" . \"State\" AS \"res6\" , \"t0\" . \"Country\" AS \"res7\" , \"t0\" . \"PostalCode\" AS \"res8\" , \"t0\" . \"Phone\" AS \"res9\" , \"t0\" . \"Fax\" AS \"res10\" , \"t0\" . \"Email\" AS \"res11\" , \"t0\" . \"SupportRepId\" AS \"res12\" FROM \"Customer\" AS \"t0\" WHERE ((( \"t0\" . \"FirstName\" ) LIKE ( ? )) AND (( \"t0\" . \"LastName\" ) LIKE ( ? ))) AND (( CASE WHEN (( \"t0\" . \"State\" ) IS NULL ) AND (( ? ) IS NULL ) THEN ? WHEN (( \"t0\" . \"State\" ) IS NULL ) OR (( ? ) IS NULL ) THEN ? ELSE ( \"t0\" . \"State\" ) = ( ? ) END ) OR ( CASE WHEN (( \"t0\" . \"State\" ) IS NULL ) AND (( ? ) IS NULL ) THEN ? WHEN (( \"t0\" . \"State\" ) IS NULL ) OR (( ? ) IS NULL ) THEN ? ELSE ( \"t0\" . \"State\" ) = ( ? ) END )); -- With values: [SQLText \"Jo%\",SQLText \"S%\",SQLText \"CA\",SQLInteger 1,SQLText \"CA\",SQLInteger 0,SQLText \"CA\",SQLText \"WA\",SQLInteger 1,SQLText \"WA\",SQLInteger 0,SQLText \"WA\"] Note We had to use the just_ function above to compare addressState (customerAddress customer) . This is because addressState (customerAddress customer) represents a nullable column which beam types as Maybe Text . Just as in Haskell, we need to explicitly unwrap the Maybe type. This is an example of beam offering stronger typing than SQL itself.","title":"WHERE clause"},{"location":"user-guide/queries/select/#limitoffset-support","text":"The limit_ and offset_ functions can be used to truncate the result set at a certain length and fetch different portions of the result. They correspond to the LIMIT and OFFSET SQL constructs. Haskell Postgres Sqlite limit_ 10 $ offset_ 100 $ filter_ ( \\ customer -> (( customerFirstName customer ` like_ ` \"Jo%\" ) &&. ( customerLastName customer ` like_ ` \"S%\" )) &&. ( addressState ( customerAddress customer ) ==. just_ \"CA\" ||. addressState ( customerAddress customer ) ==. just_ \"WA\" )) $ all_ ( customer chinookDb ) SELECT \"t0\" . \"CustomerId\" AS \"res0\" , \"t0\" . \"FirstName\" AS \"res1\" , \"t0\" . \"LastName\" AS \"res2\" , \"t0\" . \"Company\" AS \"res3\" , \"t0\" . \"Address\" AS \"res4\" , \"t0\" . \"City\" AS \"res5\" , \"t0\" . \"State\" AS \"res6\" , \"t0\" . \"Country\" AS \"res7\" , \"t0\" . \"PostalCode\" AS \"res8\" , \"t0\" . \"Phone\" AS \"res9\" , \"t0\" . \"Fax\" AS \"res10\" , \"t0\" . \"Email\" AS \"res11\" , \"t0\" . \"SupportRepId\" AS \"res12\" FROM \"Customer\" AS \"t0\" WHERE ((( \"t0\" . \"FirstName\" ) LIKE ( 'Jo%' )) AND (( \"t0\" . \"LastName\" ) LIKE ( 'S%' ))) AND ((( \"t0\" . \"State\" ) IS NOT DISTINCT FROM ( 'CA' )) OR (( \"t0\" . \"State\" ) IS NOT DISTINCT FROM ( 'WA' ))) LIMIT 10 OFFSET 100 SELECT \"t0\" . \"CustomerId\" AS \"res0\" , \"t0\" . \"FirstName\" AS \"res1\" , \"t0\" . \"LastName\" AS \"res2\" , \"t0\" . \"Company\" AS \"res3\" , \"t0\" . \"Address\" AS \"res4\" , \"t0\" . \"City\" AS \"res5\" , \"t0\" . \"State\" AS \"res6\" , \"t0\" . \"Country\" AS \"res7\" , \"t0\" . \"PostalCode\" AS \"res8\" , \"t0\" . \"Phone\" AS \"res9\" , \"t0\" . \"Fax\" AS \"res10\" , \"t0\" . \"Email\" AS \"res11\" , \"t0\" . \"SupportRepId\" AS \"res12\" FROM \"Customer\" AS \"t0\" WHERE ((( \"t0\" . \"FirstName\" ) LIKE ( ? )) AND (( \"t0\" . \"LastName\" ) LIKE ( ? ))) AND (( CASE WHEN (( \"t0\" . \"State\" ) IS NULL ) AND (( ? ) IS NULL ) THEN ? WHEN (( \"t0\" . \"State\" ) IS NULL ) OR (( ? ) IS NULL ) THEN ? ELSE ( \"t0\" . \"State\" ) = ( ? ) END ) OR ( CASE WHEN (( \"t0\" . \"State\" ) IS NULL ) AND (( ? ) IS NULL ) THEN ? WHEN (( \"t0\" . \"State\" ) IS NULL ) OR (( ? ) IS NULL ) THEN ? ELSE ( \"t0\" . \"State\" ) = ( ? ) END )) LIMIT 10 OFFSET 100 ; -- With values: [SQLText \"Jo%\",SQLText \"S%\",SQLText \"CA\",SQLInteger 1,SQLText \"CA\",SQLInteger 0,SQLText \"CA\",SQLText \"WA\",SQLInteger 1,SQLText \"WA\",SQLInteger 0,SQLText \"WA\"] Note Nested limit_ s and offset_ s compose in the way you'd expect without generating extraneous subqueries. Warning Note that the order of the limit_ and offset_ functions matter. Offseting an already limited result is not the same as limiting an offseted result. For example, if you offset three rows into a limited set of five results, you will get at most two rows. On the other hand, if you offset three rows and then limit the result to the next five, you may get up to five. Beam will generate exactly the query you specify. Notice the difference below, where the order of the clauses made beam generate a query that returns no results. Haskell Postgres Sqlite offset_ 100 $ limit_ 10 $ filter_ ( \\ customer -> (( customerFirstName customer ` like_ ` \"Jo%\" ) &&. ( customerLastName customer ` like_ ` \"S%\" )) &&. ( addressState ( customerAddress customer ) ==. just_ \"CA\" ||. addressState ( customerAddress customer ) ==. just_ \"WA\" )) $ all_ ( customer chinookDb ) SELECT \"t0\" . \"CustomerId\" AS \"res0\" , \"t0\" . \"FirstName\" AS \"res1\" , \"t0\" . \"LastName\" AS \"res2\" , \"t0\" . \"Company\" AS \"res3\" , \"t0\" . \"Address\" AS \"res4\" , \"t0\" . \"City\" AS \"res5\" , \"t0\" . \"State\" AS \"res6\" , \"t0\" . \"Country\" AS \"res7\" , \"t0\" . \"PostalCode\" AS \"res8\" , \"t0\" . \"Phone\" AS \"res9\" , \"t0\" . \"Fax\" AS \"res10\" , \"t0\" . \"Email\" AS \"res11\" , \"t0\" . \"SupportRepId\" AS \"res12\" FROM \"Customer\" AS \"t0\" WHERE ((( \"t0\" . \"FirstName\" ) LIKE ( 'Jo%' )) AND (( \"t0\" . \"LastName\" ) LIKE ( 'S%' ))) AND ((( \"t0\" . \"State\" ) IS NOT DISTINCT FROM ( 'CA' )) OR (( \"t0\" . \"State\" ) IS NOT DISTINCT FROM ( 'WA' ))) LIMIT 0 OFFSET 100 SELECT \"t0\" . \"CustomerId\" AS \"res0\" , \"t0\" . \"FirstName\" AS \"res1\" , \"t0\" . \"LastName\" AS \"res2\" , \"t0\" . \"Company\" AS \"res3\" , \"t0\" . \"Address\" AS \"res4\" , \"t0\" . \"City\" AS \"res5\" , \"t0\" . \"State\" AS \"res6\" , \"t0\" . \"Country\" AS \"res7\" , \"t0\" . \"PostalCode\" AS \"res8\" , \"t0\" . \"Phone\" AS \"res9\" , \"t0\" . \"Fax\" AS \"res10\" , \"t0\" . \"Email\" AS \"res11\" , \"t0\" . \"SupportRepId\" AS \"res12\" FROM \"Customer\" AS \"t0\" WHERE ((( \"t0\" . \"FirstName\" ) LIKE ( ? )) AND (( \"t0\" . \"LastName\" ) LIKE ( ? ))) AND (( CASE WHEN (( \"t0\" . \"State\" ) IS NULL ) AND (( ? ) IS NULL ) THEN ? WHEN (( \"t0\" . \"State\" ) IS NULL ) OR (( ? ) IS NULL ) THEN ? ELSE ( \"t0\" . \"State\" ) = ( ? ) END ) OR ( CASE WHEN (( \"t0\" . \"State\" ) IS NULL ) AND (( ? ) IS NULL ) THEN ? WHEN (( \"t0\" . \"State\" ) IS NULL ) OR (( ? ) IS NULL ) THEN ? ELSE ( \"t0\" . \"State\" ) = ( ? ) END )) LIMIT 0 OFFSET 100 ; -- With values: [SQLText \"Jo%\",SQLText \"S%\",SQLText \"CA\",SQLInteger 1,SQLText \"CA\",SQLInteger 0,SQLText \"CA\",SQLText \"WA\",SQLInteger 1,SQLText \"WA\",SQLInteger 0,SQLText \"WA\"] Backends often differ as to how they implement LIMIT/OFFSET. For example, SQLite requires that LIMIT always be given if an OFFSET is provided. Beam correctly handles this behavior. Haskell Postgres Sqlite offset_ 100 $ filter_ ( \\ customer -> (( customerFirstName customer ` like_ ` \"Jo%\" ) &&. ( customerLastName customer ` like_ ` \"S%\" )) &&. ( addressState ( customerAddress customer ) ==. just_ \"CA\" ||. addressState ( customerAddress customer ) ==. just_ \"WA\" )) $ all_ ( customer chinookDb ) SELECT \"t0\" . \"CustomerId\" AS \"res0\" , \"t0\" . \"FirstName\" AS \"res1\" , \"t0\" . \"LastName\" AS \"res2\" , \"t0\" . \"Company\" AS \"res3\" , \"t0\" . \"Address\" AS \"res4\" , \"t0\" . \"City\" AS \"res5\" , \"t0\" . \"State\" AS \"res6\" , \"t0\" . \"Country\" AS \"res7\" , \"t0\" . \"PostalCode\" AS \"res8\" , \"t0\" . \"Phone\" AS \"res9\" , \"t0\" . \"Fax\" AS \"res10\" , \"t0\" . \"Email\" AS \"res11\" , \"t0\" . \"SupportRepId\" AS \"res12\" FROM \"Customer\" AS \"t0\" WHERE ((( \"t0\" . \"FirstName\" ) LIKE ( 'Jo%' )) AND (( \"t0\" . \"LastName\" ) LIKE ( 'S%' ))) AND ((( \"t0\" . \"State\" ) IS NOT DISTINCT FROM ( 'CA' )) OR (( \"t0\" . \"State\" ) IS NOT DISTINCT FROM ( 'WA' ))) OFFSET 100 SELECT \"t0\" . \"CustomerId\" AS \"res0\" , \"t0\" . \"FirstName\" AS \"res1\" , \"t0\" . \"LastName\" AS \"res2\" , \"t0\" . \"Company\" AS \"res3\" , \"t0\" . \"Address\" AS \"res4\" , \"t0\" . \"City\" AS \"res5\" , \"t0\" . \"State\" AS \"res6\" , \"t0\" . \"Country\" AS \"res7\" , \"t0\" . \"PostalCode\" AS \"res8\" , \"t0\" . \"Phone\" AS \"res9\" , \"t0\" . \"Fax\" AS \"res10\" , \"t0\" . \"Email\" AS \"res11\" , \"t0\" . \"SupportRepId\" AS \"res12\" FROM \"Customer\" AS \"t0\" WHERE ((( \"t0\" . \"FirstName\" ) LIKE ( ? )) AND (( \"t0\" . \"LastName\" ) LIKE ( ? ))) AND (( CASE WHEN (( \"t0\" . \"State\" ) IS NULL ) AND (( ? ) IS NULL ) THEN ? WHEN (( \"t0\" . \"State\" ) IS NULL ) OR (( ? ) IS NULL ) THEN ? ELSE ( \"t0\" . \"State\" ) = ( ? ) END ) OR ( CASE WHEN (( \"t0\" . \"State\" ) IS NULL ) AND (( ? ) IS NULL ) THEN ? WHEN (( \"t0\" . \"State\" ) IS NULL ) OR (( ? ) IS NULL ) THEN ? ELSE ( \"t0\" . \"State\" ) = ( ? ) END )) LIMIT - 1 OFFSET 100 ; -- With values: [SQLText \"Jo%\",SQLText \"S%\",SQLText \"CA\",SQLInteger 1,SQLText \"CA\",SQLInteger 0,SQLText \"CA\",SQLText \"WA\",SQLInteger 1,SQLText \"WA\",SQLInteger 0,SQLText \"WA\"] Notice that the SQLite query output has provided a dummy LIMIT -1 clause, while the Postgres query has not.","title":"LIMIT/OFFSET support"},{"location":"user-guide/queries/select/#distinct-support","text":"SQL can only return unique results from a query through the SELECT DISTINCT statement. Beam supports this using the nub_ command. For example, to get all the unique postal codes where our customers live. Haskell Postgres Sqlite nub_ $ fmap ( addressPostalCode . customerAddress ) $ all_ ( customer chinookDb ) SELECT DISTINCT \"t0\" . \"PostalCode\" AS \"res0\" FROM \"Customer\" AS \"t0\" SELECT DISTINCT \"t0\" . \"PostalCode\" AS \"res0\" FROM \"Customer\" AS \"t0\" ; -- With values: []","title":"DISTINCT support"},{"location":"user-guide/queries/select/#values-support","text":"Sometimes you want to select from an explicit group of values. This is most helpful if you want to join against a set of values that isn't in the database. For example, to get all customers we know to be in New York, California, and Texas. Haskell Postgres do c <- all_ ( customer chinookDb ) st <- values_ [ \"NY\" , \"CA\" , \"TX\" ] guard_' ( just_ st ==?. addressState ( customerAddress c )) pure c SELECT \"t0\" . \"CustomerId\" AS \"res0\" , \"t0\" . \"FirstName\" AS \"res1\" , \"t0\" . \"LastName\" AS \"res2\" , \"t0\" . \"Company\" AS \"res3\" , \"t0\" . \"Address\" AS \"res4\" , \"t0\" . \"City\" AS \"res5\" , \"t0\" . \"State\" AS \"res6\" , \"t0\" . \"Country\" AS \"res7\" , \"t0\" . \"PostalCode\" AS \"res8\" , \"t0\" . \"Phone\" AS \"res9\" , \"t0\" . \"Fax\" AS \"res10\" , \"t0\" . \"Email\" AS \"res11\" , \"t0\" . \"SupportRepId\" AS \"res12\" FROM \"Customer\" AS \"t0\" CROSS JOIN ( VALUES ( 'NY' ), ( 'CA' ), ( 'TX' )) AS \"t1\" ( \"res0\" ) WHERE ( \"t1\" . \"res0\" ) = ( \"t0\" . \"State\" ) Note beam-sqlite does not support VALUES clauses anywhere within a query, but only within a common table expression.","title":"VALUES support"},{"location":"user-guide/queries/select/#ad-hoc-queries","text":"Sometimes you want to quickly query a database without having to write out all the boilerplate. Beam supports this with a feature called 'ad-hoc' queries. For example, to get all the names of customers, without having to use chinookDb at all. To use this functionality, import Database.Beam.Query.Adhoc . Note that we use TypeApplications to give each field an explicit type. If you don't, GHC will often infer the type, but it's nice to be explicit. Haskell Postgres Sqlite -- import qualified Database.Beam.Query.Adhoc as Adhoc do ( cId , firstName , lastName ) <- Adhoc . table_ Nothing {- Schema Name -} \"Customer\" ( Adhoc . field_ @ Int32 \"CustomerId\" , Adhoc . field_ @ Text \"FirstName\" , Adhoc . field_ @ Text \"LastName\" ) guard_ ( firstName ` like_ ` \"Jo%\" ) return ( cId , firstName , lastName ) SELECT \"t0\" . \"CustomerId\" AS \"res0\" , \"t0\" . \"FirstName\" AS \"res1\" , \"t0\" . \"LastName\" AS \"res2\" FROM \"Customer\" AS \"t0\" WHERE ( \"t0\" . \"FirstName\" ) LIKE ( 'Jo%' ) SELECT \"t0\" . \"CustomerId\" AS \"res0\" , \"t0\" . \"FirstName\" AS \"res1\" , \"t0\" . \"LastName\" AS \"res2\" FROM \"Customer\" AS \"t0\" WHERE ( \"t0\" . \"FirstName\" ) LIKE ( ? ); -- With values: [SQLText \"Jo%\"]","title":"Ad-hoc queries"},{"location":"user-guide/queries/window-functions/","text":"Window functions allow you to calculate aggregates over portions of your result set. They are defined in SQL2003. Some databases use the alternative nomenclature analytic functions . They are expressed in SQL with the OVER clause. The Postgres documentation offers a good overview of window functions. The withWindow_ function When you want to add windows to a query, use the withWindow_ function to introduce your frames, and compute the projection. You may notice that this is a departure from SQL syntax, where you can define window expressions inline. Beam seeks to be type-safe. Queries with window functions follow slightly different rules. Wrapping such a query with a special function allows beam to enforce these rules. For example, to get each invoice along with the average invoice total by each customer, use withWindow_ as follows. Haskell Postgres withWindow_ ( \\ i -> frame_ ( partitionBy_ ( invoiceCustomer i )) noOrder_ noBounds_ ) ( \\ i w -> ( i , avg_ ( invoiceTotal i ) ` over_ ` w )) ( all_ ( invoice chinookDb )) SELECT \"t0\" . \"InvoiceId\" AS \"res0\" , \"t0\" . \"CustomerId\" AS \"res1\" , \"t0\" . \"InvoiceDate\" AS \"res2\" , \"t0\" . \"BillingAddress\" AS \"res3\" , \"t0\" . \"BillingCity\" AS \"res4\" , \"t0\" . \"BillingState\" AS \"res5\" , \"t0\" . \"BillingCountry\" AS \"res6\" , \"t0\" . \"BillingPostalCode\" AS \"res7\" , \"t0\" . \"Total\" AS \"res8\" , AVG ( \"t0\" . \"Total\" ) OVER ( PARTITION BY \"t0\" . \"CustomerId\" ) AS \"res9\" FROM \"Invoice\" AS \"t0\" Or to get each invoice along with the ranking of each invoice by total per customer and the overall ranking, Haskell Postgres withWindow_ ( \\ i -> ( frame_ noPartition_ ( orderPartitionBy_ ( asc_ ( invoiceTotal i ))) noBounds_ , frame_ ( partitionBy_ ( invoiceCustomer i )) ( orderPartitionBy_ ( asc_ ( invoiceTotal i ))) noBounds_ )) ( \\ i ( allInvoices , customerInvoices ) -> ( i , as_ @ Int32 rank_ ` over_ ` allInvoices , as_ @ Int32 rank_ ` over_ ` customerInvoices )) ( all_ ( invoice chinookDb )) SELECT \"t0\" . \"InvoiceId\" AS \"res0\" , \"t0\" . \"CustomerId\" AS \"res1\" , \"t0\" . \"InvoiceDate\" AS \"res2\" , \"t0\" . \"BillingAddress\" AS \"res3\" , \"t0\" . \"BillingCity\" AS \"res4\" , \"t0\" . \"BillingState\" AS \"res5\" , \"t0\" . \"BillingCountry\" AS \"res6\" , \"t0\" . \"BillingPostalCode\" AS \"res7\" , \"t0\" . \"Total\" AS \"res8\" , RANK () OVER ( ORDER BY \"t0\" . \"Total\" ASC ) AS \"res9\" , RANK () OVER ( PARTITION BY \"t0\" . \"CustomerId\" ORDER BY \"t0\" . \"Total\" ASC ) AS \"res10\" FROM \"Invoice\" AS \"t0\" Note rank_ is only available in backends that implement the optional SQL2003 T611 feature \"Elementary OLAP operations\". Beam syntaxes that implement this functionality implement the IsSql2003ExpressionElementaryOLAPOperationsSyntax type class. Notice that aggregates over the result of the window expression work as you'd expect. Beam automatically generates a subquery once a query has been windowed. For example, to get the sum of the totals of the invoices, by rank. Haskell Postgres orderBy_ ( \\ ( rank , _ ) -> asc_ rank ) $ aggregate_ ( \\ ( i , rank ) -> ( group_ rank , sum_ $ invoiceTotal i )) $ withWindow_ ( \\ i -> frame_ ( partitionBy_ ( invoiceCustomer i )) ( orderPartitionBy_ ( asc_ ( invoiceTotal i ))) noBounds_ ) ( \\ i w -> ( i , as_ @ Int32 rank_ ` over_ ` w )) ( all_ ( invoice chinookDb )) SELECT \"t0\" . \"res9\" AS \"res0\" , SUM ( \"t0\" . \"res8\" ) AS \"res1\" FROM ( SELECT \"t0\" . \"InvoiceId\" AS \"res0\" , \"t0\" . \"CustomerId\" AS \"res1\" , \"t0\" . \"InvoiceDate\" AS \"res2\" , \"t0\" . \"BillingAddress\" AS \"res3\" , \"t0\" . \"BillingCity\" AS \"res4\" , \"t0\" . \"BillingState\" AS \"res5\" , \"t0\" . \"BillingCountry\" AS \"res6\" , \"t0\" . \"BillingPostalCode\" AS \"res7\" , \"t0\" . \"Total\" AS \"res8\" , RANK () OVER ( PARTITION BY \"t0\" . \"CustomerId\" ORDER BY \"t0\" . \"Total\" ASC ) AS \"res9\" FROM \"Invoice\" AS \"t0\" ) AS \"t0\" GROUP BY \"t0\" . \"res9\" ORDER BY \"t0\" . \"res9\" ASC More examples Windows and aggregates can be combined freely. For example, suppose you wanted to find, for each album, the single genre that represented most of the tracks on that album. We can begin by finding the number of tracks in a given genre on a given album using countAll_ and aggregate_ , like so Haskell Postgres Sqlite aggregate_ ( \\ t -> ( group_ ( trackAlbumId t ) , group_ ( trackGenreId t ) , as_ @ Int32 countAll_ )) $ all_ ( track chinookDb ) SELECT \"t0\" . \"AlbumId\" AS \"res0\" , \"t0\" . \"GenreId\" AS \"res1\" , COUNT ( * ) AS \"res2\" FROM \"Track\" AS \"t0\" GROUP BY \"t0\" . \"AlbumId\" , \"t0\" . \"GenreId\" SELECT \"t0\" . \"AlbumId\" AS \"res0\" , \"t0\" . \"GenreId\" AS \"res1\" , COUNT ( * ) AS \"res2\" FROM \"Track\" AS \"t0\" GROUP BY \"t0\" . \"AlbumId\" , \"t0\" . \"GenreId\" ; -- With values: [] Now, we want to find, for each album, which genre has the most tracks. We can do this by windowing over the album ID. Haskell Postgres let albumGenreCnts = aggregate_ ( \\ t -> ( group_ ( trackAlbumId t ) , group_ ( trackGenreId t ) , as_ @ Int32 countAll_ )) $ all_ ( track chinookDb ) in withWindow_ ( \\ ( albumId , _ , _ ) -> frame_ ( partitionBy_ albumId ) noOrder_ noBounds_ ) ( \\ ( albumId , genreId , trackCnt ) albumWindow -> ( albumId , genreId , trackCnt , max_ trackCnt ` over_ ` albumWindow )) $ albumGenreCnts SELECT \"t0\" . \"AlbumId\" AS \"res0\" , \"t0\" . \"GenreId\" AS \"res1\" , COUNT ( * ) AS \"res2\" , MAX ( COUNT ( * )) OVER ( PARTITION BY \"t0\" . \"AlbumId\" ) AS \"res3\" FROM \"Track\" AS \"t0\" GROUP BY \"t0\" . \"AlbumId\" , \"t0\" . \"GenreId\" We're almost there. Now, our query returns tuples of an album ID, a genre ID, the number of tracks in that genre for that album, and the number of tracks for the genre with the most tracks in that album To get just the genre with the most tracks, we have to find the genres wher the number of tracks (#3) matches #4. We can do this using filter_ . Because max_ can return NULL if there are no items in the window, we use filter_' and the nullable ordering operators. Haskell Postgres let albumGenreCnts = aggregate_ ( \\ t -> ( group_ ( trackAlbumId t ) , group_ ( trackGenreId t ) , as_ @ Int32 countAll_ )) $ all_ ( track chinookDb ) withMaxCounts = withWindow_ ( \\ ( albumId , _ , _ ) -> frame_ ( partitionBy_ albumId ) noOrder_ noBounds_ ) ( \\ ( albumId , genreId , trackCnt ) albumWindow -> ( albumId , genreId , trackCnt , max_ trackCnt ` over_ ` albumWindow )) $ albumGenreCnts in filter_' ( \\ ( _ , _ , trackCnt , maxTrackCntPerAlbum ) -> just_ trackCnt ==?. maxTrackCntPerAlbum ) withMaxCounts SELECT \"t0\" . \"res0\" AS \"res0\" , \"t0\" . \"res1\" AS \"res1\" , \"t0\" . \"res2\" AS \"res2\" , \"t0\" . \"res3\" AS \"res3\" FROM ( SELECT \"t0\" . \"AlbumId\" AS \"res0\" , \"t0\" . \"GenreId\" AS \"res1\" , COUNT ( * ) AS \"res2\" , MAX ( COUNT ( * )) OVER ( PARTITION BY \"t0\" . \"AlbumId\" ) AS \"res3\" FROM \"Track\" AS \"t0\" GROUP BY \"t0\" . \"AlbumId\" , \"t0\" . \"GenreId\" ) AS \"t0\" WHERE ( \"t0\" . \"res2\" ) = ( \"t0\" . \"res3\" ) Frame syntax The frame_ function takes a partition, ordering, and bounds parameter, all of which are optional. To specify no partition, use noPartition_ . For no ordering, use noOrder_ . For no bounds, use noBounds_ . To specify a partition, use partitionBy_ with an expression or a tuple of expressions. To specify an ordering use orderPartitionBy_ with an ordering expression or a tuple of ordering expressions. Ordering expressions are scalar expressions passed to either asc_ or desc_ . Finally, to specify bounds, use bounds_ or fromBound_ . fromBound_ starts the window at the specified position, which can be unbounded_ (the default) to include all rows seen thus far. bounds_ lets you specify an optional ending bound, which can be Nothing (the default), Just unbounded_ (the semantic default, but producing an explicit bound syntactically), or Just (nrows_ x) , where x is an integer expression, specifying the number of rows before or after to include in the calculation. The following query illustrates some of these features. Along with each invoice, it returns The average total of all invoices, given by the frame with no partition, ordering, and bounds. The average total of all invoices, by customer. The rank of each invoice over all the rows, when ordered by total. The average of the totals of the invoices starting at the two immediately preceding and ending with the two immediately succeeding invoices, when ordered by date. Haskell Postgres withWindow_ ( \\ i -> ( frame_ noPartition_ noOrder_ noBounds_ , frame_ ( partitionBy_ ( invoiceCustomer i )) noOrder_ noBounds_ , frame_ noPartition_ ( orderPartitionBy_ ( asc_ ( invoiceTotal i ))) noBounds_ , frame_ noPartition_ ( orderPartitionBy_ ( asc_ ( invoiceDate i ))) ( bounds_ ( nrows_ 2 ) ( Just ( nrows_ 2 ))))) ( \\ i ( allRows_ , sameCustomer_ , totals_ , fourInvoicesAround_ ) -> ( i , avg_ ( invoiceTotal i ) ` over_ ` allRows_ , avg_ ( invoiceTotal i ) ` over_ ` sameCustomer_ , as_ @ Int32 rank_ ` over_ ` totals_ , avg_ ( invoiceTotal i ) ` over_ ` fourInvoicesAround_ )) ( all_ ( invoice chinookDb )) SELECT \"t0\" . \"InvoiceId\" AS \"res0\" , \"t0\" . \"CustomerId\" AS \"res1\" , \"t0\" . \"InvoiceDate\" AS \"res2\" , \"t0\" . \"BillingAddress\" AS \"res3\" , \"t0\" . \"BillingCity\" AS \"res4\" , \"t0\" . \"BillingState\" AS \"res5\" , \"t0\" . \"BillingCountry\" AS \"res6\" , \"t0\" . \"BillingPostalCode\" AS \"res7\" , \"t0\" . \"Total\" AS \"res8\" , AVG ( \"t0\" . \"Total\" ) OVER () AS \"res9\" , AVG ( \"t0\" . \"Total\" ) OVER ( PARTITION BY \"t0\" . \"CustomerId\" ) AS \"res10\" , RANK () OVER ( ORDER BY \"t0\" . \"Total\" ASC ) AS \"res11\" , AVG ( \"t0\" . \"Total\" ) OVER ( ORDER BY \"t0\" . \"InvoiceDate\" ASC ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING ) AS \"res12\" FROM \"Invoice\" AS \"t0\"","title":"Window functions"},{"location":"user-guide/queries/window-functions/#the-withwindow_-function","text":"When you want to add windows to a query, use the withWindow_ function to introduce your frames, and compute the projection. You may notice that this is a departure from SQL syntax, where you can define window expressions inline. Beam seeks to be type-safe. Queries with window functions follow slightly different rules. Wrapping such a query with a special function allows beam to enforce these rules. For example, to get each invoice along with the average invoice total by each customer, use withWindow_ as follows. Haskell Postgres withWindow_ ( \\ i -> frame_ ( partitionBy_ ( invoiceCustomer i )) noOrder_ noBounds_ ) ( \\ i w -> ( i , avg_ ( invoiceTotal i ) ` over_ ` w )) ( all_ ( invoice chinookDb )) SELECT \"t0\" . \"InvoiceId\" AS \"res0\" , \"t0\" . \"CustomerId\" AS \"res1\" , \"t0\" . \"InvoiceDate\" AS \"res2\" , \"t0\" . \"BillingAddress\" AS \"res3\" , \"t0\" . \"BillingCity\" AS \"res4\" , \"t0\" . \"BillingState\" AS \"res5\" , \"t0\" . \"BillingCountry\" AS \"res6\" , \"t0\" . \"BillingPostalCode\" AS \"res7\" , \"t0\" . \"Total\" AS \"res8\" , AVG ( \"t0\" . \"Total\" ) OVER ( PARTITION BY \"t0\" . \"CustomerId\" ) AS \"res9\" FROM \"Invoice\" AS \"t0\" Or to get each invoice along with the ranking of each invoice by total per customer and the overall ranking, Haskell Postgres withWindow_ ( \\ i -> ( frame_ noPartition_ ( orderPartitionBy_ ( asc_ ( invoiceTotal i ))) noBounds_ , frame_ ( partitionBy_ ( invoiceCustomer i )) ( orderPartitionBy_ ( asc_ ( invoiceTotal i ))) noBounds_ )) ( \\ i ( allInvoices , customerInvoices ) -> ( i , as_ @ Int32 rank_ ` over_ ` allInvoices , as_ @ Int32 rank_ ` over_ ` customerInvoices )) ( all_ ( invoice chinookDb )) SELECT \"t0\" . \"InvoiceId\" AS \"res0\" , \"t0\" . \"CustomerId\" AS \"res1\" , \"t0\" . \"InvoiceDate\" AS \"res2\" , \"t0\" . \"BillingAddress\" AS \"res3\" , \"t0\" . \"BillingCity\" AS \"res4\" , \"t0\" . \"BillingState\" AS \"res5\" , \"t0\" . \"BillingCountry\" AS \"res6\" , \"t0\" . \"BillingPostalCode\" AS \"res7\" , \"t0\" . \"Total\" AS \"res8\" , RANK () OVER ( ORDER BY \"t0\" . \"Total\" ASC ) AS \"res9\" , RANK () OVER ( PARTITION BY \"t0\" . \"CustomerId\" ORDER BY \"t0\" . \"Total\" ASC ) AS \"res10\" FROM \"Invoice\" AS \"t0\" Note rank_ is only available in backends that implement the optional SQL2003 T611 feature \"Elementary OLAP operations\". Beam syntaxes that implement this functionality implement the IsSql2003ExpressionElementaryOLAPOperationsSyntax type class. Notice that aggregates over the result of the window expression work as you'd expect. Beam automatically generates a subquery once a query has been windowed. For example, to get the sum of the totals of the invoices, by rank. Haskell Postgres orderBy_ ( \\ ( rank , _ ) -> asc_ rank ) $ aggregate_ ( \\ ( i , rank ) -> ( group_ rank , sum_ $ invoiceTotal i )) $ withWindow_ ( \\ i -> frame_ ( partitionBy_ ( invoiceCustomer i )) ( orderPartitionBy_ ( asc_ ( invoiceTotal i ))) noBounds_ ) ( \\ i w -> ( i , as_ @ Int32 rank_ ` over_ ` w )) ( all_ ( invoice chinookDb )) SELECT \"t0\" . \"res9\" AS \"res0\" , SUM ( \"t0\" . \"res8\" ) AS \"res1\" FROM ( SELECT \"t0\" . \"InvoiceId\" AS \"res0\" , \"t0\" . \"CustomerId\" AS \"res1\" , \"t0\" . \"InvoiceDate\" AS \"res2\" , \"t0\" . \"BillingAddress\" AS \"res3\" , \"t0\" . \"BillingCity\" AS \"res4\" , \"t0\" . \"BillingState\" AS \"res5\" , \"t0\" . \"BillingCountry\" AS \"res6\" , \"t0\" . \"BillingPostalCode\" AS \"res7\" , \"t0\" . \"Total\" AS \"res8\" , RANK () OVER ( PARTITION BY \"t0\" . \"CustomerId\" ORDER BY \"t0\" . \"Total\" ASC ) AS \"res9\" FROM \"Invoice\" AS \"t0\" ) AS \"t0\" GROUP BY \"t0\" . \"res9\" ORDER BY \"t0\" . \"res9\" ASC","title":"The withWindow_ function"},{"location":"user-guide/queries/window-functions/#more-examples","text":"Windows and aggregates can be combined freely. For example, suppose you wanted to find, for each album, the single genre that represented most of the tracks on that album. We can begin by finding the number of tracks in a given genre on a given album using countAll_ and aggregate_ , like so Haskell Postgres Sqlite aggregate_ ( \\ t -> ( group_ ( trackAlbumId t ) , group_ ( trackGenreId t ) , as_ @ Int32 countAll_ )) $ all_ ( track chinookDb ) SELECT \"t0\" . \"AlbumId\" AS \"res0\" , \"t0\" . \"GenreId\" AS \"res1\" , COUNT ( * ) AS \"res2\" FROM \"Track\" AS \"t0\" GROUP BY \"t0\" . \"AlbumId\" , \"t0\" . \"GenreId\" SELECT \"t0\" . \"AlbumId\" AS \"res0\" , \"t0\" . \"GenreId\" AS \"res1\" , COUNT ( * ) AS \"res2\" FROM \"Track\" AS \"t0\" GROUP BY \"t0\" . \"AlbumId\" , \"t0\" . \"GenreId\" ; -- With values: [] Now, we want to find, for each album, which genre has the most tracks. We can do this by windowing over the album ID. Haskell Postgres let albumGenreCnts = aggregate_ ( \\ t -> ( group_ ( trackAlbumId t ) , group_ ( trackGenreId t ) , as_ @ Int32 countAll_ )) $ all_ ( track chinookDb ) in withWindow_ ( \\ ( albumId , _ , _ ) -> frame_ ( partitionBy_ albumId ) noOrder_ noBounds_ ) ( \\ ( albumId , genreId , trackCnt ) albumWindow -> ( albumId , genreId , trackCnt , max_ trackCnt ` over_ ` albumWindow )) $ albumGenreCnts SELECT \"t0\" . \"AlbumId\" AS \"res0\" , \"t0\" . \"GenreId\" AS \"res1\" , COUNT ( * ) AS \"res2\" , MAX ( COUNT ( * )) OVER ( PARTITION BY \"t0\" . \"AlbumId\" ) AS \"res3\" FROM \"Track\" AS \"t0\" GROUP BY \"t0\" . \"AlbumId\" , \"t0\" . \"GenreId\" We're almost there. Now, our query returns tuples of an album ID, a genre ID, the number of tracks in that genre for that album, and the number of tracks for the genre with the most tracks in that album To get just the genre with the most tracks, we have to find the genres wher the number of tracks (#3) matches #4. We can do this using filter_ . Because max_ can return NULL if there are no items in the window, we use filter_' and the nullable ordering operators. Haskell Postgres let albumGenreCnts = aggregate_ ( \\ t -> ( group_ ( trackAlbumId t ) , group_ ( trackGenreId t ) , as_ @ Int32 countAll_ )) $ all_ ( track chinookDb ) withMaxCounts = withWindow_ ( \\ ( albumId , _ , _ ) -> frame_ ( partitionBy_ albumId ) noOrder_ noBounds_ ) ( \\ ( albumId , genreId , trackCnt ) albumWindow -> ( albumId , genreId , trackCnt , max_ trackCnt ` over_ ` albumWindow )) $ albumGenreCnts in filter_' ( \\ ( _ , _ , trackCnt , maxTrackCntPerAlbum ) -> just_ trackCnt ==?. maxTrackCntPerAlbum ) withMaxCounts SELECT \"t0\" . \"res0\" AS \"res0\" , \"t0\" . \"res1\" AS \"res1\" , \"t0\" . \"res2\" AS \"res2\" , \"t0\" . \"res3\" AS \"res3\" FROM ( SELECT \"t0\" . \"AlbumId\" AS \"res0\" , \"t0\" . \"GenreId\" AS \"res1\" , COUNT ( * ) AS \"res2\" , MAX ( COUNT ( * )) OVER ( PARTITION BY \"t0\" . \"AlbumId\" ) AS \"res3\" FROM \"Track\" AS \"t0\" GROUP BY \"t0\" . \"AlbumId\" , \"t0\" . \"GenreId\" ) AS \"t0\" WHERE ( \"t0\" . \"res2\" ) = ( \"t0\" . \"res3\" )","title":"More examples"},{"location":"user-guide/queries/window-functions/#frame-syntax","text":"The frame_ function takes a partition, ordering, and bounds parameter, all of which are optional. To specify no partition, use noPartition_ . For no ordering, use noOrder_ . For no bounds, use noBounds_ . To specify a partition, use partitionBy_ with an expression or a tuple of expressions. To specify an ordering use orderPartitionBy_ with an ordering expression or a tuple of ordering expressions. Ordering expressions are scalar expressions passed to either asc_ or desc_ . Finally, to specify bounds, use bounds_ or fromBound_ . fromBound_ starts the window at the specified position, which can be unbounded_ (the default) to include all rows seen thus far. bounds_ lets you specify an optional ending bound, which can be Nothing (the default), Just unbounded_ (the semantic default, but producing an explicit bound syntactically), or Just (nrows_ x) , where x is an integer expression, specifying the number of rows before or after to include in the calculation. The following query illustrates some of these features. Along with each invoice, it returns The average total of all invoices, given by the frame with no partition, ordering, and bounds. The average total of all invoices, by customer. The rank of each invoice over all the rows, when ordered by total. The average of the totals of the invoices starting at the two immediately preceding and ending with the two immediately succeeding invoices, when ordered by date. Haskell Postgres withWindow_ ( \\ i -> ( frame_ noPartition_ noOrder_ noBounds_ , frame_ ( partitionBy_ ( invoiceCustomer i )) noOrder_ noBounds_ , frame_ noPartition_ ( orderPartitionBy_ ( asc_ ( invoiceTotal i ))) noBounds_ , frame_ noPartition_ ( orderPartitionBy_ ( asc_ ( invoiceDate i ))) ( bounds_ ( nrows_ 2 ) ( Just ( nrows_ 2 ))))) ( \\ i ( allRows_ , sameCustomer_ , totals_ , fourInvoicesAround_ ) -> ( i , avg_ ( invoiceTotal i ) ` over_ ` allRows_ , avg_ ( invoiceTotal i ) ` over_ ` sameCustomer_ , as_ @ Int32 rank_ ` over_ ` totals_ , avg_ ( invoiceTotal i ) ` over_ ` fourInvoicesAround_ )) ( all_ ( invoice chinookDb )) SELECT \"t0\" . \"InvoiceId\" AS \"res0\" , \"t0\" . \"CustomerId\" AS \"res1\" , \"t0\" . \"InvoiceDate\" AS \"res2\" , \"t0\" . \"BillingAddress\" AS \"res3\" , \"t0\" . \"BillingCity\" AS \"res4\" , \"t0\" . \"BillingState\" AS \"res5\" , \"t0\" . \"BillingCountry\" AS \"res6\" , \"t0\" . \"BillingPostalCode\" AS \"res7\" , \"t0\" . \"Total\" AS \"res8\" , AVG ( \"t0\" . \"Total\" ) OVER () AS \"res9\" , AVG ( \"t0\" . \"Total\" ) OVER ( PARTITION BY \"t0\" . \"CustomerId\" ) AS \"res10\" , RANK () OVER ( ORDER BY \"t0\" . \"Total\" ASC ) AS \"res11\" , AVG ( \"t0\" . \"Total\" ) OVER ( ORDER BY \"t0\" . \"InvoiceDate\" ASC ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING ) AS \"res12\" FROM \"Invoice\" AS \"t0\"","title":"Frame syntax"}]}